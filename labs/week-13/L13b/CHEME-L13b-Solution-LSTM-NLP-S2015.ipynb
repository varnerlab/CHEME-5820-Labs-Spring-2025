{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1fd89e-6844-4ada-870e-d0c796094955",
   "metadata": {},
   "source": [
    "# L13b: Long Short Term Memory (LSTM) Model for Natural Language Text\n",
    "Fill me in\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: Let's take 5 minutes to load and analyze a weather dataset downloaded from [the National Oceanic and Atmospheric Administration (NOAA)](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USC00304174/detail). Once we load the data, we'll do some data wrangling (scaling).\n",
    "* __Task 2: Setup the model structure and training (15 min)__: In this task, we'll construct and train the RNN model, i.e., we'll learn the model parameters, using [the gradient descent with momentum algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize [the mean-squared error (mse) loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.mse). \n",
    "* __Task 3: Play around with the model structure and parameters (20 min)__: In this task, we'll change the model structure, e.g., how many hidden states we have, and include other layers. We'll also change the learning rate and other hyperparameters and look at their effect on the model performance. We'll also look at the effect of changing the number of training epochs and the batch size.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e02a2-1b69-45dd-8216-86196c411c73",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba258d4",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "We'll load a public dataset of headlines that have been curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
    "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
    "\n",
    "The data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We've developed a parser to read the sarcasm data file. The [`corpus(...)` method](src/Files.jl) takes the `path::String` argument (the path to the datafile) and returns a [`MySarcasmRecordCorpusModel` instance](src/Types.jl) which holds the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3de5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = joinpath(_PATH_TO_DATA, \"Sarcasm_Headlines_Dataset_v2.txt\") |> corpus;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095162c",
   "metadata": {},
   "source": [
    "The [`MySarcasmRecordCorpusModel` instance](src/Types.jl) has the fields that are populated when we read the file:\n",
    "* The `records::Dict{Int, MySarcasmRecordModel}` field holds the original records data as a dictionary, where the keys of the dictionary correspond to the headline index, and the values are [instances of the `MySarcasmRecordModel` type](src/Types.jl).\n",
    "* The `tokens::Dict{String, Int64}` field holds the vocabulary computed over the dataset as a dictionary, where the dictionary's keys are the words (called tokens) and the values of the index of the word. We assemble the `tokens` dictionary in alphabetical order. This is initially undefined.\n",
    "* The `inverse::Dict{Int64, String}` field is the inverse of the `tokens` dictionary, where the keys are the token indexes and the values are the tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc249b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e019dbc",
   "metadata": {},
   "source": [
    "Each [`MySarcasmRecordModel` instance](src/Types.jl) has the three fields in the original data records: an `issarcastic::Bool` field holding the label for this record, the `headline::String` field holding the headline and the `article::String` field holding a link to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078a7939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mother comes pretty close to using word streaming correctly\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records[5].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba904f",
   "metadata": {},
   "source": [
    "### Tokenize the headline records\n",
    "In this task, we'll use the corpus model, particularly the `tokens::Dict{String, Int64}` dictionary, to tokenize headlines in our dataset, i.e., convert a text representation into a numerical vector representation. \n",
    "\n",
    "To better understand how this works, let's first examine a single (random) record and tokenize it.  We'll select a random record from the `number_of_records::Int64` possible records [using the built-in `rand(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand), and store it in the `random_test_record::MySarcasmRecordModel` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "953b5b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"oat farmer seriously thinking about getting into barley\", \"https://local.theonion.com/oat-farmer-seriously-thinking-about-getting-into-barley-1825109075\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_records = corpusmodel.records |> length; # what is going on here?\n",
    "random_test_record = rand(1:number_of_records) |> i -> corpusmodel.records[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575769c",
   "metadata": {},
   "source": [
    "Next, let's call [the `tokenize(...)` method](src/Compute.jl), which takes the `headline::String` that we want to tokenize, and our vocabulary stored in the `tokens::Dict{String, Int64}` dictionary and returns a token vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed40fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Int64}:\n",
       "  5225\n",
       " 10571\n",
       "  9020\n",
       "  5225\n",
       " 10571\n",
       "  9020"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(random_test_record.headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488a597",
   "metadata": {},
   "source": [
    "### Hmmm. What happens if a token is not in the dataset?\n",
    "We have created the vocabulary in the `tokens::Dict{String, Int64}` dictionary by analyzing the entire dataset, but suppose we have new samples that aren't in the dataset; what happens then? We've added the `<OOV>` token to our dataset; let's see if that works. \n",
    "* Let's take the headline from the `random_test_record::MySarcasmRecordModel` instance and add something to the end, e.g., `#ilovemyroomba`. we should get the `<OOV>` token at the end of the token vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3904d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = corpusmodel.tokens |> keys |> collect; # what?? We are getting keys (words) and turning into an array\n",
    "\"#ilovemyroomba\" ∈ words # fancy way of checking if item is in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b52116",
   "metadata": {},
   "source": [
    "Create a new headline by appending `#ilovemyroomba` to the old headline. String append operations in Julia use [the `*` method](https://docs.julialang.org/en/v1/manual/strings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f9faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"chinese food emojis chinese food emojis #ilovemyroomba\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_headline = random_test_record.headline * \" \" * \"#ilovemyroomba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b733c",
   "metadata": {},
   "source": [
    "Tokenize the `new_test_headline::String`, and let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ecdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Int64}:\n",
       "  5225\n",
       " 10571\n",
       "  9020\n",
       "  5225\n",
       " 10571\n",
       "  9020\n",
       "   912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(new_test_headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c95b",
   "metadata": {},
   "source": [
    "### Compute the maximum pad length\n",
    "Not every headline has the same length, but we want the token vectors to have the same size. Thus, we'll find the longest vectors in the dataset and pad the token vectors to that length. To do that, let's iterate through each headline, compute its size, and then save this length if it is longer than we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a3eee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_pad_length = let\n",
    "\n",
    "    max_pad_length = 0; # initialize: we have 0 length\n",
    "    for i ∈ 1:number_of_records\n",
    "        test_record_length = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens) |> length; # tokenize, and calc the number of tokens\n",
    "        if (test_record_length > max_pad_length)\n",
    "            max_pad_length = test_record_length; # we've found a new longest headline!\n",
    "        end\n",
    "    end\n",
    "    max_pad_length\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fa686",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all headline samples\n",
    "Finally, now that we have found the `max_pad_length::Int64`, we can tokenize all records using the `max_pad_length::Int64` value as the `pad` value in [the `tokenize(...)` method](src/Compute.jl). \n",
    "* We'll use `right-padding` and will store the tokenized records for each headline in the `token_record_dictionary::Dict{Int64, Array{Int64,1}}` dictionary, where the keys of this dictionary are the record indexes, and the values of the tokenized records (which are of type `Array{Int64,1}.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c228e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(24824 => [25877, 6523, 16124, 24452, 13458, 7184, 19562, 4737, 913, 913  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 25754 => [20180, 17482, 12832, 18535, 19766, 25507, 3017, 20259, 28438, 913  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 11950 => [2446, 8040, 4362, 1645, 6930, 18873, 21117, 913, 913, 913  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 1703 => [8236, 6707, 23707, 26826, 29323, 16580, 18615, 4068, 23172, 913  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 12427 => [17647, 22223, 26826, 12327, 20945, 29192, 28514, 21852, 8483, 913  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 7685 => [26618, 26363, 27362, 26826, 16117, 26534, 22568, 22967, 29484, 3017  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 18374 => [10013, 18296, 16586, 29538, 18115, 12257, 23172, 8929, 28693, 22342  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 3406 => [11741, 15064, 1643, 6108, 26534, 20435, 18533, 6330, 13458, 20187  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 23970 => [5539, 6661, 17968, 28345, 26826, 13630, 19952, 26731, 18533, 9965  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913], 27640 => [16776, 18533, 1233, 11341, 4001, 26857, 15143, 5733, 18533, 19594  …  913, 913, 913, 913, 913, 913, 913, 913, 913, 913]…), Dict(24824 => 0, 25754 => 0, 11950 => 1, 1703 => 1, 12427 => 0, 7685 => 0, 18374 => 1, 3406 => 0, 23970 => 1, 27640 => 1…))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_record_dictionary, labels = let\n",
    "\n",
    "    # initialize -\n",
    "    token_record_dictionary = Dict{Int64, Array{Int64,1}}();\n",
    "    labels = Dict{Int64, Int64}();\n",
    "    \n",
    "    for i ∈ 1:number_of_records\n",
    "        v = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens, \n",
    "                pad = max_pad_length); \n",
    "        l = corpusmodel.records[i].issarcastic; # 1 for sarcastic, 0 for not sarcastic\n",
    "        token_record_dictionary[i] = v;\n",
    "        labels[i] = l;\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    token_record_dictionary, labels\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1100d5",
   "metadata": {},
   "source": [
    "### Save tokenized data and labels to disk\n",
    "We did a bunch of stuff in this example, and we don't want to have to recompute the corpus, token dictionary, etc. So let's save it [in an HDF5 encoded binary file](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    "\n",
    "To start, we specify a path. We'll then write data to disk as a `jld2` (binary) saved file using [the `save(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). This will save the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). The save file is [an HDF5 encoded file format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), which is small (compressed), which is excellent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d053fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    path_to_save_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\"); \n",
    "    save(path_to_save_file, Dict(\"corpus\" => corpusmodel, \n",
    "        \"number_of_records\" => number_of_records, \n",
    "        \"tokenrecorddictionary\" => token_record_dictionary, \n",
    "        \"labeldictionary\" => labels)); # encode, and write\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722e7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
