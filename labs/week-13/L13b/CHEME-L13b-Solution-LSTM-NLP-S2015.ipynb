{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1fd89e-6844-4ada-870e-d0c796094955",
   "metadata": {},
   "source": [
    "# L13b: Long Short Term Memory (LSTM) Model for Natural Language Text\n",
    "In this lab, we'll compare the binary classification performance of a traditional feedforward neural network (FNN) and a recurrent neural network (RNN) using long short-term memory (LSTM) cells on a sarcasm detection task. The dataset is a set of approximate 28,000 news headlines, labeled as sarcastic or not sarcastic. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection).\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: Let's take 5 minutes to load and analyze a weather dataset downloaded from [the National Oceanic and Atmospheric Administration (NOAA)](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USC00304174/detail). Once we load the data, we'll do some data wrangling (scaling).\n",
    "* __Task 2: Setup the model structure and training (15 min)__: In this task, we'll construct and train the RNN model, i.e., we'll learn the model parameters, using [the gradient descent with momentum algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize [the mean-squared error (mse) loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.mse). \n",
    "* __Task 3: Play around with the model structure and parameters (20 min)__: In this task, we'll change the model structure, e.g., how many hidden states we have, and include other layers. We'll also change the learning rate and other hyperparameters and look at their effect on the model performance. We'll also look at the effect of changing the number of training epochs and the batch size.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e02a2-1b69-45dd-8216-86196c411c73",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba258d4",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "We'll load a public dataset of headlines that have been curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
    "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
    "\n",
    "The data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We've developed a parser to read the sarcasm data file. The [`corpus(...)` method](src/Files.jl) takes the `path::String` argument (the path to the datafile) and returns a [`MySarcasmRecordCorpusModel` instance](src/Types.jl) which holds the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3de5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = joinpath(_PATH_TO_DATA, \"Sarcasm_Headlines_Dataset_v2.txt\") |> corpus;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095162c",
   "metadata": {},
   "source": [
    "The [`MySarcasmRecordCorpusModel` instance](src/Types.jl) has the fields that are populated when we read the file:\n",
    "* The `records::Dict{Int, MySarcasmRecordModel}` field holds the original records data as a dictionary, where the keys of the dictionary correspond to the headline index, and the values are [instances of the `MySarcasmRecordModel` type](src/Types.jl).\n",
    "* The `tokens::Dict{String, Int64}` field holds the vocabulary computed over the dataset as a dictionary, where the dictionary's keys are the words (called tokens) and the values of the index of the word. We assemble the `tokens` dictionary in alphabetical order. This is initially undefined.\n",
    "* The `inverse::Dict{Int64, String}` field is the inverse of the `tokens` dictionary, where the keys are the token indexes and the values are the tokens (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e019dbc",
   "metadata": {},
   "source": [
    "Each [`MySarcasmRecordModel` instance](src/Types.jl) has the three fields in the original data records: an `issarcastic::Bool` field holding the label for this record, the `headline::String` field holding the headline and the `article::String` field holding a link to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078a7939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mother comes pretty close to using word streaming correctly\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records[5].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba904f",
   "metadata": {},
   "source": [
    "### Tokenize the headline records\n",
    "In this task, we'll use the corpus model, particularly the `tokens::Dict{String, Int64}` dictionary, to tokenize headlines in our dataset, i.e., convert a text representation into a numerical vector representation. \n",
    "\n",
    "To better understand how this works, let's first examine a single (random) record and tokenize it.  We'll select a random record from the `number_of_records::Int64` possible records [using the built-in `rand(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand), and store it in the `random_test_record::MySarcasmRecordModel` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953b5b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(false, \"paul oneal teen shot by chicago cops suffered gunshot wound to his back\", \"https://www.huffingtonpost.com/entry/paul-oneal-teen-shot-by-chicago-cops-suffered-gunshot-wound-to-his-back_us_57b5eaa8e4b0fd5a2f41ce3c\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_records = corpusmodel.records |> length; # what is going on here?\n",
    "random_test_record = rand(1:number_of_records) |> i -> corpusmodel.records[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575769c",
   "metadata": {},
   "source": [
    "Next, let's call [the `tokenize(...)` method](src/Compute.jl), which takes the `headline::String` that we want to tokenize, and our vocabulary stored in the `tokens::Dict{String, Int64}` dictionary and returns a token vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed40fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Int64}:\n",
       " 19452\n",
       " 18668\n",
       " 26359\n",
       " 24017\n",
       "  4362\n",
       "  5173\n",
       "  6353\n",
       " 25720\n",
       " 11974\n",
       " 29338\n",
       " 26826\n",
       " 12721\n",
       "  2594"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(random_test_record.headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488a597",
   "metadata": {},
   "source": [
    "### Hmmm. What happens if a token is not in the dataset?\n",
    "We have created the vocabulary in the `tokens::Dict{String, Int64}` dictionary by analyzing the entire dataset, but suppose we have new samples that aren't in the dataset; what happens then? We've added the `<OOV>` token to our dataset; let's see if that works. \n",
    "* Let's take the headline from the `random_test_record::MySarcasmRecordModel` instance and add something to the end, e.g., `#ilovemyroomba`. we should get the `<OOV>` token at the end of the token vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3904d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = corpusmodel.tokens |> keys |> collect; # what?? We are getting keys (words) and turning into an array\n",
    "\"#ilovemyroomba\" âˆˆ words # fancy way of checking if item is in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b52116",
   "metadata": {},
   "source": [
    "Create a new headline by appending `#ilovemyroomba` to the old headline. String append operations in Julia use [the `*` method](https://docs.julialang.org/en/v1/manual/strings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f9faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul oneal teen shot by chicago cops suffered gunshot wound to his back #ilovemyroomba\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_headline = random_test_record.headline * \" \" * \"#ilovemyroomba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b733c",
   "metadata": {},
   "source": [
    "Tokenize the `new_test_headline::String`, and let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ecdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Vector{Int64}:\n",
       " 19452\n",
       " 18668\n",
       " 26359\n",
       " 24017\n",
       "  4362\n",
       "  5173\n",
       "  6353\n",
       " 25720\n",
       " 11974\n",
       " 29338\n",
       " 26826\n",
       " 12721\n",
       "  2594\n",
       "   912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(new_test_headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c95b",
   "metadata": {},
   "source": [
    "### Compute the maximum pad length\n",
    "Not every headline has the same length, but we want the token vectors to have the same size. Thus, we'll find the longest vectors in the dataset and pad the token vectors to that length. To do that, let's iterate through each headline, compute its size, and then save this length if it is longer than we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3eee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_pad_length = let\n",
    "\n",
    "    max_pad_length = 0; # initialize: we have 0 length\n",
    "    for i âˆˆ 1:number_of_records\n",
    "        test_record_length = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens) |> length; # tokenize, and calc the number of tokens\n",
    "        if (test_record_length > max_pad_length)\n",
    "            max_pad_length = test_record_length; # we've found a new longest headline!\n",
    "        end\n",
    "    end\n",
    "    max_pad_length\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fa686",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all headline samples\n",
    "Finally, now that we have found the `max_pad_length::Int64`, we can tokenize all records using the `max_pad_length::Int64` value as the `pad` value in [the `tokenize(...)` method](src/Compute.jl). \n",
    "* We'll use `right-padding` and will store the tokenized records for each headline in the `token_record_dictionary::Dict{Int64, Array{Int64,1}}` dictionary, where the keys of this dictionary are the record indexes, and the values of the tokenized records (which are of type `Array{Int64,1}.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0c228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_record_dictionary, labels = let\n",
    "\n",
    "    # initialize -\n",
    "    token_record_dictionary = Dict{Int64, Array{Float32,1}}();\n",
    "    labels = Dict{Int64, Float32}();\n",
    "    \n",
    "    for i âˆˆ 1:number_of_records\n",
    "        v = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens, \n",
    "                pad = max_pad_length); \n",
    "        l = corpusmodel.records[i].issarcastic; # 1 for sarcastic, 0 for not sarcastic\n",
    "        token_record_dictionary[i] = v .|> Float32; # convert to float32\n",
    "        labels[i] = l .|> Float32; # convert to float32\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    token_record_dictionary, labels\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1100d5",
   "metadata": {},
   "source": [
    "### Save tokenized data and labels to disk\n",
    "We did a bunch of stuff in this example, and we don't want to have to recompute the corpus, token dictionary, etc. So let's save it [in an HDF5 encoded binary file](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    "\n",
    "To start, we specify a path. We'll then write data to disk as a `jld2` (binary) saved file using [the `save(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). This will save the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). The save file is [an HDF5 encoded file format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), which is small (compressed), which is excellent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d053fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    path_to_save_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\"); \n",
    "    save(path_to_save_file, Dict(\"corpus\" => corpusmodel, \n",
    "        \"number_of_records\" => number_of_records, \n",
    "        \"tokenrecorddictionary\" => token_record_dictionary, \n",
    "        \"labeldictionary\" => labels)); # encode, and write\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afcb1",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants that we will use in the exercise. Check the comment next to the value for a description of its meaning, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a170bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 10000;\n",
    "number_of_inputs = max_pad_length; # dimension of the input\n",
    "number_of_hidden_states = 2^8; # dimension of hidden state memory\n",
    "Ïƒâ‚‚ = NNlib.tanh_fast; # activation function\n",
    "number_of_epochs = 50; # TODO: update how many epochs we want to train for\n",
    "number_digit_array = range(0,length=2,step=1) |> collect; # numbers 0 ... 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79cb3c",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d98234",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_headlines_dataset = let\n",
    "\n",
    "    # generate random index set -\n",
    "    random_training_index_set = Set{Int64}();\n",
    "    \n",
    "    # Uncomment me for random selection -\n",
    "    # should_stop_loop = false;\n",
    "    # counter = 0;\n",
    "    # while (should_stop_loop == false)\n",
    "    #     i = rand(1:number_of_records);\n",
    "    #     push!(random_training_index_set, i);\n",
    "\n",
    "    #     if (length(random_training_index_set) â‰¥ number_of_training_examples)\n",
    "    #         should_stop_loop = true; # ok to stop the loop\n",
    "    #     else\n",
    "    #         counter += 1;\n",
    "    #     end\n",
    "    # end\n",
    "    # random_training_index_array = random_training_index_set |> collect |> sort;\n",
    "\n",
    "    # Uncomment me for sequential selection -\n",
    "    random_training_index_array = range(1,\n",
    "        length=number_of_training_examples, step=1) |> collect; # sequential selection\n",
    "    \n",
    "    \n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for index âˆˆ eachindex(random_training_index_array)\n",
    "        i = random_training_index_array[index];\n",
    "        token_record = token_record_dictionary[i] .|> Float32; # get the tokenized headline\n",
    "        one_hot_label = onehot(labels[i],number_digit_array); # get the label\n",
    "        push!(training_dataset, (token_record, one_hot_label)); # add to the dataset\n",
    "    end\n",
    "    \n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851691c5",
   "metadata": {},
   "source": [
    "## Task 2: Construct, Train and Analyze a Sarcasm FFN \n",
    "In this task, we'll construct a feedforward neural network (FNN) model to classify the sarcasm dataset. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a6aced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxFeedForwardNeuralNetworkModel trainable=(input, middle, final, output); # create a \"namespaced\" of sorts\n",
    "MyFNNModel() = MyFluxFeedForwardNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        input = Flux.Dense(number_of_inputs => number_of_hidden_states, Ïƒâ‚‚),  # hidden layer\n",
    "        middle = Flux.Dense(number_of_hidden_states => number_of_hidden_states, Ïƒâ‚‚), # output layer\n",
    "        final = Flux.Dense(number_of_hidden_states => number_of_hidden_states, Ïƒâ‚‚), # output layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, Ïƒâ‚‚), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "fnnmodel = MyFNNModel().chain; # Hmmm. fnnmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31ab32",
   "metadata": {},
   "source": [
    "_Which FNN optimizer_? The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `Î»` parameter denotes the `learning rate` and `Î²` denotes the momentum parameter. We save information about the optimizer in the `opt_fnn` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a86def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fnn = let\n",
    "\n",
    "    Î» = 0.50; # TODO: update the learning rate\n",
    "    Î² = 0.10; # TODO: update the momentum parameter\n",
    "    opt_state = Flux.setup(Momentum(Î», Î²), fnnmodel); # opt_state has all the details of the optimizer\n",
    "\n",
    "    # return -\n",
    "    opt_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e697588",
   "metadata": {},
   "source": [
    "__Training loop__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a020209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(151 => 256, tanh_fast),  \u001b[90m# 38_912 parameters\u001b[39m\n",
       "  middle = Dense(256 => 256, tanh_fast),  \u001b[90m# 65_792 parameters\u001b[39m\n",
       "  final = Dense(256 => 256, tanh_fast),  \u001b[90m# 65_792 parameters\u001b[39m\n",
       "  output = Dense(256 => 2, tanh_fast),  \u001b[90m# 514 parameters\u001b[39m\n",
       "  softmax = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 8 arrays, \u001b[39m171_010 parameters, 668.414 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_fnn_model = let \n",
    "   \n",
    "    should_we_train = false; # TODO: set this flag to {true | false}\n",
    "    model = fnnmodel;\n",
    "    if (should_we_train == true)\n",
    "        for i = 1:number_of_epochs\n",
    "        \n",
    "            # train the model -\n",
    "            Flux.train!(model, training_headlines_dataset, opt_fnn) do m, x, y\n",
    "                Flux.Losses.logitcrossentropy(m(x), y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "            end\n",
    "    \n",
    "            if (rem(i,10) == 0)\n",
    "                @info \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "            end\n",
    "    \n",
    "            # save the state of the model, in case something happens. We can reload from this state\n",
    "            jldsave(joinpath(_PATH_TO_DATA, \"tmp-model-training-checkpoint.jld2\"), model_state = Flux.state(model))  \n",
    "        end\n",
    "    else\n",
    "\n",
    "        # if we don't train: load up a previous model\n",
    "        model_state = JLD2.load(joinpath(_PATH_TO_DATA, \"tmp-model-training-checkpoint.jld2\"), \"model_state\");\n",
    "        Flux.loadmodel!(model, model_state);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347d8a9-19e4-4f90-956c-d0015a8c7186",
   "metadata": {},
   "source": [
    "### Correct prediction `training` dataset\n",
    "In the code block below, we pass the pixel data from the image into the `model` instance, compute the predicted label `yÌ‚,` and compare the predicted and actual labels for the `training` dataset.\n",
    "* _Logic_: If the prediction and the actual label agree, we update the `S` variable (a running count of the number of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `training` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db106e0c-a632-4aaa-88f5-41fc85170d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 48.17%\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    S_training = 0;\n",
    "    for i âˆˆ eachindex(training_headlines_dataset)\n",
    "    \n",
    "        x = training_headlines_dataset[i][1];\n",
    "        y = training_headlines_dataset[i][2];\n",
    "        yÌ‚ = trained_fnn_model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,[0,1])\n",
    "        y == yÌ‚ ? S_training +=1 : nothing\n",
    "    end\n",
    "    correct_prediction_training = (S_training/length(training_headlines_dataset))*100;\n",
    "    println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722e7b2",
   "metadata": {},
   "source": [
    "## Task 3: Construct, Train and Analyze a Sarchasim LSTM\n",
    "In this task, we'll construct a LSTM with a dense output layer and train it. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxLSTMNeuralNetworkModel trainable=(lstm, output); # create a \"namespaced\" of sorts\n",
    "MyLSTMRNNModel() = MyFluxLSTMNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        lstm = Flux.LSTM(number_of_inputs => number_of_hidden_states),  # hidden layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, Ïƒâ‚‚), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "lstmmodel = MyLSTMRNNModel().chain; # Hmmm. lstmmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ceb3f",
   "metadata": {},
   "source": [
    "_Which LSTM optimizer_? The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `Î»` parameter denotes the `learning rate` and `Î²` denotes the momentum parameter. We save information about the optimizer in the `opt_lstm` variable, which will eventually get passed to the training method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0604cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_lstm = let\n",
    "\n",
    "    Î» = 0.50; # TODO: update the learning rate\n",
    "    Î² = 0.10; # TODO: update the momentum parameter\n",
    "    opt_state = Flux.setup(Momentum(Î», Î²), lstmmodel); # opt_state has all the details of the optimizer\n",
    "\n",
    "    # return -\n",
    "    opt_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184cfa4",
   "metadata": {},
   "source": [
    "__Training loop__. `Unhide` the code block below to see the training loop for our Elman RNN. In the training loop, we process the training data for `number_of_epochs::Int` epochs (each epoch is one complete pass through all the training data). The implementation below uses [a few interesting `Flux.jl` specific features](https://github.com/FluxML/Flux.jl). \n",
    "* _Automatic gradient?_: The [`Flux.jl` package](https://fluxml.ai/Flux.jl/stable/) has [the `gradient(...)` method](https://fluxml.ai/Flux.jl/stable/guide/models/basics/#man-taking-gradients) which [uses automatic differentiation](https://arxiv.org/abs/1502.05767) to compute _exact_ gradient values. This is a super interesting feature that removes much of the headache associated with computing the gradient of neural networks.\n",
    "* _Update!?_ The [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) is a [mutating method](https://docs.julialang.org/en/v1/manual/functions/#man-functions), i.e., changes made in the method are visible in the calling scope. In this case, the [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) using the gradient and the optimizer to update the model parameters stored in the model instance. It also updates the `opt_state` data, although what it is doing is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2934236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  lstm = LSTM(151 => 256),              \u001b[90m# 417_792 parameters\u001b[39m\n",
       "  output = Dense(256 => 2, tanh_fast),  \u001b[90m# 514 parameters\u001b[39m\n",
       "  softmax = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 5 arrays, \u001b[39m418_306 parameters, 1.596 MiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_lstm_model = let\n",
    "    \n",
    "    # put the training data in the right format -\n",
    "    x = Array{Float32, 3}(undef, (number_of_inputs, 1, number_of_training_examples)); # initialize\n",
    "    y = Array{Any, 3}(undef, (2, 1, number_of_training_examples)); # initialize\n",
    "\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        x[:, 1, i] = training_headlines_dataset[i][1]; # get the tokenized headline\n",
    "        y[:, 1, i] = training_headlines_dataset[i][2]; # get the label\n",
    "    end\n",
    "\n",
    "\n",
    "    model = lstmmodel; # this is the model we want to train (with default parameters initially)\n",
    "    tree = opt_lstm; # details of the optimizer\n",
    "    for i âˆˆ 1:number_of_epochs\n",
    "        \n",
    "        g = gradient(m -> Flux.logitcrossentropy(m(x), y), model); # Hmmm. This uses automatic differentiation, cool!\n",
    "        (newtree, newmodel) = Flux.update!(tree, model, g[1]) # run the model to convergence(?) - not sure. Docs are bad. Come on Flux.jl!!\n",
    "        \n",
    "        model = newmodel; # reset the model to the new *updated* instance\n",
    "        tree = newtree; # reset the opt tree to the new *updated* instance (not sure what is going on here, Docs bad! Get it together Flux.jl!)\n",
    "    end\n",
    "    model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07ae18",
   "metadata": {},
   "source": [
    "### Correct prediction `training` dataset for the LSTM\n",
    "In the code block below, we pass the pixel data from the image into the `model` instance, compute the predicted label `yÌ‚,` and compare the predicted and actual labels for the `training` dataset.\n",
    "* _Logic_: If the prediction and the actual label agree, we update the `S` variable (a running count of the number of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `training` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "319d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚ = let \n",
    "\n",
    "    # put the training data in the right format -\n",
    "    x = Array{Float32, 3}(undef, (number_of_inputs, 1, number_of_training_examples)); # initialize\n",
    "    yÌ‚ = Vector{OneHotVector{UInt32}}(undef, number_of_training_examples); # initialize\n",
    " \n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        x[:, 1, i] = training_headlines_dataset[i][1]; # get the tokenized headline\n",
    "    end\n",
    "\n",
    "    YÌ‚ = trained_lstm_model(x); # compute the output from evaluating the inoute training data \n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        yáµ¢ = YÌ‚[:,:,i] |> vec # get the i-th output\n",
    "        choice = argmax(yáµ¢) |> z-> number_digit_array[z] |> z-> onehot(z,[0,1])\n",
    "        yÌ‚[i] = choice; # add to the output \n",
    "    end\n",
    "    \n",
    "    yÌ‚ # vector of one-hot vectors for prediction\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 0.0%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i âˆˆ eachindex(training_headlines_dataset)\n",
    "    \n",
    "    y = training_headlines_dataset[i][2];\n",
    "    y == yÌ‚ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_headlines_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a052ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
