{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1fd89e-6844-4ada-870e-d0c796094955",
   "metadata": {},
   "source": [
    "# L13b: Long Short Term Memory (LSTM) Model for Natural Language Text\n",
    "Fill me in\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: Let's take 5 minutes to load and analyze a weather dataset downloaded from [the National Oceanic and Atmospheric Administration (NOAA)](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USC00304174/detail). Once we load the data, we'll do some data wrangling (scaling).\n",
    "* __Task 2: Setup the model structure and training (15 min)__: In this task, we'll construct and train the RNN model, i.e., we'll learn the model parameters, using [the gradient descent with momentum algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize [the mean-squared error (mse) loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.mse). \n",
    "* __Task 3: Play around with the model structure and parameters (20 min)__: In this task, we'll change the model structure, e.g., how many hidden states we have, and include other layers. We'll also change the learning rate and other hyperparameters and look at their effect on the model performance. We'll also look at the effect of changing the number of training epochs and the batch size.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e02a2-1b69-45dd-8216-86196c411c73",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba258d4",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "We'll load a public dataset of headlines that have been curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
    "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
    "\n",
    "The data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We've developed a parser to read the sarcasm data file. The [`corpus(...)` method](src/Files.jl) takes the `path::String` argument (the path to the datafile) and returns a [`MySarcasmRecordCorpusModel` instance](src/Types.jl) which holds the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3de5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = joinpath(_PATH_TO_DATA, \"Sarcasm_Headlines_Dataset_v2.txt\") |> corpus;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095162c",
   "metadata": {},
   "source": [
    "The [`MySarcasmRecordCorpusModel` instance](src/Types.jl) has the fields that are populated when we read the file:\n",
    "* The `records::Dict{Int, MySarcasmRecordModel}` field holds the original records data as a dictionary, where the keys of the dictionary correspond to the headline index, and the values are [instances of the `MySarcasmRecordModel` type](src/Types.jl).\n",
    "* The `tokens::Dict{String, Int64}` field holds the vocabulary computed over the dataset as a dictionary, where the dictionary's keys are the words (called tokens) and the values of the index of the word. We assemble the `tokens` dictionary in alphabetical order. This is initially undefined.\n",
    "* The `inverse::Dict{Int64, String}` field is the inverse of the `tokens` dictionary, where the keys are the token indexes and the values are the tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc249b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e019dbc",
   "metadata": {},
   "source": [
    "Each [`MySarcasmRecordModel` instance](src/Types.jl) has the three fields in the original data records: an `issarcastic::Bool` field holding the label for this record, the `headline::String` field holding the headline and the `article::String` field holding a link to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078a7939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mother comes pretty close to using word streaming correctly\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records[5].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba904f",
   "metadata": {},
   "source": [
    "### Tokenize the headline records\n",
    "In this task, we'll use the corpus model, particularly the `tokens::Dict{String, Int64}` dictionary, to tokenize headlines in our dataset, i.e., convert a text representation into a numerical vector representation. \n",
    "\n",
    "To better understand how this works, let's first examine a single (random) record and tokenize it.  We'll select a random record from the `number_of_records::Int64` possible records [using the built-in `rand(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand), and store it in the `random_test_record::MySarcasmRecordModel` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953b5b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"news website likes to set aside a little ad space to promote own articles\", \"https://local.theonion.com/news-website-likes-to-set-aside-a-little-ad-space-to-pr-1819579398\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_records = corpusmodel.records |> length; # what is going on here?\n",
    "random_test_record = rand(1:number_of_records) |> i -> corpusmodel.records[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575769c",
   "metadata": {},
   "source": [
    "Next, let's call [the `tokenize(...)` method](src/Compute.jl), which takes the `headline::String` that we want to tokenize, and our vocabulary stored in the `tokens::Dict{String, Int64}` dictionary and returns a token vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed40fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Vector{Int64}:\n",
       " 17990\n",
       " 28821\n",
       " 15543\n",
       " 26826\n",
       " 23707\n",
       "  2224\n",
       "   914\n",
       " 15641\n",
       "  1115\n",
       " 24839\n",
       " 26826\n",
       " 20860\n",
       " 19081\n",
       "  2176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(random_test_record.headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488a597",
   "metadata": {},
   "source": [
    "### Hmmm. What happens if a token is not in the dataset?\n",
    "We have created the vocabulary in the `tokens::Dict{String, Int64}` dictionary by analyzing the entire dataset, but suppose we have new samples that aren't in the dataset; what happens then? We've added the `<OOV>` token to our dataset; let's see if that works. \n",
    "* Let's take the headline from the `random_test_record::MySarcasmRecordModel` instance and add something to the end, e.g., `#ilovemyroomba`. we should get the `<OOV>` token at the end of the token vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3904d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = corpusmodel.tokens |> keys |> collect; # what?? We are getting keys (words) and turning into an array\n",
    "\"#ilovemyroomba\" âˆˆ words # fancy way of checking if item is in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b52116",
   "metadata": {},
   "source": [
    "Create a new headline by appending `#ilovemyroomba` to the old headline. String append operations in Julia use [the `*` method](https://docs.julialang.org/en/v1/manual/strings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f9faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"news website likes to set aside a little ad space to promote own articles #ilovemyroomba\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_headline = random_test_record.headline * \" \" * \"#ilovemyroomba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b733c",
   "metadata": {},
   "source": [
    "Tokenize the `new_test_headline::String`, and let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ecdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Vector{Int64}:\n",
       " 17990\n",
       " 28821\n",
       " 15543\n",
       " 26826\n",
       " 23707\n",
       "  2224\n",
       "   914\n",
       " 15641\n",
       "  1115\n",
       " 24839\n",
       " 26826\n",
       " 20860\n",
       " 19081\n",
       "  2176\n",
       "   912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tv = tokenize(new_test_headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c95b",
   "metadata": {},
   "source": [
    "### Compute the maximum pad length\n",
    "Not every headline has the same length, but we want the token vectors to have the same size. Thus, we'll find the longest vectors in the dataset and pad the token vectors to that length. To do that, let's iterate through each headline, compute its size, and then save this length if it is longer than we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3eee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_pad_length = let\n",
    "\n",
    "    max_pad_length = 0; # initialize: we have 0 length\n",
    "    for i âˆˆ 1:number_of_records\n",
    "        test_record_length = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens) |> length; # tokenize, and calc the number of tokens\n",
    "        if (test_record_length > max_pad_length)\n",
    "            max_pad_length = test_record_length; # we've found a new longest headline!\n",
    "        end\n",
    "    end\n",
    "    max_pad_length\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fa686",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all headline samples\n",
    "Finally, now that we have found the `max_pad_length::Int64`, we can tokenize all records using the `max_pad_length::Int64` value as the `pad` value in [the `tokenize(...)` method](src/Compute.jl). \n",
    "* We'll use `right-padding` and will store the tokenized records for each headline in the `token_record_dictionary::Dict{Int64, Array{Int64,1}}` dictionary, where the keys of this dictionary are the record indexes, and the values of the tokenized records (which are of type `Array{Int64,1}.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0c228e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Int64, Vector{Float32}}(24824 => [25877.0, 6523.0, 16124.0, 24452.0, 13458.0, 7184.0, 19562.0, 4737.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 25754 => [20180.0, 17482.0, 12832.0, 18535.0, 19766.0, 25507.0, 3017.0, 20259.0, 28438.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 11950 => [2446.0, 8040.0, 4362.0, 1645.0, 6930.0, 18873.0, 21117.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 1703 => [8236.0, 6707.0, 23707.0, 26826.0, 29323.0, 16580.0, 18615.0, 4068.0, 23172.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 12427 => [17647.0, 22223.0, 26826.0, 12327.0, 20945.0, 29192.0, 28514.0, 21852.0, 8483.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 7685 => [26618.0, 26363.0, 27362.0, 26826.0, 16117.0, 26534.0, 22568.0, 22967.0, 29484.0, 3017.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 18374 => [10013.0, 18296.0, 16586.0, 29538.0, 18115.0, 12257.0, 23172.0, 8929.0, 28693.0, 22342.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 3406 => [11741.0, 15064.0, 1643.0, 6108.0, 26534.0, 20435.0, 18533.0, 6330.0, 13458.0, 20187.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 23970 => [5539.0, 6661.0, 17968.0, 28345.0, 26826.0, 13630.0, 19952.0, 26731.0, 18533.0, 9965.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], 27640 => [16776.0, 18533.0, 1233.0, 11341.0, 4001.0, 26857.0, 15143.0, 5733.0, 18533.0, 19594.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0]â€¦), Dict{Int64, Float32}(24824 => 0.0, 25754 => 0.0, 11950 => 1.0, 1703 => 1.0, 12427 => 0.0, 7685 => 0.0, 18374 => 1.0, 3406 => 0.0, 23970 => 1.0, 27640 => 1.0â€¦))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_record_dictionary, labels = let\n",
    "\n",
    "    # initialize -\n",
    "    token_record_dictionary = Dict{Int64, Array{Float32,1}}();\n",
    "    labels = Dict{Int64, Float32}();\n",
    "    \n",
    "    for i âˆˆ 1:number_of_records\n",
    "        v = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens, \n",
    "                pad = max_pad_length); \n",
    "        l = corpusmodel.records[i].issarcastic; # 1 for sarcastic, 0 for not sarcastic\n",
    "        token_record_dictionary[i] = v .|> Float32; # convert to float32\n",
    "        labels[i] = l .|> Float32; # convert to float32\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    token_record_dictionary, labels\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1100d5",
   "metadata": {},
   "source": [
    "### Save tokenized data and labels to disk\n",
    "We did a bunch of stuff in this example, and we don't want to have to recompute the corpus, token dictionary, etc. So let's save it [in an HDF5 encoded binary file](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    "\n",
    "To start, we specify a path. We'll then write data to disk as a `jld2` (binary) saved file using [the `save(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). This will save the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). The save file is [an HDF5 encoded file format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), which is small (compressed), which is excellent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d053fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    path_to_save_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\"); \n",
    "    save(path_to_save_file, Dict(\"corpus\" => corpusmodel, \n",
    "        \"number_of_records\" => number_of_records, \n",
    "        \"tokenrecorddictionary\" => token_record_dictionary, \n",
    "        \"labeldictionary\" => labels)); # encode, and write\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afcb1",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants that we will use in the exercise. Check the comment next to the value for a description of its meaning, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a170bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 20000;\n",
    "number_of_inputs = max_pad_length; # dimension of the input\n",
    "number_of_hidden_states = 2^10; # number of hidden layers\n",
    "Ïƒâ‚‚ = NNlib.tanh_fast; # activation function\n",
    "number_of_epochs = 1000; # TODO: update how many epochs we want to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79cb3c",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d98234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000-element Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}:\n",
       " ([26617.0, 23295.0, 27980.0, 8295.0, 5553.0, 18533.0, 12047.0, 15828.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([7439.0, 22069.0, 26972.0, 17722.0, 29031.0, 6091.0, 14100.0, 9853.0, 23998.0, 18652.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([8743.0, 29545.0, 28233.0, 869.0, 7418.0, 7808.0, 21629.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([13505.0, 28804.0, 20665.0, 15447.0, 10890.0, 11322.0, 26826.0, 29282.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17429.0, 5812.0, 20655.0, 5563.0, 26826.0, 28097.0, 29278.0, 25501.0, 6408.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17677.0, 28994.0, 13711.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([660.0, 28770.0, 26826.0, 10197.0, 29545.0, 26286.0, 29192.0, 15397.0, 25515.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22445.0, 3878.0, 11455.0, 8271.0, 17853.0, 2192.0, 17524.0, 2192.0, 6441.0, 18533.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([23784.0, 11625.0, 11322.0, 26902.0, 15138.0, 26826.0, 16657.0, 13458.0, 16345.0, 6042.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([15835.0, 18533.0, 19309.0, 14883.0, 26618.0, 23228.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " â‹®\n",
       " ([17975.0, 11578.0, 11631.0, 13474.0, 21371.0, 21238.0, 18533.0, 6398.0, 13669.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22100.0, 28915.0, 28417.0, 10058.0, 20655.0, 24561.0, 22490.0, 958.0, 18312.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([21322.0, 23778.0, 23008.0, 914.0, 15641.0, 18892.0, 18533.0, 14141.0, 26842.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17379.0, 1673.0, 9853.0, 10603.0, 11314.0, 22444.0, 24478.0, 18983.0, 914.0, 15515.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([13013.0, 18533.0, 3561.0, 18723.0, 8484.0, 29123.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([16128.0, 25880.0, 13921.0, 9660.0, 8201.0, 22089.0, 1111.0, 9662.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([3615.0, 8200.0, 7020.0, 6257.0, 3123.0, 20752.0, 28705.0, 17969.0, 958.0, 5539.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22100.0, 17276.0, 11519.0, 26826.0, 17871.0, 29538.0, 26826.0, 19951.0, 13458.0, 2138.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([10800.0, 4821.0, 5577.0, 13458.0, 18652.0, 7174.0, 26826.0, 22404.0, 12423.0, 4633.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_headlines_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "       \n",
    "        token_record = token_record_dictionary[i] .|> Float32; # get the tokenized headline\n",
    "        one_hot_label = onehot(labels[i],[0,1]); # get the label\n",
    "        push!(training_dataset, (token_record, one_hot_label)); # add to the dataset\n",
    "    end\n",
    "    \n",
    "    training_dataset;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851691c5",
   "metadata": {},
   "source": [
    "## Task 2: Construct, Train and Analyze a Sarcasm FFN \n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6aced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxFeedForwardNeuralNetworkModel trainable=(input, output); # create a \"namespaced\" of sorts\n",
    "MyFNNModel() = MyFluxFeedForwardNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        input = Flux.Dense(number_of_inputs => number_of_hidden_states, Ïƒâ‚‚),  # hidden layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, Ïƒâ‚‚), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "fnnmodel = MyFNNModel().chain; # Hmmm. lstmmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbf8c9",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a52604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment below to setup the loss function -\n",
    "fnnloss(yÌ‚, y) = Flux.Losses.logitcrossentropy(yÌ‚, y; agg = mean); # loss for training multiclass classifiers, what is the agg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31ab32",
   "metadata": {},
   "source": [
    "_Which FNN optimizer_? The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `Î»` parameter denotes the `learning rate` and `Î²` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a86def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fnn = let\n",
    "\n",
    "    Î» = 0.50; # TODO: update the learning rate\n",
    "    Î² = 0.10; # TODO: update the momentum parameter\n",
    "    opt_state = Flux.setup(Momentum(Î», Î²), fnnmodel); # opt_state has all the details of the optimizer\n",
    "\n",
    "    # return -\n",
    "    opt_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e697588",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a020209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”Œ Info: Epoch 1 of 1000 completed\n",
      "â”” @ Main /Users/jeffreyvarner/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Labs-Spring-2025/labs/week-13/L13b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sZmlsZQ==.jl:10\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `model` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `model` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Labs-Spring-2025/labs/week-13/L13b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sZmlsZQ==.jl:13"
     ]
    }
   ],
   "source": [
    "trained_fnn_model = let \n",
    "   \n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(fnnmodel, training_headlines_dataset, opt_fnn) do m, x, y\n",
    "            fnnloss(m(x), y)\n",
    "        end\n",
    "\n",
    "        @info \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "\n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(joinpath(_PATH_TO_DATA, \"tmp-model-training-checkpoint.jld2\"), model_state = Flux.state(fnnmodel))  \n",
    "    end\n",
    "\n",
    "    fnnmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722e7b2",
   "metadata": {},
   "source": [
    "## Task 3: Construct, Train and Analyze a Sarchasim LSTM\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b9f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxLSTMNeuralNetworkModel trainable=(lstm, output); # create a \"namespaced\" of sorts\n",
    "MyLSTMRNNModel() = MyFluxLSTMNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        lstm = Flux.LSTM(number_of_inputs => number_of_hidden_states),  # hidden layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, Ïƒâ‚‚), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "lstmmodel = MyLSTMRNNModel().chain; # Hmmm. lstmmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ceb3f",
   "metadata": {},
   "source": [
    "### Training\n",
    "Next, let's set up the model training. One of the shortcomings of [the `Flux.jl` package](https://fluxml.ai/Flux.jl/stable/) is the generally opaque nature of model training. It's a headache, but we've figured it out (maybe). On the other hand, [Flux.jl` package](https://fluxml.ai/Flux.jl/stable/) does handle the model unrolling step for us, so the training works like a feedforward model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0604cb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c5d2165",
   "metadata": {},
   "source": [
    "__Training data__. In the code block below, we specify the training data for our RNN. To simplify our life, we grab the first `number_of_batches::Int` blocks of `number_of_inputs::Int` days of data to train the model.\n",
    "* _What?_ We will train the model of `number_of_batches::Int` blocks of data, e.g., `4` blocks that are `number_of_inputs::Int` days long, e.g., `252-days`. Thus, we are training the model on four years of data in one trading-year increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22254306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151-element Vector{Float32}:\n",
       " 26617.0\n",
       " 23295.0\n",
       " 27980.0\n",
       "  8295.0\n",
       "  5553.0\n",
       " 18533.0\n",
       " 12047.0\n",
       " 15828.0\n",
       "   913.0\n",
       "   913.0\n",
       "     â‹®\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0\n",
       "   913.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_record_dictionary[1] .|> Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9eb0a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cb0bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000-element Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}:\n",
       " ([26617.0, 23295.0, 27980.0, 8295.0, 5553.0, 18533.0, 12047.0, 15828.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([7439.0, 22069.0, 26972.0, 17722.0, 29031.0, 6091.0, 14100.0, 9853.0, 23998.0, 18652.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([8743.0, 29545.0, 28233.0, 869.0, 7418.0, 7808.0, 21629.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([13505.0, 28804.0, 20665.0, 15447.0, 10890.0, 11322.0, 26826.0, 29282.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17429.0, 5812.0, 20655.0, 5563.0, 26826.0, 28097.0, 29278.0, 25501.0, 6408.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17677.0, 28994.0, 13711.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([660.0, 28770.0, 26826.0, 10197.0, 29545.0, 26286.0, 29192.0, 15397.0, 25515.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22445.0, 3878.0, 11455.0, 8271.0, 17853.0, 2192.0, 17524.0, 2192.0, 6441.0, 18533.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([23784.0, 11625.0, 11322.0, 26902.0, 15138.0, 26826.0, 16657.0, 13458.0, 16345.0, 6042.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([15835.0, 18533.0, 19309.0, 14883.0, 26618.0, 23228.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " â‹®\n",
       " ([17975.0, 11578.0, 11631.0, 13474.0, 21371.0, 21238.0, 18533.0, 6398.0, 13669.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22100.0, 28915.0, 28417.0, 10058.0, 20655.0, 24561.0, 22490.0, 958.0, 18312.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([21322.0, 23778.0, 23008.0, 914.0, 15641.0, 18892.0, 18533.0, 14141.0, 26842.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([17379.0, 1673.0, 9853.0, 10603.0, 11314.0, 22444.0, 24478.0, 18983.0, 914.0, 15515.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([13013.0, 18533.0, 3561.0, 18723.0, 8484.0, 29123.0, 913.0, 913.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([16128.0, 25880.0, 13921.0, 9660.0, 8201.0, 22089.0, 1111.0, 9662.0, 913.0, 913.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([3615.0, 8200.0, 7020.0, 6257.0, 3123.0, 20752.0, 28705.0, 17969.0, 958.0, 5539.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])\n",
       " ([22100.0, 17276.0, 11519.0, 26826.0, 17871.0, 29538.0, 26826.0, 19951.0, 13458.0, 2138.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [0, 1])\n",
       " ([10800.0, 4821.0, 5577.0, 13458.0, 18652.0, 7174.0, 26826.0, 22404.0, 12423.0, 4633.0  â€¦  913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0, 913.0], [1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_headlines_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5f732",
   "metadata": {},
   "source": [
    "__Loss function__: Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f449c61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element OneHotVector(::UInt32) with eltype Bool:\n",
       " â‹…\n",
       " 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_headlines_dataset[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184cfa4",
   "metadata": {},
   "source": [
    "__Training loop__. `Unhide` the code block below to see the training loop for our Elman RNN. In the training loop, we process the training data for `number_of_epochs::Int` epochs (each epoch is one complete pass through all the training data). The implementation below uses [a few interesting `Flux.jl` specific features](https://github.com/FluxML/Flux.jl). \n",
    "* _Automatic gradient?_: The [`Flux.jl` package](https://fluxml.ai/Flux.jl/stable/) has [the `gradient(...)` method](https://fluxml.ai/Flux.jl/stable/guide/models/basics/#man-taking-gradients) which [uses automatic differentiation](https://arxiv.org/abs/1502.05767) to compute _exact_ gradient values. This is a super interesting feature that removes much of the headache associated with computing the gradient of neural networks.\n",
    "* _Update!?_ The [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) is a [mutating method](https://docs.julialang.org/en/v1/manual/functions/#man-functions), i.e., changes made in the method are visible in the calling scope. In this case, the [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) using the gradient and the optimizer to update the model parameters stored in the model instance. It also updates the `opt_state` data, although what it is doing is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2934236b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `opt_state` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `opt_state` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Labs-Spring-2025/labs/week-13/L13b/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X66sZmlsZQ==.jl:14"
     ]
    }
   ],
   "source": [
    "trainedmodel = let\n",
    "    \n",
    "    # put the training data in the right format -\n",
    "    x = Array{Float32, 3}(undef, (number_of_inputs, 1, number_of_training_examples)); # initialize\n",
    "    y = Array{OneHotVector{UInt32}, 3}(undef, (1, 1, number_of_training_examples)); # initialize\n",
    "\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        x[:, 1, i] = training_headlines_dataset[i][1]; # get the tokenized headline\n",
    "        # y[:, :, i] = training_headlines_dataset[i][2]; # get the label\n",
    "    end\n",
    "\n",
    "\n",
    "    model = lstmmodel; # this is the model we want to train (with default parameters initially)\n",
    "    tree = opt_state; # details of the optimizer\n",
    "    for i âˆˆ 1:number_of_epochs\n",
    "        \n",
    "        g = gradient(m -> Flux.logitcrossentropy(m(x), y), model); # Hmmm. This uses automatic differentiation, cool!\n",
    "        (newtree, newmodel) = Flux.update!(tree, model, g[1]) # run the model to convergence(?) - not sure. Docs are bad. Come on Flux.jl!!\n",
    "        \n",
    "        model = newmodel; # reset the model to the new *updated* instance\n",
    "        tree = newtree; # reset the opt tree to the new *updated* instance (not sure what is going on here, Docs bad! Get it together Flux.jl!)\n",
    "    end\n",
    "    model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07ae18",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the model\n",
    "Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "319d1ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
