{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1fd89e-6844-4ada-870e-d0c796094955",
   "metadata": {},
   "source": [
    "# L13b: Long Short-Term Memory (LSTM) Model for Natural Language Text\n",
    "In this lab, we'll compare the binary classification performance of a traditional feedforward neural network (FNN) and a recurrent neural network (RNN) using long short-term memory (LSTM) cells on a sarcasm detection task. The dataset is a set of approximately 28,000 news headlines labeled as sarcastic or not sarcastic. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection).\n",
    "\n",
    "__This is a _super hard_ problem__. Classification of sarcasm text is challenging because sarcasm often relies on subtle contextual cues, tone, and cultural knowledge that are difficult for algorithms to detect and interpret accurately.\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we'll load a public dataset of headlines curated as either sarcastic or not sarcastic. Our dataset is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection). After loading the data, we'll tokenize the data (convert text strings to numerical arrays).\n",
    "* __Task 2: Construct, Train, and Analyze a Sarcasm FFN (15 min)__: In this task, we'll construct a feedforward neural network (FNN) model to classify (binary classification task) the sarcasm dataset. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct and train the model. Does an FNN model beat an RNN for this task?\n",
    "* __Task 3: Construct, Train, and Analyze a Sarchasim LSTM (15 min)__: In this task, we'll construct an LSTM with a dense output layer and train it using a collection of labeled headlines. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct and train the model.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e02a2-1b69-45dd-8216-86196c411c73",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba258d4",
   "metadata": {},
   "source": [
    "### Sarcasm Data\n",
    "We'll load a public dataset of headlines that have been curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).\n",
    "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
    "\n",
    "The data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We've developed a parser to read the sarcasm data file. The [`corpus(...)` method](src/Files.jl) takes the `path::String` argument (the path to the data file) and returns a [`MySarcasmRecordCorpusModel` instance](src/Types.jl) that holds the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3de5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = joinpath(_PATH_TO_DATA, \"Sarcasm_Headlines_Dataset_v2.txt\") |> corpus;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095162c",
   "metadata": {},
   "source": [
    "The [`MySarcasmRecordCorpusModel` instance](src/Types.jl) has the fields that are populated when we read the file:\n",
    "* The `records::Dict{Int, MySarcasmRecordModel}` field holds the original records data as a dictionary, where the keys of the dictionary correspond to the headline index, and the values are [instances of the `MySarcasmRecordModel` type](src/Types.jl).\n",
    "* The `tokens::Dict{String, Int64}` field holds the vocabulary computed over the dataset as a dictionary, where the dictionary's keys are the words (called tokens) and the values of the index of the word. We assemble the `tokens` dictionary in alphabetical order. This is initially undefined.\n",
    "* The `inverse::Dict{Int64, String}` field is the inverse of the `tokens` dictionary, where the keys are the token indexes and the values are the tokens (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e019dbc",
   "metadata": {},
   "source": [
    "Each [`MySarcasmRecordModel` instance](src/Types.jl) has the three fields in the original data records: an `issarcastic::Bool` field holding the label for this record, the `headline::String` field holding the headline and the `article::String` field holding a link to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078a7939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mother comes pretty close to using word streaming correctly\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[5].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba904f",
   "metadata": {},
   "source": [
    "### Tokenize the headline records\n",
    "In this task, we'll use the corpus model, particularly the `tokens::Dict{String, Int64}` dictionary, to tokenize headlines in our dataset, i.e., convert a text representation into a numerical vector representation. \n",
    "\n",
    "To better understand how this works, let's first examine a single (random) record and tokenize it.  We'll select a random record from the `number_of_records::Int64` possible records [using the built-in `rand(...)` method](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand), and store it in the `random_test_record::MySarcasmRecordModel` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953b5b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(false, \"palestinians suspicious of alaqsa surveillance promoted by kerry\", \"https://www.huffingtonpost.com/entry/palestinians-suspicious-of-al-aqsa-surveillance-promoted-by-kerry_us_562d5454e4b0443bb564547a\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_records = corpusmodel.records |> length; # what is going on here?\n",
    "random_test_record = rand(1:number_of_records) |> i -> corpusmodel.records[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575769c",
   "metadata": {},
   "source": [
    "Next, let's call [the `tokenize(...)` method](src/Compute.jl), which takes the `headline::String` that we want to tokenize, and our vocabulary stored in the `tokens::Dict{String, Int64}` dictionary and returns a token vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed40fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 19193\n",
       " 25945\n",
       " 18533\n",
       "  1421\n",
       " 25920\n",
       " 20861\n",
       "  4362\n",
       " 14698"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = tokenize(random_test_record.headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488a597",
   "metadata": {},
   "source": [
    "### Hmmm. What happens if a token is not in the dataset?\n",
    "We have created the vocabulary in the `tokens::Dict{String, Int64}` dictionary by analyzing the entire dataset, but suppose we have new samples that aren't in the dataset; what happens then? We've added the `<OOV>` token to our dataset; let's see if that works. \n",
    "* Let's take the headline from the `random_test_record::MySarcasmRecordModel` instance and add something to the end, e.g., `#ilovemyroomba`. we should get the `<OOV>` token at the end of the token vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3904d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = corpusmodel.tokens |> keys |> collect; # what?? We are getting keys (words) and turning into an array\n",
    "\"#ilovemyroomba\" ∈ words # fancy way of checking if item is in array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b52116",
   "metadata": {},
   "source": [
    "Create a new headline by appending `#ilovemyroomba` to the old headline. String append operations in Julia use [the `*` method](https://docs.julialang.org/en/v1/manual/strings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06f9faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"palestinians suspicious of alaqsa surveillance promoted by kerry #ilovemyroomba\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_headline = random_test_record.headline * \" \" * \"#ilovemyroomba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b733c",
   "metadata": {},
   "source": [
    "Tokenize the `new_test_headline::String`, and let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ecdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Vector{Int64}:\n",
       " 19193\n",
       " 25945\n",
       " 18533\n",
       "  1421\n",
       " 25920\n",
       " 20861\n",
       "  4362\n",
       " 14698\n",
       "   912"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = tokenize(new_test_headline, corpusmodel.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c95b",
   "metadata": {},
   "source": [
    "### Compute the maximum pad length\n",
    "Not every headline has the same length, but we want the token vectors to have the same size. Thus, we'll find the longest vectors in the dataset and pad the token vectors to that length. To do that, let's iterate through each headline, compute its size, and then save this length if it is longer than we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3eee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pad_length = let\n",
    "\n",
    "    max_pad_length = 0; # initialize: we have 0 length\n",
    "    for i ∈ 1:number_of_records\n",
    "        test_record_length = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens) |> length; # tokenize, and calc the number of tokens\n",
    "        if (test_record_length > max_pad_length)\n",
    "            max_pad_length = test_record_length; # we've found a new longest headline!\n",
    "        end\n",
    "    end\n",
    "    max_pad_length\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fa686",
   "metadata": {},
   "source": [
    "### Compute the vector representation of all headline samples\n",
    "Finally, now that we have found the `max_pad_length::Int64`, we can tokenize all records using the `max_pad_length::Int64` value as the `pad` value in [the `tokenize(...)` method](src/Compute.jl). \n",
    "* We'll use `right-padding` and will store the tokenized records for each headline in the `token_record_dictionary::Dict{Int64, Array{Int64,1}}` dictionary, where the keys of this dictionary are the record indexes, and the values of the tokenized records (which are of type `Array{Int64,1}.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce0c228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_record_dictionary, labels = let\n",
    "\n",
    "    # initialize -\n",
    "    token_record_dictionary = Dict{Int64, Array{Float32,1}}();\n",
    "    labels = Dict{Int64, Float32}();\n",
    "    \n",
    "    for i ∈ 1:number_of_records\n",
    "        v = tokenize(corpusmodel.records[i].headline, corpusmodel.tokens, \n",
    "                pad = max_pad_length); \n",
    "        l = corpusmodel.records[i].issarcastic; # 1 for sarcastic, 0 for not sarcastic\n",
    "        token_record_dictionary[i] = v .|> Float32; # convert to float32\n",
    "        labels[i] = l .|> Float32; # convert to float32\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    token_record_dictionary, labels\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1100d5",
   "metadata": {},
   "source": [
    "### Save tokenized data and labels to disk\n",
    "We did a bunch of stuff in this example, and we don't want to have to recompute the corpus, token dictionary, etc. So let's save it [in an HDF5 encoded binary file](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    "* _Details_: To start, we specify a path. We'll then write data to disk as a `jld2` (binary) saved file using [the `save(...)` method exported by the FileIO.jl package](https://github.com/JuliaIO/FileIO.jl). This will save the data as a [Julia `Dict` type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict). The save file is [an HDF5 encoded file format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), which is small (compressed), which is excellent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d053fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    path_to_save_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\"); \n",
    "    save(path_to_save_file, Dict(\"corpus\" => corpusmodel, \n",
    "        \"number_of_records\" => number_of_records, \n",
    "        \"tokenrecorddictionary\" => token_record_dictionary, \n",
    "        \"labeldictionary\" => labels)); # encode, and write\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afcb1",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants we will use in the exercise. Check the comment next to the value for a description of its meaning, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a170bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 10000; # how many training examples?\n",
    "number_of_inputs = max_pad_length; # dimension of the input\n",
    "number_of_hidden_states = 2^10; # dimension of hidden state memory\n",
    "σ₂ = NNlib.tanh_fast; # activation function\n",
    "number_of_epochs = 50; # TODO: update how many epochs we want to train for\n",
    "number_digit_array = range(0,length=2,step=1) |> collect; # numbers 0 ... 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79cb3c",
   "metadata": {},
   "source": [
    "__Build a training dataset__. The training data will consist of a vector of tuples, where the first element is the tokenized headline and the second is the label [in OneHot format](https://en.wikipedia.org/wiki/One-hot). \n",
    "* We save the training data in the `training_headlines_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable. This vector will have `number_of_training_examples::Int` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d98234",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_headlines_dataset = let\n",
    "\n",
    "    # generate random index set -\n",
    "    random_training_index_set = Set{Int64}();\n",
    "    \n",
    "    # Uncomment me for random selection -\n",
    "    # should_stop_loop = false;\n",
    "    # counter = 0;\n",
    "    # while (should_stop_loop == false)\n",
    "    #     i = rand(1:number_of_records);\n",
    "    #     push!(random_training_index_set, i);\n",
    "\n",
    "    #     if (length(random_training_index_set) ≥ number_of_training_examples)\n",
    "    #         should_stop_loop = true; # ok to stop the loop\n",
    "    #     else\n",
    "    #         counter += 1;\n",
    "    #     end\n",
    "    # end\n",
    "    # random_training_index_array = random_training_index_set |> collect |> sort;\n",
    "\n",
    "    # Uncomment me for sequential selection -\n",
    "    random_training_index_array = range(1,\n",
    "        length=number_of_training_examples, step=1) |> collect; # sequential selection\n",
    "    \n",
    "    \n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for index ∈ eachindex(random_training_index_array)\n",
    "        i = random_training_index_array[index];\n",
    "        token_record = token_record_dictionary[i] .|> Float32; # get the tokenized headline\n",
    "        one_hot_label = onehot(labels[i],number_digit_array); # get the label\n",
    "        push!(training_dataset, (token_record, one_hot_label)); # add to the dataset\n",
    "    end\n",
    "    \n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851691c5",
   "metadata": {},
   "source": [
    "## Task 2: Construct, Train, and Analyze a Sarcasm FFN \n",
    "In this task, we'll construct a feedforward neural network (FNN) model to classify the sarcasm dataset. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct and train the model. \n",
    "\n",
    "Let's start by building the model, which we'll store in the `fnnmodel::Chain` variable, where the [`Chain` type is exported by the `Flux.jl` package](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Chain). One of the nice things about this formulation is that it is easy to abb (or subtract) layers from the FNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6aced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxFeedForwardNeuralNetworkModel trainable=(input, output); # create a \"namespaced\" of sorts\n",
    "MyFNNModel() = MyFluxFeedForwardNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        input = Flux.Dense(number_of_inputs => number_of_hidden_states, σ₂),  # hidden layer\n",
    "        # middle = Flux.Dense(number_of_hidden_states => number_of_hidden_states, σ₂), # output layer\n",
    "        # final = Flux.Dense(number_of_hidden_states => number_of_hidden_states, σ₂), # output layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, σ₂), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "fnnmodel = MyFNNModel().chain; # Hmmm. fnnmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31ab32",
   "metadata": {},
   "source": [
    "_Which FNN optimizer do we use_? The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. \n",
    "* We save information about the optimizer in the `opt_fnn` variable, which will eventually be passed to the feedforward network training loop. The `opt_fnn` variable is a complex data structure composed [of `NamedTuples` instances](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a86def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fnn = let\n",
    "\n",
    "    λ = 0.50; # TODO: update the learning rate\n",
    "    β = 0.10; # TODO: update the momentum parameter\n",
    "    opt_state = Flux.setup(Momentum(λ, β), fnnmodel); # opt_state has all the details of the optimizer\n",
    "\n",
    "    # return -\n",
    "    opt_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e697588",
   "metadata": {},
   "source": [
    "__Training loop__. The training loop for an FNN is simpler than that of an RNN. If the `should_we_train::Bool` flag is `true,` then we process all the batches in the `training_headlines_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` dataset `number_of_epochs::Int` times. The updated model instance is returned in the `trained_fnn_model::Chain` variable.\n",
    "* _What happens if the training flag is false?_ If the `should_we_train::Bool` flag is set to `false,` we load a previously saved model state and use that for computation. If we change the model, then this previous state is no longer valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a020209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 10 of 50 completed\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 20 of 50 completed\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 30 of 50 completed\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 40 of 50 completed\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mEpoch 50 of 50 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(151 => 1024, tanh_fast),  \u001b[90m# 155_648 parameters\u001b[39m\n",
       "  output = Dense(1024 => 2, tanh_fast),  \u001b[90m# 2_050 parameters\u001b[39m\n",
       "  softmax = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m157_698 parameters, 616.211 KiB."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_fnn_model = let \n",
    "   \n",
    "    should_we_train = true; # TODO: set this flag to {true | false}\n",
    "    model = fnnmodel;\n",
    "    if (should_we_train == true)\n",
    "        for i = 1:number_of_epochs\n",
    "        \n",
    "            # train the model -\n",
    "            Flux.train!(model, training_headlines_dataset, opt_fnn) do m, x, y\n",
    "                Flux.Losses.logitcrossentropy(m(x), y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "            end\n",
    "    \n",
    "            if (rem(i,10) == 0)\n",
    "                @info \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "            end\n",
    "    \n",
    "            # save the state of the model, in case something happens. We can reload from this state\n",
    "            jldsave(joinpath(_PATH_TO_DATA, \"tmp-model-training-checkpoint.jld2\"), model_state = Flux.state(model))  \n",
    "        end\n",
    "    else\n",
    "\n",
    "        # if we don't train: load up a previous model\n",
    "        model_state = JLD2.load(joinpath(_PATH_TO_DATA, \"tmp-model-training-checkpoint.jld2\"), \"model_state\");\n",
    "        Flux.loadmodel!(model, model_state);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347d8a9-19e4-4f90-956c-d0015a8c7186",
   "metadata": {},
   "source": [
    "### How well does the FNN classify the `training` dataset?\n",
    "In the code block below, we pass the headline training dataset into the `fnnmodel::Chain` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels for the `training_headlines_dataset` dataset.\n",
    "* __Logic__: If the prediction and the actual label agree, we update the `S` variable (a running count of the number of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `training_headlines_dataset` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db106e0c-a632-4aaa-88f5-41fc85170d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 51.81%\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    S_training = 0;\n",
    "    for i ∈ eachindex(training_headlines_dataset)\n",
    "    \n",
    "        x = training_headlines_dataset[i][1];\n",
    "        y = training_headlines_dataset[i][2];\n",
    "        ŷ = trained_fnn_model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,[0,1])\n",
    "        y == ŷ ? S_training +=1 : nothing\n",
    "    end\n",
    "    correct_prediction_training = (S_training/length(training_headlines_dataset))*100;\n",
    "    println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722e7b2",
   "metadata": {},
   "source": [
    "## Task 3: Construct, Train, and Analyze a Sarchasim LSTM\n",
    "In this task, we'll construct an LSTM with a dense output layer and train it using a collection of labeled headlines. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to construct and train the model.\n",
    "\n",
    "Let's start by building the LSTM model. This follows much the same structure as the FNN model above, but now we have (as our first layer) an [`LSTM` block type exported by `Flux.jl`](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.LSTM). This layer takes a text vector $\\mathbf{x}$ and returns the _hidden_ state vector $\\mathbf{h}_{t}$. We run this through a dense output layer and then [a `softmax(...)` method](https://fluxml.ai/NNlib.jl/dev/reference/#Softmax) to compute the probability of the binary label (sarcastic, or not sarcastic). \n",
    "\n",
    "We store the LSTM model in the `lstmmodel::Chain` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b9f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxLSTMNeuralNetworkModel trainable=(lstm, output); # create a \"namespaced\" of sorts\n",
    "MyLSTMRNNModel() = MyFluxLSTMNeuralNetworkModel( # a strange type of constructor\n",
    "    Flux.Chain(\n",
    "        lstm = Flux.LSTM(number_of_inputs => number_of_hidden_states),  # hidden layer\n",
    "        output = Flux.Dense(number_of_hidden_states => 2, σ₂), # output layer\n",
    "        softmax = NNlib.softmax # softmax layer\n",
    "    )\n",
    ");\n",
    "lstmmodel = MyLSTMRNNModel().chain; # Hmmm. lstmmodel is callable? (Yes, because of a cool Julia syntax quirk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ceb3f",
   "metadata": {},
   "source": [
    "_Which LSTM optimizer_? The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. \n",
    "* We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_lstm` variable, which will eventually get passed to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0604cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_lstm = let\n",
    "\n",
    "    λ = 0.50; # TODO: update the learning rate\n",
    "    β = 0.10; # TODO: update the momentum parameter\n",
    "    opt_state = Flux.setup(Momentum(λ, β), lstmmodel); # opt_state has all the details of the optimizer\n",
    "\n",
    "    # return -\n",
    "    opt_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184cfa4",
   "metadata": {},
   "source": [
    "__Training loop__. `Unhide` the code block below to see the training loop for our Elman RNN. In the training loop, we process the training data for `number_of_epochs::Int` epochs (each epoch is one complete pass through all the training data). The implementation below uses [a few interesting `Flux.jl` specific features](https://github.com/FluxML/Flux.jl). \n",
    "* _Automatic gradient?_: The [`Flux.jl` package](https://fluxml.ai/Flux.jl/stable/) has [the `gradient(...)` method](https://fluxml.ai/Flux.jl/stable/guide/models/basics/#man-taking-gradients) which [uses automatic differentiation](https://arxiv.org/abs/1502.05767) to compute _exact_ gradient values. This is a super interesting feature that removes much of the headache associated with computing the gradient of neural networks.\n",
    "* _Update!?_ The [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) is a [mutating method](https://docs.julialang.org/en/v1/manual/functions/#man-functions), i.e., changes made in the method are visible in the calling scope. In this case, the [`update!(...)` method](https://fluxml.ai/Flux.jl/stable/reference/training/reference/#Optimisers.update!) using the gradient and the optimizer to update the model parameters stored in the model instance. It also updates the `opt_state` data, although what it is doing is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2934236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  lstm = LSTM(151 => 1024),             \u001b[90m# 4_816_896 parameters\u001b[39m\n",
       "  output = Dense(1024 => 2, tanh_fast),  \u001b[90m# 2_050 parameters\u001b[39m\n",
       "  softmax = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 5 arrays, \u001b[39m4_818_946 parameters, 18.383 MiB."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lstm_model = let\n",
    "    \n",
    "    # put the training data in the right format -\n",
    "    x = Array{Float32, 3}(undef, (number_of_inputs, 1, number_of_training_examples)); # initialize\n",
    "    y = Array{Any, 3}(undef, (2, 1, number_of_training_examples)); # initialize\n",
    "\n",
    "    # package the data up\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        x[:, 1, i] = training_headlines_dataset[i][1]; # get the tokenized headline\n",
    "        y[:, 1, i] = training_headlines_dataset[i][2]; # get the label\n",
    "    end\n",
    "\n",
    "    # training loop: Notice this is a hassle compared to the FNN loop. \n",
    "    model = lstmmodel; # This is the model we want to train (with default parameters initially)\n",
    "    tree = opt_lstm; # details of the optimizer that we'll use\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        \n",
    "        g = gradient(m -> Flux.logitcrossentropy(m(x), y), model); # Hmmm. This uses automatic differentiation, cool!\n",
    "        (newtree, newmodel) = Flux.update!(tree, model, g[1]) # run the model to convergence(?) - not sure. Docs are bad. Come on Flux.jl!!\n",
    "        \n",
    "        model = newmodel; # reset the model to the new *updated* instance\n",
    "        tree = newtree; # reset the opt tree to the new *updated* instance (not sure what is going on here, Docs bad! Get it together Flux.jl!)\n",
    "    end\n",
    "    model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07ae18",
   "metadata": {},
   "source": [
    "### How well does the LSTM classify the `training` dataset?\n",
    "In the code block below, we pass the headline training dataset into the `lstmmodel::Chain` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels for the `training_headlines_dataset` dataset.\n",
    "* __Logic__: If the prediction and the actual label agree, we update the `S` variable (a running count of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `training_headlines_dataset` dataset.\n",
    "\n",
    "The LSTM model has _weird_ data needs, so let's first compute the `ŷ` vector, i.e., the _predicted_ labels for the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "319d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,Ŷ = let \n",
    "\n",
    "    # put the training data in the right format -\n",
    "    x = Array{Float32, 3}(undef, (number_of_inputs, 1, number_of_training_examples)); # initialize\n",
    "    ŷ = Vector{OneHotVector{UInt32}}(undef, number_of_training_examples); # initialize\n",
    "\n",
    "    # package the tokenized headlines\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        x[:, 1, i] = training_headlines_dataset[i][1]; # get the tokenized headline\n",
    "    end\n",
    "\n",
    "    # output\n",
    "    Ŷ = trained_lstm_model(x); # compute the output from evaluating the input training data \n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        yᵢ = Ŷ[:,:,i] |> vec # get the i-th output\n",
    "        choice = argmax(yᵢ) |> z-> number_digit_array[z] |> z-> onehot(z,[0,1])\n",
    "        ŷ[i] = choice; # add to the output \n",
    "    end\n",
    "    \n",
    "    ŷ, Ŷ # ŷ: vector of one-hot vectors for prediction, Ŷ is the original output from the LSTM\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94994c64-bab0-45ea-9c31-ff00dfd10fd2",
   "metadata": {},
   "source": [
    "Compute the performance on the training data for the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20aa32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 53.73%\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    S_training = 0;\n",
    "    for i ∈ eachindex(training_headlines_dataset)\n",
    "        \n",
    "        y = training_headlines_dataset[i][2];\n",
    "        y == ŷ[i] ? S_training +=1 : nothing\n",
    "    end\n",
    "    correct_prediction_training = (S_training/length(training_headlines_dataset))*100;\n",
    "    println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a052ca1",
   "metadata": {},
   "source": [
    "## Next time\n",
    "In the lecture `L13c` (and associated lab), we'll introduce another (more advanced) approach for modeling long sequences based on [Time Invariant Linear State Space models](https://en.wikipedia.org/wiki/State-space_representation). We'll consider two different approaches to modeling long sequences:\n",
    "\n",
    "* [Gu, A., Goel, K., & Ré, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. ArXiv, abs/2111.00396.](https://arxiv.org/abs/2111.00396)\n",
    "* [Gu, A., Johnson, I., Timalsina, A., Rudra, A., & Ré, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. ArXiv, abs/2206.12037.](https://arxiv.org/abs/2206.12037)\n",
    "* [Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. ArXiv, abs/2312.00752.](https://arxiv.org/abs/2312.00752)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
