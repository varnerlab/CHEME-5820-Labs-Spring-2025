{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c027c225-eebb-48e6-b2f6-b99efc6c7343",
   "metadata": {},
   "source": [
    "# L16b: Developing a Deep Q-learning (DQN) Formulation Agent\n",
    "In this lab, we'll examine a Deep Q-learning (DQN) agent whose objective is to learn to mix $m$ different materials to maximize the utility of the resulting mixture. Each component in the mix has a different coefficient in the utility function and a different unit cost.\n",
    "\n",
    "* __Scenario__: You are tasked with formulating a product composed of $m$ components, where each component has a unit price $C_{i}$ (which is variable). The product is constrained to cost less than or equal to a specified budget. The states for our problem will be continuous values $n_{i}$, which are the abundance of component $i$. The actions will be the increase/decrease of component $i$.\n",
    "* __Agent__: We'll use a DQN agent to explore the composition space, where the _Utility_ of a particular composition (random variable) is measured using a _Utility function_, subject to a budget constraint. \n",
    "\n",
    "\n",
    "### Tasks\n",
    "Before you start, execute the `Run All Cells` command to check if you have any code or setup issues - let's get those fixed!\n",
    "\n",
    "* __Task 1: Setup, Data, Constants (15 min)__: In this task, we set up the computational environment, load the necessary packages, and prepare the `world(...)` function for our formulation problem. We will also define any constants we use throughout the problem set.\n",
    "* __Task 2: Set up the Context, Main, Target Networks, and the Replay Buffer (20 min)__: In this task, we will build several required models for our deep Q-learning agent. We know the agent will have a _main_ and a _target_ network, and we will also need a replay buffer to store the agent's experiences. In addition, we'll need a context model.\n",
    "* __Task 3: Let's watch the DQN agent in action (15 min)__: In this task, we train the agent briefly, then give it some random vectors to see what it says. However, before we do that, let's review the training process.\n",
    "\n",
    "Tests (and other checkpoints) are located throughout the notebook to help us determine if things are running correctly. Let's go! \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9cabb",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62897123",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e39983",
   "metadata": {},
   "source": [
    "Next, let's build the `world(...)` function. \n",
    "* _What does this function do_? The `world(...)` function takes a state `s::Array{Float32,1}` and action vector `a::Array{Float32,1}` where the elements of the state vector `s::Array{Float32,1}` describes the composition vector, and the action vector `a::Array{Float32,1}` describes the increase/decrease of a component of the mixture. The `world(...)` function also takes a `context` model, which encapsulates the experimental environment, including the budget constraint and the penalty for exceeding it. More on the `context` in `Task 2`.\n",
    "\n",
    "We've assumed a _linear utility function_ for the personal shopper problem, where the utility is a linear combination of the items chosen minus a penalty for exceeding the budget. The utility function $U:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}$ is defined as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "U_{\\lambda}(\\mathbf{n},\\mathbf{\\gamma}) = \\sum_{i=1}^{N} \\gamma_{i}\\cdot n_i - \\lambda \\cdot \\left[\\max(0, \\sum_{i=1}^{N} c_i \\cdot n_{i} - B)\\right]^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\gamma_{i}$ is the marginal utility of option $i$ (unkown to the agent, only known to the world), while the term $n_i$ denotes the amount of component $i$ in the mixture,  The quadratic penalty term is subtracted from the utility if the total cost exceeds the budget, where $c_i$ is the unit cost of item $i$.\n",
    "\n",
    "__Hmmm__. Sometimes, we are uncertain about the benefit gained from the abundance of $i$, and the unit price of component $i$ is variable. Let's add some randomness to the problem. In the presence of uncertainty, the utility function becomes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "U_{\\lambda}(\\mathbf{n},\\mathbf{\\gamma}) = \\sum_{i=1}^{N} (\\gamma_{i} + \\sigma_{i} \\cdot Z_i) \\cdot n_i - \\lambda \\cdot \\left[\\max\\left(0, \\left\\{\\sum_{i=1}^{N} (c_i + \\sigma^{\\prime}_{i} \\cdot Z_i) \\cdot n_{i} - B\\right\\}\\right)\\right]^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $Z_i \\sim \\mathcal{N}(0,1)$ is a random variable drawn from a standard normal distribution for each item $i$, and $\\sigma^{\\star}_{i}\\geq{0}$ denotes the strength of the uncertainty associated with good $i$ (hyperparameter set by us). This adds a stochastic element to the utility function, making it more realistic when the benefits or costs associated with an item is uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4134a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function world(s::Array{Float32,1}, a::Array{Float32,1}, context::MyDQNworldContextModel)\n",
    "\n",
    "    # initialize -\n",
    "    γ = context.γ; # consumer preferences (unknown to agent)\n",
    "    σ = context.σ; # noise in utility calculation (unknown to agent)\n",
    "    B = context.B; # max budget (unknown to agent)\n",
    "    C = context.C; # unit costs of goods (unknown to agent)\n",
    "    λ = context.λ; # sensitivity to the budget\n",
    "    Z = context.Z; # noise model\n",
    "    number_of_goods = context.m; # number of possible combinations\n",
    "\n",
    "    # compute the reward for this choice -\n",
    "    Ū = 0.0; # initial utility\n",
    "    BC = 0.0; # initial budget constraint\n",
    "    for i ∈ 1:number_of_goods\n",
    "        \n",
    "        nᵢ = s[i]; # this is the quantity purchased of good aᵢ in category i\n",
    "        Cᵢ = C[i]; # cost of chosen good in category i\n",
    "        γᵢ = γ[i]; # preference of good in category i\n",
    "   \n",
    "        # update the utility and the budget constraint -\n",
    "        Ū += γᵢ*(nᵢ + σ[i,1]*rand(Z)); # compute the utility for this good, with noise. We'll use a linear utility model\n",
    "        BC += nᵢ*(Cᵢ + σ[i,2]*rand(Z)); # compute the budget constraint -\n",
    "    end\n",
    "\n",
    "    # Compute the utility with the budget constraint\n",
    "    U = (Ū - λ*max(0.0, (BC - B))^2) .|> Float32 ; # use a penalty method to capture budget constraint\n",
    "\n",
    "    # compute the next state -\n",
    "    s′ = (s .+ (a.*s)); # update the state with the action taken to get the next state\n",
    "\n",
    "    # check: for NaN replace with zero\n",
    "    s′ = max.(s′, 1e-6); # ensure that the state is non-negative\n",
    "    \n",
    "    # implement state constraints -\n",
    "    s′ = s′ .|> Float32; # ensure that the state is non-negative\n",
    "    \n",
    "    # return to caller -\n",
    "    return s′, U; # return the next state and the reward\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8599d",
   "metadata": {},
   "source": [
    "__Constants__: Set constants we'll use in the subsequent tasks. See the comment beside the value for a description of what it is, its permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e599fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 12; # TODO: Let's consider 12 different items that we need to mix together\n",
    "number_of_actions = 2*K; # TODO: number of actions (2 for each item,increase/decrease)\n",
    "number_of_hidden_states = 2^K; # TODO: number of hidden states (2^K made this up)\n",
    "number_of_episodes = 2; # TODO: number of episodes (2^K\n",
    "max_number_of_iterations = 2^14; # TODO: number of rounds for each decision task (should be geq 2^{K})\n",
    "budget = 1000.0; # TODO: Budget for agent, assume 100 USD. We can change this later if we want\n",
    "buffersize = 2^10; # TODO: buffer size for the agent\n",
    "B = 2^6; # TODO: minibatch size for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bb7bb",
   "metadata": {},
   "source": [
    "## Task 2: Setup the Context, Main, Target Networks, and the Replay Buffer\n",
    "In this task, we will build several required models for our deep Q-learning agent. We know the agent will have a _main_ and a _target_ network, and we will also need a replay buffer to store the agent's experiences. In addition, we'll need a context model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8deee",
   "metadata": {},
   "source": [
    "### Context Model\n",
    "The context model is an instance of [the `MyDQNworldContextModel` type](src/Types.jl). It holds various parameters that will be used in the `world(...)` function that we developed in Task 1. We save our context model instance in the `contextmodel::MyDQNworldContextModel` variable.\n",
    "\n",
    "Let's walk through what we are saying here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff621f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextmodel = let\n",
    "\n",
    "    # initialize -\n",
    "    context = nothing; # initialize the context variable to nothing; this variable will be used to store the context model\n",
    "    γ = Array{Float32,1}(undef, K); # consumer preferences (unknown to agent)\n",
    "    σ = Array{Float32,2}(undef, K, 2); # noise in utility calculation (unknown to agent). First col is noise for good, seccond col is noise for price\n",
    "    C = Array{Float32,1}(undef, K); # unit costs of goods (unknown to agent)\n",
    "    Z = Normal(0,1); # use a standard normal distribution for the noise model; this can be changed to any distribution as required\n",
    "    λ = 0.0; # sensitivity to the budget constraint λ ≥ 0. If zero, then no penalty for budget constraint violation.\n",
    "\n",
    "    # set the parameters -\n",
    "    for i ∈ 1:K\n",
    "        # Assigning values for γ, σ, and C for each good and price in the context model\n",
    "        # For simplicity, let's assume we have K goods with equal preference\n",
    "        # This can be customized as per the requirements of the simulation\n",
    "        rem(i, 2) == 0 ? γ[i] = 1.0 : γ[i] = -1.0; # Fancy! if i is even, then γ[i] = 1.0, else γ[i] = -1.0       \n",
    "        σ[i,:] .= 0.1; # uniform uncertainty for all goods and prices, this can be adjusted based on the specific needs of the simulation\n",
    "        C[i] = 10.0 + 10.0 * (i - 1); # linearly increasing costs for goods, this can be customized as per the requirement\n",
    "    end\n",
    "\n",
    "    # TODO: Uncomment the code below to build the context model -\n",
    "    # build a context model with the required parameters -\n",
    "    context = build(MyDQNworldContextModel, (\n",
    "        γ = γ, # consumer preferences (unknown to agent)\n",
    "        σ = σ, # noise in utility calculation (unknown to agent)\n",
    "        B = B, # max budget (unknown to agent)\n",
    "        C = C, # unit costs of goods (unknown to agent)\n",
    "        λ = λ, # sensitivity to the budget\n",
    "        Z = Z, # noise model\n",
    "        m = K, # number of components\n",
    "    )); # build the context\n",
    "\n",
    "    # return the model -\n",
    "    context;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1423cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: TODO: What's in the context model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32ebb403-e21f-4bfa-88a4-824f00a472d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Vector{Float32}:\n",
       "  10.0\n",
       "  20.0\n",
       "  30.0\n",
       "  40.0\n",
       "  50.0\n",
       "  60.0\n",
       "  70.0\n",
       "  80.0\n",
       "  90.0\n",
       " 100.0\n",
       " 110.0\n",
       " 120.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextmodel.C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d4d9b",
   "metadata": {},
   "source": [
    "#### Main and Target Networks\n",
    "The main network is used to select actions, while the target network is used to evaluate the actions taken by the main network. These networks have the same architecture but are updated at different rates. The main network is updated more frequently, while the target network is updated less frequently to provide a stable target for the Q-value updates.\n",
    "\n",
    "__Implementation__: For the main and target models, let's build an empty model with default (random) parameter values but a fixed structure. The number and dimension of the layers and the activation functions for each layer are specified when we build the model (but we'll update the parameters during training).\n",
    "* _Library_: We use [the `Flux.jl` machine learning library](https://github.com/FluxML/Flux.jl) to construct the neural network model. The model will have three layers: the input layer is a `K` $\\times$ $2^{K}$ layer with [tanh activation functions](https://fluxml.ai/Flux.jl/stable/reference/models/activation/#NNlib.tanh_fast), the hidden layer is a $2^{K}$ $\\times$ $\\dim\\mathcal{A}$ layer and the output layer is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "* _Syntax_: The [`Flux.jl` package](https://github.com/FluxML/Flux.jl) uses some next level syntax. The model is built using [the `Chain` function](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Chain), which takes a list of layers as input. Each layer is defined using the [`Dense` type](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Dense) (in this case), which takes the number of input and output neurons as arguments. The activation function is an additional argument to [the `Dense` type](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Dense). The final layer uses [the `softmax(...)` method exported by the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/reference/#Softmax) to produce a probability distribution over the classes.\n",
    "\n",
    "We save the main network in the `M::Chain` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7670c0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(12 => 4096, tanh_fast),  \u001b[90m# 53_248 parameters\u001b[39m\n",
       "  hidden = Dense(4096 => 24, tanh_fast),  \u001b[90m# 98_328 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m151_576 parameters, 592.297 KiB."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = let \n",
    "    \n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(K, number_of_hidden_states, tanh_fast),  # layer 1\n",
    "            hidden = Dense(number_of_hidden_states, number_of_actions, tanh_fast), # layer 2\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    model = MyModel().chain;\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fa4ef",
   "metadata": {},
   "source": [
    "The target network is a copy of the main network, but its parameters are updated less frequently. The target network is updated by copying the parameters from the main network every few episodes.\n",
    "\n",
    "We save the target network in the `T::Chain` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277f89e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(12 => 4096, tanh_fast),  \u001b[90m# 53_248 parameters\u001b[39m\n",
       "  hidden = Dense(4096 => 24, tanh_fast),  \u001b[90m# 98_328 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m151_576 parameters, 592.297 KiB."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = let\n",
    "\n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(K, number_of_hidden_states, tanh_fast),  # layer 1\n",
    "            hidden = Dense(number_of_hidden_states, number_of_actions, tanh_fast), # layer 2\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    model = MyModel().chain;\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad92112",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "The replay buffer is a data structure that stores the agent's experiences. It is used to sample random batches of experiences for training the main network. We will implement a _vanilla DQN_ agent whose replay buffer is a [simple circular buffer](https://en.wikipedia.org/wiki/Circular_buffer#:~:text=In%20computer%20science%2C%20a%20circular,easily%20to%20buffering%20data%20streams.). \n",
    "* _What goes into the buffer_? The replay buffer stores the agent's experiences in the form of tuples $(s, a,r,s^{\\prime})$, where $s$ is the current state, $a$ is the action taken, $r$ is the reward received, $s^{\\prime}$ is the next state.\n",
    "* _Implementation_? We'll use [the circular buffer implementation exported by the `DataStructures.jl` package](https://github.com/JuliaCollections/DataStructures.jl). The [`CircularBuffer` type](https://juliacollections.github.io/DataStructures.jl/stable/circ_buffer/) is a fixed-size buffer that overwrites the oldest elements when it becomes full. The size of the buffer is set by the `buffersize::Int64`. We can generate $B$ random samples, i.e., our training minibatch from the replay buffer using [an extended `rand(...)` function](https://docs.julialang.org/en/v1/stdlib/Random/#Base.rand). \n",
    "\n",
    "The agent builds its replay buffer, so let's try a sample `CircularBuffer` to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "992f85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_replay_buffer = CircularBuffer{Int}(10); # TODO: create a circular buffer that holdes int    \n",
    "for i ∈ 1:buffersize\n",
    "    push!(test_replay_buffer, i); # TODO: fill the buffer with some random values\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08da6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: TODO: What happens if we push more than the buffer size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b711970d-ef65-4987-add6-e76cba5e217b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element CircularBuffer{Int64}:\n",
       " 1015\n",
       " 1016\n",
       " 1017\n",
       " 1018\n",
       " 1019\n",
       " 1020\n",
       " 1021\n",
       " 1022\n",
       " 1023\n",
       " 1024"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04071f23-dcf8-4690-b075-1257b6315338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element CircularBuffer{Int64}:\n",
       " 1022\n",
       " 1023\n",
       " 1024\n",
       "    6\n",
       "   30\n",
       "    1\n",
       "   43\n",
       "   29\n",
       "    2\n",
       "   27"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(test_replay_buffer, rand(1:50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32f98c1f-f521-48c6-93b6-bce36f7ba0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       "    1\n",
       " 1023\n",
       "   29\n",
       "   43\n",
       "   27"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand(test_replay_buffer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f819c",
   "metadata": {},
   "source": [
    "### Learning agent model\n",
    "Finally, we need to build a model for the learning agent. The agent model is an instance of [the `MyDQNLearningAgentModel` type](src/Types.jl). We save our problem model instance in the `agentmmodel::MyDQNworldProblemModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9203aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentmodel = let\n",
    "\n",
    "    model = build(MyDQNLearningAgentModel, (\n",
    "        mainnetwork = M, # the main network\n",
    "        targetnetwork = T, # the target network\n",
    "        number_of_actions = number_of_actions, # number of actions\n",
    "        number_of_inputs = K, # number of inputs\n",
    "        Δ = 0.1, # state peturbation \n",
    "    )); # build the agent\n",
    "\n",
    "    # return -\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e7eee",
   "metadata": {},
   "source": [
    "What is in the agent model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b3be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checkpoint: TODO: make sure we understand what data our agent has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdd70f",
   "metadata": {},
   "source": [
    "## Task 3: Let's watch the DQN agent in action.\n",
    "In this task, we train the agent briefly, then give it some random vectors to see what it says. However, before we do that, let's review the training process.\n",
    "\n",
    "#### DQN Training Algorithm\n",
    "__Initialize__ the parameters of the main Q-network $Q_{\\theta}(s)$ and the target Q-network $Q^{\\prime}_{\\theta^{-}}(s)$ to random values. Initialize a (potentially infinite) replay buffer $\\mathcal{B}$. Set the hyperparameters: the learning rate $\\alpha$, the discount factor $\\gamma$, the exploration rate $\\epsilon_{t}$, the minimum number of experiences in the replay buffer $B$, and the parameter update count $\\mathcal{C}$.\n",
    "- For each episode, initialize the state to $s_0$ and:\n",
    "   - For each time step $t=1,\\ldots,T$:\n",
    "        1. Role a random number $p\\in[0,1]$. If $p\\leq\\epsilon_{t}$, choose a random (uniform) action $a_{t}\\in\\mathcal{A}$. Otherwise, choose a greedy action $a_{t} = \\text{arg}\\max_{a\\in\\mathcal{A}}{Q_{\\theta}(s_{t})}$.\n",
    "        2. Execute action $a_{t}$, observe the reward $r_{t}$ from the _world_ and transition to the next state $s_{t+1}$. \n",
    "        3. Store the transition (experience) $\\mathcal{e}=(s_t, a_t, r_t, s_{t+1})$ in the replay buffer: $\\mathcal{e}\\rightarrow\\mathcal{B}$. \n",
    "        5. If the replay buffer $\\mathcal{B}$ has a _minium number of elements_: sample a mini-batch of experiances $(s_i, a_i, r_i, s_{i+1})$ from the replay buffer.  The agent randomly samples a mini-batch of $B$ transitions from the replay buffer:  $(s_j, a_j, r_j, s_{j+1}),\\, j = 1, 2, \\dots, B$. Each tuple represents a state-action-reward-next state experience example collected during environment interaction.\n",
    "        6. Compute the _target Q-value_ for each transition in the mini-batch using the _target Q-network_: $y_i = r_i + \\gamma \\cdot \\max_{a^{\\prime}\\in\\mathcal{A}}Q^{\\prime}_{\\theta^{-}}(s_{i+1})$ for $i=1,2,\\ldots,B$.\n",
    "        7. Compute the _mean squared loss_ function over the $B$ experiances collected in the mini-batch: $L(\\theta) = \\frac{1}{B}\\sum_{i=1}^{B}\\left(y_i - Q_{\\theta}(s_i)\\right)^2$.\n",
    "        8. Perform a _single_ gradient descent step to minimize the loss function $L(\\theta)$ with respect to the parameters $\\theta$ of the main Q-network $Q_{\\theta}(s)$: $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta}L(\\theta)$, where $\\alpha$ is the learning rate. \n",
    "            - _Why only a single step_? Each mini-batch is just a _small sample of the environment’s dynamics._ The goal of DQN is _online learning_: the network parameters are continuously updated as new experiences come in. If we force training to converge on each mini-batch, it risks _overfitting to that mini-batch_.\n",
    "        10. Update the state $s_t \\leftarrow s_{t+1}$.\n",
    "        9. Every $C$ steps, update the target Q-network parameters: $\\theta^{-} \\leftarrow \\theta$.\n",
    "    - End For\n",
    "- End For\n",
    "\n",
    "We've implemented this algorithm in [the `learn(...)` method](src/Compute.jl). Let's run the training process and test the agent on random vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ad721b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_agent = learn(agentmodel, world, \n",
    "    context = contextmodel, numberofepisodes = number_of_episodes, maxnumberofsteps = max_number_of_iterations, \n",
    "    minibatchsize = B, maxreplaybuffersize=buffersize);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949e9f2",
   "metadata": {},
   "source": [
    "What's in the agent's replay buffer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12be5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, s′ = let\n",
    "    \n",
    "    # initialize -\n",
    "    agent_replay_buffer = my_trained_agent.replaybuffer;\n",
    "\n",
    "    # draw a random experience from the replay buffer -\n",
    "    experience = rand(agent_replay_buffer); # draw a random experience\n",
    "    s, a, r, s′ = experience; # unpack the experience\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc5ed",
   "metadata": {},
   "source": [
    "__Check__: Do we recover the next state $s^{\\prime}$ from the $s$ and $a$ values in the replay buffer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ddd3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    true_s′ = s′;\n",
    "    computed_s′ = s .+ (a.*s); # compute the next state\n",
    "    @assert true_s′ ≈ computed_s′; # check if the computed next state is equal to the true next state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c305d6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.854555f0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecdc349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12×3 Matrix{Float32}:\n",
       "  6.69435    6.69435   -0.0\n",
       "  1.03138    1.03138   -0.0\n",
       "  1.05264    1.05264   -0.0\n",
       "  0.315708   0.315708  -0.0\n",
       "  0.896279   0.896279  -0.0\n",
       "  1.08324    1.08324   -0.0\n",
       "  0.933885   0.933885  -0.0\n",
       "  1.11662    1.11662   -0.0\n",
       "  0.837063   0.837063  -0.0\n",
       " 22.3394    22.3394    -0.0\n",
       "  0.83514    0.83514   -0.0\n",
       "  1.0f-6     1.0f-6    -0.01"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: What does a s -a-> s′ look like?\n",
    "[s s′ a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f31d06fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    a = my_trained_agent.actions;\n",
    "    sₒ = rand(Float32, K); # initial state\n",
    "    i = my_trained_agent.mainnetwork(sₒ) |> u-> argmax(u)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4b155-47f5-46a4-84e0-91df9f2d94c6",
   "metadata": {},
   "source": [
    "# The End: Thank You!\n",
    "\n",
    "Thank you each for your hard work and engagement throughout this Machine Learning and Artificial Intelligence course. \n",
    "* _What did we do_? We’ve explored many exciting topics, from __clustering__ and __binary/multiclass classification__ to __kernels__, early models such as __Hopfield networks__, and __Boltzmann machines__. We’ve also navigated the landscape of __neural networks__ in their many forms and explored online learning approaches like __multi-armed bandits__ and __reinforcement learning__.\n",
    "\n",
    "Your curiosity, persistence, and thoughtful questions made this an enriching experience. I hope you carry forward not just the technical skills but also a deeper appreciation for the power and possibilities of machine learning.\n",
    "\n",
    "Wishing you all the best in your future studies and projects—**keep exploring and pushing boundaries!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
