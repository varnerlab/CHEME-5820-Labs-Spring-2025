{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14f4219-73df-493c-9d3a-0e01298bae57",
   "metadata": {},
   "source": [
    "# L3b: Classification of Clinical Breast Cancer Samples\n",
    "Linear regression can be adapted for classification tasks by transforming the continuous output of the linear regression model directly to a class designation, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$ or into a probability using an output function $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ and applying a threshold to categorize predictions into discrete classes. Let's take a look at two examples of these strategies:\n",
    "\n",
    "* [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) is a simple yet powerful algorithm used in machine learning for binary classification tasks. It operates by _incrementally_ learning a linear decision boundary (linear regression model) that separates two classes based on input features by directly mapping the continuous output to a class such as $\\sigma:\\mathbb{R}\\rightarrow\\{-1,+1\\}$, where the output function is $\\sigma(\\star) = \\text{sign}(\\star)$.\n",
    "* [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression#) is a statistical method used in machine learning for binary classification tasks using the [logistics function](https://en.wikipedia.org/wiki/Logistic_function) as the transformation function. Applying the logistic function transforms the output of a linear regression model into a probability, enabling effective decision-making in various applications. We'll consider this approach next time.\n",
    "\n",
    "### Perceptron\n",
    "[The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) takes the (scalar) output of a linear regression model $y_{i}\\in\\mathbb{R}$ and then transforms it using the $\\sigma(\\star) = \\text{sign}(\\star)$ function to a discrete set of values representing categories, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$ in the binary classification case. \n",
    "* Suppose there exists a data set\n",
    "$\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ _labeled_ examples, where each example has been labeled by an expert, i.e., a human to be in a category $\\hat{y}_{i}\\in\\{-1,1\\}$, given the $m$-dimensional feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$. \n",
    "* [The Perceptron](https://en.wikipedia.org/wiki/Perceptron) _incrementally_ learns a linear decision boundary between _two_ classes of possible objects (binary classification) in $\\mathcal{D}$ by repeatedly processing the data. During each pass, a regression parameter vector $\\mathbf{\\beta}$ is updated until it makes no more than a specified number of mistakes. \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) computes the estimated label $\\hat{y}_{i}$ for feature vector $\\hat{\\mathbf{x}}_{i}$ using the $\\texttt{sign}:\\mathbb{R}\\to\\{-1,1\\}$ function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_{i} = \\texttt{sign}\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\cdot\\beta\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\beta=\\left(w_{1},\\dots,w_{n}, b\\right)$ is a column vector of (unknown) classifier parameters, $w_{j}\\in\\mathbb{R}$ corresponding to the importance of feature $j$ and $b\\in\\mathbb{R}$ is a bias parameter, the features $\\hat{\\mathbf{x}}^{\\top}_{i}=\\left(x^{(i)}_{1},\\dots,x^{(i)}_{m}, 1\\right)$ are $p = m+1$-dimensional (row) vectors (features augmented with bias term), and $\\texttt{sign}(z)$ is the function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\texttt{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "__Hypothesis__: If data set $\\mathcal{D}$ is linearly separable, the Perceptron will _incrementally_ learn a separating hyperplane in a finite number of passes through the data set $\\mathcal{D}$. However, if the data set $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge. Check out a [perceptron pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf)\n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem. We then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns back an updated problem instance holding the updated parameters.\n",
    "* __Challenge__: We've never seen this data and have no idea if it's linearly separable. Thus, we have no theoretical guarantee that [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) will work. Let's load the dataset, do some preprocessing, and then explore the performance of [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) on this data.\n",
    "* Let's compare our results to the study [Sidey-Gibbons, J., Sidey-Gibbons, C. Machine learning in medicine: a practical introduction. BMC Med Res Methodol 19, 64 (2019). https://doi.org/10.1186/s12874-019-0681-4](https://rdcu.be/d5NjG), that used the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ad205-611d-47eb-bbc0-c69736429a47",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447d14fd-baf9-47f6-bafe-4c1839460fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd8452-683e-462e-ade3-39edb8c22e37",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this lab, we'll use [the Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) to classify clinical Breast Cancer samples taken from the University of Wisconsin. \n",
    "* __Description__: The breast cancer dataset developed by [Wolberg, W. (1990)](https://doi.org/10.24432/C5HP4Z) was obtained from the University of Wisconsin Hospitals, Madison, from [Dr. William H. Wolberg](https://pages.cs.wisc.edu/~olvi/uwmp/cancer.html), and is available [from the UCI dataset archive](https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original). It contains `699` instances, with `9` clinical features and a class label `{benign | malignant}.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bd5500-6f9a-4b19-b1c0-0adf61b0ab0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>699×11 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">674 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">id</th><th style = \"text-align: left;\">ClumpThickness</th><th style = \"text-align: left;\">UniformityCellSize</th><th style = \"text-align: left;\">UniformityCellShape</th><th style = \"text-align: left;\">MarginalAdhesion</th><th style = \"text-align: left;\">SingleEpithelialCellSize</th><th style = \"text-align: left;\">BareNuclei</th><th style = \"text-align: left;\">BlandChromatin</th><th style = \"text-align: left;\"> NormalNucleoli</th><th style = \"text-align: left;\">Mitoses</th><th style = \"text-align: left;\">Class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1000025</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1002945</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1015425</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1016277</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1017023</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1017122</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">1018099</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">1018561</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1035283</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1036172</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1041801</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">688</td><td style = \"text-align: right;\">566346</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">689</td><td style = \"text-align: right;\">603148</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">690</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">691</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">692</td><td style = \"text-align: right;\">695091</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">693</td><td style = \"text-align: right;\">714039</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">694</td><td style = \"text-align: right;\">763235</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">695</td><td style = \"text-align: right;\">776715</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">696</td><td style = \"text-align: right;\">841769</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">697</td><td style = \"text-align: right;\">888820</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">698</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">699</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& id & ClumpThickness & UniformityCellSize & UniformityCellShape & MarginalAdhesion & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1000025 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t2 & 1002945 & 5 & 4 & 4 & 5 & $\\dots$ \\\\\n",
       "\t3 & 1015425 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t4 & 1016277 & 6 & 8 & 8 & 1 & $\\dots$ \\\\\n",
       "\t5 & 1017023 & 4 & 1 & 1 & 3 & $\\dots$ \\\\\n",
       "\t6 & 1017122 & 8 & 10 & 10 & 8 & $\\dots$ \\\\\n",
       "\t7 & 1018099 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t8 & 1018561 & 2 & 1 & 2 & 1 & $\\dots$ \\\\\n",
       "\t9 & 1033078 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t10 & 1033078 & 4 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t11 & 1035283 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t12 & 1036172 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t13 & 1041801 & 5 & 3 & 3 & 3 & $\\dots$ \\\\\n",
       "\t14 & 1043999 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t15 & 1044572 & 8 & 7 & 5 & 10 & $\\dots$ \\\\\n",
       "\t16 & 1047630 & 7 & 4 & 6 & 4 & $\\dots$ \\\\\n",
       "\t17 & 1048672 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t18 & 1049815 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t19 & 1050670 & 10 & 7 & 7 & 6 & $\\dots$ \\\\\n",
       "\t20 & 1050718 & 6 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t21 & 1054590 & 7 & 3 & 2 & 10 & $\\dots$ \\\\\n",
       "\t22 & 1054593 & 10 & 5 & 5 & 3 & $\\dots$ \\\\\n",
       "\t23 & 1056784 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t24 & 1057013 & 8 & 4 & 5 & 1 & $\\dots$ \\\\\n",
       "\t25 & 1059552 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t26 & 1065726 & 5 & 2 & 3 & 4 & $\\dots$ \\\\\n",
       "\t27 & 1066373 & 3 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t28 & 1066979 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t29 & 1067444 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t30 & 1070935 & 1 & 1 & 3 & 1 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m699×11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id      \u001b[0m\u001b[1m ClumpThickness \u001b[0m\u001b[1m UniformityCellSize \u001b[0m\u001b[1m UniformityCellShape \u001b[0m\u001b[1m Margi\u001b[0m ⋯\n",
       "     │\u001b[90m Int64   \u001b[0m\u001b[90m Int64          \u001b[0m\u001b[90m Int64              \u001b[0m\u001b[90m Int64               \u001b[0m\u001b[90m Int64\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ 1000025               5                   1                    1        ⋯\n",
       "   2 │ 1002945               5                   4                    4\n",
       "   3 │ 1015425               3                   1                    1\n",
       "   4 │ 1016277               6                   8                    8\n",
       "   5 │ 1017023               4                   1                    1        ⋯\n",
       "   6 │ 1017122               8                  10                   10\n",
       "   7 │ 1018099               1                   1                    1\n",
       "   8 │ 1018561               2                   1                    2\n",
       "   9 │ 1033078               2                   1                    1        ⋯\n",
       "  10 │ 1033078               4                   2                    1\n",
       "  11 │ 1035283               1                   1                    1\n",
       "  ⋮  │    ⋮           ⋮                 ⋮                    ⋮                 ⋱\n",
       " 690 │  654546               1                   1                    1\n",
       " 691 │  654546               1                   1                    1        ⋯\n",
       " 692 │  695091               5                  10                   10\n",
       " 693 │  714039               3                   1                    1\n",
       " 694 │  763235               3                   1                    1\n",
       " 695 │  776715               3                   1                    1        ⋯\n",
       " 696 │  841769               2                   1                    1\n",
       " 697 │  888820               5                  10                   10\n",
       " 698 │  897471               4                   8                    6\n",
       " 699 │  897471               4                   8                    8        ⋯\n",
       "\u001b[36m                                                  7 columns and 678 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.read(joinpath(_PATH_TO_DATA, \"breast-cancer-wisconsin.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119421d2-c887-450c-b7c4-34e0e29b98dd",
   "metadata": {},
   "source": [
    "__Data wrangling__: The `Class` label is not in the form of $\\{-1,1\\}$ that the perceptron expects, so let's transform the original data where we map $2\\rightarrow{-1}$ and $4\\rightarrow{1}$. We'll save the transformed data in the `dataset::DataFrame` variable. In our transformed dataset the `-1` labels is _not cancer_ while the label `1` denotes _cancer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d90a573-7377-4390-807b-13268b3bd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>699×11 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">674 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">id</th><th style = \"text-align: left;\">ClumpThickness</th><th style = \"text-align: left;\">UniformityCellSize</th><th style = \"text-align: left;\">UniformityCellShape</th><th style = \"text-align: left;\">MarginalAdhesion</th><th style = \"text-align: left;\">SingleEpithelialCellSize</th><th style = \"text-align: left;\">BareNuclei</th><th style = \"text-align: left;\">BlandChromatin</th><th style = \"text-align: left;\"> NormalNucleoli</th><th style = \"text-align: left;\">Mitoses</th><th style = \"text-align: left;\">Class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1000025</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1002945</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1015425</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1016277</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">1017023</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">1017122</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">1018099</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">1018561</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1033078</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1035283</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">1036172</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1041801</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">688</td><td style = \"text-align: right;\">566346</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">689</td><td style = \"text-align: right;\">603148</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">690</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">691</td><td style = \"text-align: right;\">654546</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">692</td><td style = \"text-align: right;\">695091</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">693</td><td style = \"text-align: right;\">714039</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">694</td><td style = \"text-align: right;\">763235</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">695</td><td style = \"text-align: right;\">776715</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">696</td><td style = \"text-align: right;\">841769</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">697</td><td style = \"text-align: right;\">888820</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">698</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">699</td><td style = \"text-align: right;\">897471</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& id & ClumpThickness & UniformityCellSize & UniformityCellShape & MarginalAdhesion & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1000025 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t2 & 1002945 & 5 & 4 & 4 & 5 & $\\dots$ \\\\\n",
       "\t3 & 1015425 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t4 & 1016277 & 6 & 8 & 8 & 1 & $\\dots$ \\\\\n",
       "\t5 & 1017023 & 4 & 1 & 1 & 3 & $\\dots$ \\\\\n",
       "\t6 & 1017122 & 8 & 10 & 10 & 8 & $\\dots$ \\\\\n",
       "\t7 & 1018099 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t8 & 1018561 & 2 & 1 & 2 & 1 & $\\dots$ \\\\\n",
       "\t9 & 1033078 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t10 & 1033078 & 4 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t11 & 1035283 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t12 & 1036172 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t13 & 1041801 & 5 & 3 & 3 & 3 & $\\dots$ \\\\\n",
       "\t14 & 1043999 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t15 & 1044572 & 8 & 7 & 5 & 10 & $\\dots$ \\\\\n",
       "\t16 & 1047630 & 7 & 4 & 6 & 4 & $\\dots$ \\\\\n",
       "\t17 & 1048672 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t18 & 1049815 & 4 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t19 & 1050670 & 10 & 7 & 7 & 6 & $\\dots$ \\\\\n",
       "\t20 & 1050718 & 6 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t21 & 1054590 & 7 & 3 & 2 & 10 & $\\dots$ \\\\\n",
       "\t22 & 1054593 & 10 & 5 & 5 & 3 & $\\dots$ \\\\\n",
       "\t23 & 1056784 & 3 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t24 & 1057013 & 8 & 4 & 5 & 1 & $\\dots$ \\\\\n",
       "\t25 & 1059552 & 1 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t26 & 1065726 & 5 & 2 & 3 & 4 & $\\dots$ \\\\\n",
       "\t27 & 1066373 & 3 & 2 & 1 & 1 & $\\dots$ \\\\\n",
       "\t28 & 1066979 & 5 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t29 & 1067444 & 2 & 1 & 1 & 1 & $\\dots$ \\\\\n",
       "\t30 & 1070935 & 1 & 1 & 3 & 1 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m699×11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m id      \u001b[0m\u001b[1m ClumpThickness \u001b[0m\u001b[1m UniformityCellSize \u001b[0m\u001b[1m UniformityCellShape \u001b[0m\u001b[1m Margi\u001b[0m ⋯\n",
       "     │\u001b[90m Int64   \u001b[0m\u001b[90m Int64          \u001b[0m\u001b[90m Int64              \u001b[0m\u001b[90m Int64               \u001b[0m\u001b[90m Int64\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ 1000025               5                   1                    1        ⋯\n",
       "   2 │ 1002945               5                   4                    4\n",
       "   3 │ 1015425               3                   1                    1\n",
       "   4 │ 1016277               6                   8                    8\n",
       "   5 │ 1017023               4                   1                    1        ⋯\n",
       "   6 │ 1017122               8                  10                   10\n",
       "   7 │ 1018099               1                   1                    1\n",
       "   8 │ 1018561               2                   1                    2\n",
       "   9 │ 1033078               2                   1                    1        ⋯\n",
       "  10 │ 1033078               4                   2                    1\n",
       "  11 │ 1035283               1                   1                    1\n",
       "  ⋮  │    ⋮           ⋮                 ⋮                    ⋮                 ⋱\n",
       " 690 │  654546               1                   1                    1\n",
       " 691 │  654546               1                   1                    1        ⋯\n",
       " 692 │  695091               5                  10                   10\n",
       " 693 │  714039               3                   1                    1\n",
       " 694 │  763235               3                   1                    1\n",
       " 695 │  776715               3                   1                    1        ⋯\n",
       " 696 │  841769               2                   1                    1\n",
       " 697 │  888820               5                  10                   10\n",
       " 698 │  897471               4                   8                    6\n",
       " 699 │  897471               4                   8                    8        ⋯\n",
       "\u001b[36m                                                  7 columns and 678 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = let\n",
    "\n",
    "    number_of_examples = nrow(df);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        c = df[i,:Class];\n",
    "        if (c == 2)\n",
    "            df[i,:Class] = -1 # not cancer\n",
    "        elseif (c == 4)\n",
    "            df[i,:Class] = 1 # cancer\n",
    "        end\n",
    "    end\n",
    "    df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390f5e1-d65f-4e75-be89-be65fd09fa5f",
   "metadata": {},
   "source": [
    "Next, let's (randomly) partition the clinical data into `training` and `test` sets. We'll use `training` to estimate the model parameters, and test to estimate how well the classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2807cc0-14b1-4e55-9547-8ec1c71d86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 456; # from Sidey-Gibbons, 2019\n",
    "    D = Matrix(dataset);\n",
    "    number_of_features = size(D,2); # number of cols of housing data\n",
    "    number_of_examples = size(D,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c3acdf0-7202-4a31-8940-278a984a55b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243×11 Matrix{Int64}:\n",
       "  897471   4   8   8  5   4   5  10   4   1   1\n",
       " 1124651   1   3   3  2   2   1   7   2   1  -1\n",
       "  718641   1   1   1  1   5   1   3   1   1  -1\n",
       "  536708   1   1   1  1   2   1   1   1   1  -1\n",
       "  749653   3   1   1  1   2   1   2   1   1  -1\n",
       " 1275807   4   2   4  3   2   2   2   1   1  -1\n",
       " 1147748   5  10   6  1  10   4   4  10  10   1\n",
       " 1257200  10  10  10  7  10  10   8   2   1   1\n",
       "  763235   3   1   1  1   2   1   2   1   2  -1\n",
       "  695091   5  10  10  5   4   5   4   4   1   1\n",
       " 1293966   4   1   1  1   2   1   1   1   1  -1\n",
       " 1276091   2   1   1  1   2   1   2   1   1  -1\n",
       " 1132347   1   1   4  1   2   1   2   1   1  -1\n",
       "       ⋮                  ⋮                   ⋮\n",
       "  657753   3   1   1  4   3   1   2   2   1  -1\n",
       " 1081791   6   2   1  1   1   1   7   1   1  -1\n",
       "  566509   5   1   1  1   2   1   1   1   1  -1\n",
       "  743348   3   2   2  1   2   1   2   3   1  -1\n",
       " 1321942   5   1   1  1   2   1   3   1   1  -1\n",
       " 1224329   1   1   1  2   2   1   3   1   1  -1\n",
       " 1231853   4   2   2  1   2   1   2   1   1  -1\n",
       " 1197527   5   1   1  1   2   1   2   1   1  -1\n",
       " 1201936   5  10  10  3   8   1   5  10   3   1\n",
       " 1266124   5   1   2  1   2   1   1   1   1  -1\n",
       " 1116192   1   1   1  1   2   1   2   1   1  -1\n",
       " 1204558   4   1   1  1   2   1   2   1   1  -1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89207eca-49a5-4ac0-b4f7-7d884534d52e",
   "metadata": {},
   "source": [
    "## Task 1: Build Classification Model Object and Learn Parameters\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ee2374-edba-46d5-8195-7ad63e8770ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # How many features do we have?\n",
    "    D = training; # let's look at the training data\n",
    "    number_of_features = size(D,2) - 1; # why minus one?\n",
    "    \n",
    "    # build a model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to like with m mistakes\n",
    "    ));\n",
    "\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee0f62-b682-4d7f-b1c2-183fc50cff78",
   "metadata": {},
   "source": [
    "Next, we'll using the `training` dataset to estimate the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35eb0bee-44e1-4251-b91c-e78637447d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyPerceptronClassificationModel([26.0, -4.0, 20.0, 12.0, 2.0, 17.0, 26.0, -7.0, 4.0, -365.0], 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedmodel = let\n",
    "\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2) - 1; # how many features do we have (cols)?\n",
    "    X = [D[:,2:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # train the model -\n",
    "    trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return\n",
    "    trainedmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f824e-d5ce-4019-949c-980c73da15f9",
   "metadata": {},
   "source": [
    "## Task 2: Run a prediction to compute misclassification rate\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4353492b-c1ea-4620-89c2-595da58bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,y = let\n",
    "\n",
    "    D = test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2) - 1; # how many features do we have (cols)?\n",
    "    X = [D[:,2:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f53d4-e700-4807-8758-b2d18f7bc503",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "There are many ways to compute the performance of the binary classifier. Let's consider a few versions of the misclassification rate. We'll start with the overall misclassification rate (defined as the number of mistakes divided by the total number of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f4daa7-b59e-4552-9e20-894e0de3e9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03292181069958848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_misclassified_percentage = let\n",
    "    \n",
    "    number_of_test_examples = length(ŷ);\n",
    "    error_counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (ŷ[i] != y[i])\n",
    "            error_counter += 1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error_counter/number_of_test_examples\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb06294-37a3-48ed-9e76-38b4db4bcfe7",
   "metadata": {},
   "source": [
    "__Accuracy__: Accuracy is a fundamental metric used to evaluate the performance of binary classifiers. Accuracy is mathematically defined as the ratio of correctly predicted instances (both true positives and true negatives) to the total number of cases. \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{Accuracy} = \\frac{N_{+}+N_{-}}{N_{T}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $N_{+}$ (or $N_{-}$) denotes the number of true positive (or negative) classifications, i.e., the number of times the classifier estimates the proper label, and $N_{T}$ denotes the total number of prediction samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153825c8-163c-4da7-adf5-4f26f01582fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9670781893004116"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = let\n",
    "\n",
    "    number_of_test_examples = length(ŷ);\n",
    "    correct_counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (ŷ[i] == y[i])\n",
    "            correct_counter += 1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    correct_counter/number_of_test_examples\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e2a2e-8869-445a-bf9b-cf54de3bccb2",
   "metadata": {},
   "source": [
    "__Specificity__: The specificity, also known as the True Negative Rate (TNR), measures how well the model distinguishes between the positive and negative classes. The specificity is defined as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{Specificity} = \\frac{N_{-,-}}{N_{-,-}+N_{+,-}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $N_{-,-}$ denotes the number of actual `negative` samples that were classified as `negative,` and $N_{+,-}$ denotes the number of (actual) `positive` samples that were (mis)classified as `negative` (false negative). The denominator is the total number of samples predicted to be `negative.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1567f9ea-37bc-4b5a-af8d-dadbb2146be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876543209876543"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = let\n",
    "\n",
    "    number_of_test_examples = length(ŷ);\n",
    "    total_number_of_predicted_negatives = findall(c-> c == -1, ŷ) |> length;\n",
    "    counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (y[i] == -1 && ŷ[i] == -1) # N(-1,-1)\n",
    "            counter += 1;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    counter/total_number_of_predicted_negatives\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea9334-31dd-49d5-b003-3e1218892b6f",
   "metadata": {},
   "source": [
    "__Precision__: The precision measures the proportion of true positive predictions among all positive predictions. Precision is defined as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{Precision} = \\frac{N_{+,+}}{N_{+,+} + N_{-,+}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $N_{+,+}$  denotes the number of actual `positive` classifications in which the model correctly predicts `positive,` and $N_{-,+}$ denotes the number of actual `negative` samples that are predicted by the model to be `positive` (false positive). The denominator is the total number of `positive` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4a5bdb4-7f84-459e-ab6c-a85e06c604ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259259259259259"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = let\n",
    "\n",
    "    number_of_test_examples = length(ŷ);\n",
    "    total_number_of_predicted_positives = findall(c-> c == 1, ŷ) |> length;\n",
    "    counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (y[i] == 1 && ŷ[i] == 1)\n",
    "            counter += 1;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    counter/total_number_of_predicted_positives\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
