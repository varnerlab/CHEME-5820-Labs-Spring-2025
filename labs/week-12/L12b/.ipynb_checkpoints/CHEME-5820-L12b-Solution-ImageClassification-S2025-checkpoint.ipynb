{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# L12b: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "\n",
    "___\n",
    "\n",
    "In this lab, students will develop a [Feed-Forward Neural Network (FNN)](https://en.wikipedia.org/wiki/Feedforward_neural_network) that will classify images of handwritten numbers between `0,...,9` taken from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). Each digit between `0` and `9` has approximately 5000 example images, each of which is a `28`$\\times$`28` grayscale image; thus, each image has `784` pixels.  \n",
    "\n",
    "* _Objective_: We'll train a _deep_ feedforward neural network to classify images of handwritten digits. The FNN will be a _multiclass classifier_, i.e., it will classify images into one of `10` classes, each corresponding to a digit between `0` and `9`.\n",
    "* _Implementation_: Toward this task, we'll use the [Flux.jl package](https://github.com/FluxML/Flux.jl) to build, train, and test our image classifier. However, there are two excellent libraries for ANNs in Python (sort of), namely the [PyTorch library](https://pytorch.org/) from the [AI group at META](https://ai.meta.com/meta-ai/) and the [TensorFlow library](https://www.tensorflow.org/) developed by [Google](https://research.google/). \n",
    "\n",
    "## Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (5 min)__: Let's take 5 minutes to load [the MNIST handwritten digits dataset](https://en.wikipedia.org/wiki/MNIST_database) that our feed-forward network will model. We've seen this data before, so we won't spend much time reviewing it.\n",
    "* __Task 2: Setup the model structure and training (30 min)__: In this task, we'll construct and train a feedforward model, i.e., we'll learn the model parameters, using example images encoded in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}`. The training will use the [Gradient Descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) algorithm to minimize the logit cross-entropy loss function. There is some new content here, so we'll spend a few minutes going over this material.\n",
    "* __Task 3: How well does the model predict unseen versus observed images (10 min)?__: In this task, we'll check the network's generalization, i.e., how well it does on data it has not seen, by computing the number of correct predictions on the test set. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f403b43-7d09-4058-a96c-aae3bbfca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {},
   "source": [
    "### Load the MNIST digits data set\n",
    "Before training and testing the `FFN,` we need to construct two datasets. First, we build a `training dataset` of images that we will use to estimate the model parameters. We'll save training data in the `training_image_dataset` variable. Next, we'll construct a `test dataset,` which we'll use to see how well our `FFN` generalizes (predicts) data it has never seen. We'll save this data in the `testing_image_dataset` variable.\n",
    "* Both the `training_image_dataset` and `testing_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x,` i.e., the pixels of the image arranged in vector format. The second element is the `label,` i.e., whether the image corresponds to `0,....,9`.\n",
    "* The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of weird features. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values).\n",
    "\n",
    "Before we load the training data, let's set some constants, which we use below. The comment next to each constant describes its permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047ec2c5-0c96-4c94-ace8-7165162fe37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 3000; # how many training examples of *each* number to include from the library\n",
    "number_of_test_examples = 500; # how many examples are we going to test on?\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "#### Select a set of `training` images and build the `training_image_dataset.`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable. \n",
    "* We load `number_of_training_examples::Int` images into the `training_image_dictionary::Dict{Int64, Array{Gray{N0f8},3}}` and then convert these to a vector vector format, i.e.., linearize the `28`$\\times$`28` matrix of `Gray` values into a vector of `784` pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_image_dictionary = let\n",
    "    training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "    for i ∈ number_digit_array\n",
    "        \n",
    "        # create a set for this digit -\n",
    "        image_digit_array = Array{Gray{N0f8},3}(undef, number_of_rows, number_of_cols, number_of_training_examples);\n",
    "        files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "        imagecount = 1;\n",
    "        for fileindex ∈ 1:number_of_training_examples\n",
    "            filename = files[fileindex];\n",
    "            ext = file_extension(filename)\n",
    "            if (ext == \"jpg\")\n",
    "                image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "                imagecount += 1\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # capture -\n",
    "        training_image_dictionary[i] = image_digit_array\n",
    "    end\n",
    "    training_image_dictionary\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e53240",
   "metadata": {},
   "source": [
    "What's in the `training_image_dictionary` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c42c51-185f-48c6-8e28-c03f3c6fce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;235m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;233;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;242m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;234;48;5;245m▀\u001b[38;5;246;48;5;255m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;247m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;243;48;5;249m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;239;48;5;244m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;243;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;251m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;246;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;250m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;253;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;245m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;233;48;5;234m▀\u001b[38;5;239;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;244;48;5;236m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;255;48;5;233m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;245m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;246;48;5;235m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;246m▀\u001b[38;5;253;48;5;233m▀\u001b[38;5;255;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;255m▀\u001b[38;5;239;48;5;255m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;241m▀\u001b[38;5;241;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;250m▀\u001b[38;5;249;48;5;234m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;238;48;5;232m▀\u001b[38;5;252;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;233;48;5;248m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;242m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;253m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;251;48;5;233m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;247;48;5;255m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;235;48;5;255m▀\u001b[38;5;232;48;5;250m▀\u001b[38;5;232;48;5;251m▀\u001b[38;5;234;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;248;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;240;48;5;232m▀\u001b[38;5;249;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dictionary[8][:,:,10] # how does the indexing work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576da3f-8165-4ee2-90a0-6650704b08cd",
   "metadata": {},
   "source": [
    "__Vectorize the training images__.\n",
    "* _What do we mean by vectorize_? Each $N\\times{N}$ image array containing the grayscale values at each pixel is converted to an $N^{2}$ vector of values. What image class, i.e., what number it represents, is then converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). The converted data is stored in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n",
    "* _What's the deal with Float32_? Most neural network libraries (or other machine learning calculations) use `Float32` (or lower) to save memory space because of the large number of parameters associated with the network. Additionally, model training is often carried out using specialized hardware [such as Graphical Processing Units (GPUs)](https://www.nvidia.com/en-us/data-center/h100/), which has different memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1554fa2-cfeb-4776-a186-be578f27fffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_image_dataset = let\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array);\n",
    "        X = training_image_dictionary[i];\n",
    "        \n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = Array{Float32,1}(undef, number_of_pixels);\n",
    "            linearindex = 1;\n",
    "            for row ∈ 1:number_of_rows\n",
    "                for col ∈ 1:number_of_cols\n",
    "                    D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                    linearindex+=1;\n",
    "                end\n",
    "            end\n",
    "    \n",
    "            training_tuple = (D,Y);\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "    end\n",
    "    training_image_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84c2f9",
   "metadata": {},
   "source": [
    "What's in the `training_image_dataset` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf99db-760b-405c-a494-dac537ddda6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dataset[6000] # how does the indexing work here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bf4b-8671-4975-bf37-95d3b0d8a072",
   "metadata": {},
   "source": [
    "#### Select a set of `test` images and build the `testing_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `testing_image_dataset` variable.\n",
    "* We load `number_of_test_examples::Int` images into the `testing_image_dataset::Dict{Int64, Array{Gray{N0f8},3}}` and then convert these to a vector vector format, i.e.., linearize the `28`$\\times$`28` matrix of `Gray` values into a vector of `784` pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ad3577-cdd3-491b-8b14-636e0c913c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Array{Gray{N0f8}, 3}} with 10 entries:\n",
       "  0 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  4 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  5 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.02) Gray{N0f8}(0.016); G…\n",
       "  6 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  2 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  7 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  9 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  8 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…\n",
       "  3 => [Gray{N0f8}(0.0) Gray{N0f8}(0.004) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gr…\n",
       "  1 => [Gray{N0f8}(0.0) Gray{N0f8}(0.0) … Gray{N0f8}(0.0) Gray{N0f8}(0.0); Gray…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_image_dictionary = let\n",
    "    testing_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "    for i ∈ number_digit_array\n",
    "        \n",
    "        # create a set for this digit -\n",
    "        image_digit_array = Array{Gray{N0f8},3}(undef, number_of_rows, number_of_cols, number_of_test_examples);\n",
    "        files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "        imagecount = 1;\n",
    "        for fileindex ∈ (number_of_training_examples + 1):(number_of_training_examples+number_of_test_examples)\n",
    "            filename = files[fileindex];\n",
    "            ext = file_extension(filename)\n",
    "            if (ext == \"jpg\")\n",
    "                image_digit_array[:,:,imagecount] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "                imagecount += 1\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # capture -\n",
    "        testing_image_dictionary[i] = image_digit_array\n",
    "    end\n",
    "\n",
    "    testing_image_dictionary # return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368b55-eeb2-4963-a4c5-6b1785fd34ab",
   "metadata": {},
   "source": [
    "__Vectorize the testing images__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1c5708-f557-4625-899e-24bb8605f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_dataset = let\n",
    "    testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array);\n",
    "        X = testing_image_dictionary[i];\n",
    "        \n",
    "        for t ∈ 1:number_of_test_examples\n",
    "            D = Array{Float32,1}(undef, number_of_pixels);\n",
    "            linearindex = 1;\n",
    "            for row ∈ 1:number_of_rows\n",
    "                for col ∈ 1:number_of_cols\n",
    "                    D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                    linearindex+=1;\n",
    "                end\n",
    "            end\n",
    "    \n",
    "            testing_tuple = (D,Y);\n",
    "            push!(testing_image_dataset, testing_tuple);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    testing_image_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Task 2: Setup the model structure and training\n",
    "In this task, we'll construct and train a feedforward model, i.e., learn the model parameters, using example images encoded in the the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}`. \n",
    "* _Hmmm_. This is an example of _supervised batch learning_, i.e., the model learns its parameters on a fixed set of labeled examples. However, unlike an _online_ learning approach, e.g., the perceptron, there is no update rule. Thus, new data cannot be easily included (once we have the parameters, we have them).\n",
    "\n",
    "Let's start by getting the `number_of_input_states::Int64`; this will be the input dimension of the first layer in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Then, we build an empty model with default (random) parameter values but a fixed structure. The number and dimension of the layers and the activation functions for each layer are specified when we build the model (but we'll update the parameters during training).\n",
    "* _Library_: We use [the `Flux.jl` machine learning library](https://github.com/FluxML/Flux.jl) to construct the neural network model. The model will have three layers: the input layer is a `784` $\\times$ `512` layer with [relu activation functions](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), the hidden layer is a `512` $\\times$ `10` layer and the output layer is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "* _Syntax_: The [`Flux.jl` package](https://github.com/FluxML/Flux.jl) uses some next level syntax. The model is built using [the `Chain` function](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Chain), which takes a list of layers as input. Each layer is defined using the [`Dense` type](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Dense) (in this case), which takes the number of input and output neurons as arguments. The activation function is an additional argument to [the `Dense` type](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Dense). The final layer uses [the `softmax(...)` method exported by the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/reference/#Softmax) to produce a probability distribution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel; # create a \"namespaced\" of sorts\n",
    "MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  # layer 1\n",
    "        Dense(512, 10, relu), # layer 2\n",
    "        NNlib.softmax) # layer 3 (output layer)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "__Loss function__: Next, specify the `loss` function we will minimize to estimate the model parameters. We choose a loss function that is appropriate for a _multiclass classification problem_, namely a [logit cross-entropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy):\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C} y_{ij}\\log(p_{ij}(\\theta))\n",
    "$$\n",
    "where the outer summation is over all $N$ training examples, and the inner summation is over the $C$ possible classes. The $y_{ij}$ is the one-hot encoded label for the $i$th training example, and $p_{ij}$ is the predicted probability of the $i$th training example being in class $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "We'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method.\n",
    "* _Why_? We could have chosen one of _many_ possible training approaches. The [`Flux.jl` library supports _many_ optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference) which are all some version of gradient descent. However, there is no _technical_ reason we couldn't have chosen an optimizer that doesn't rely on computing the gradient of the loss function. For example, we could have used a genetic algorithm or other optimization method. However, these methods are _likely_ not as efficient as gradient descent for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "We are now ready to train the model. If the `should_we_train = true,` then we use the [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize a [logit cross-entropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy).\n",
    "* _Restart_: Because the error landscape is non-convex, we have to start from many different locations. We do `number_of_epochs` passes through the data, i.e., a forward pass for prediction and a backpropagation step for parameter updates. Although the training is a little opaque, intuition suggests that the library is choosing different initial parameter guesses for each pass through the data and then driving these to convergence.\n",
    "* _Training takes a long time_. For each complete pass through the data, i.e., for each `epoch,` we save a `tmp` file holding the network state... just in case of `BOOOOOOOOM.`  We also have some pre-trained models to load if the `should_we_train` flag is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 512, relu),              \u001b[90m# 401_920 parameters\u001b[39m\n",
       "  Dense(512 => 10, relu),               \u001b[90m# 5_130 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m407_050 parameters, 1.553 MiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "should_we_train = false; # set this flag to {true | false}\n",
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250;\n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    # if we don't train: load up a previous model\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## Task 3: How well does the model predict unseen versus observed images?\n",
    "In this task, we'll check the network's generalization, i.e., how well it does on data it has not seen. One of the challenges with [Neural Networks)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is the lack of generalizability, i.e., they _may not_ perform well on data the model has not seen. \n",
    "\n",
    "Let's explore this question:\n",
    "* First, compute the fraction of the `training data` that is correctly classified. This will help us understand how many of the `N` training samples we get correct and how many we get wrong. We expect to be _mostly correct_ on the training data.\n",
    "* Next, we'll do the same thing but with the `test data,` i.e., data the model has never seen. We expect the correct prediction fraction on the test data to be less than or, at best, equal to the equivalent training data value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Correct prediction `training` dataset\n",
    "In the code block below, we pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels for the `training` dataset.\n",
    "* _Logic_: If the prediction and the actual label agree, we update the `S` variable (a running count of the number of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `training` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 89.92999999999999%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "    x = training_image_dataset[i][1];\n",
    "    y = training_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "    y == ŷ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ececb",
   "metadata": {},
   "source": [
    "### Correct prediction `test` dataset\n",
    "In the code block below, we pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual label for the `test` dataset. \n",
    "* _Logic_: If the prediction and the actual label agree, we update the `S` variable (a running count of the number of correct predictions). Finally, we compute the fraction of _correct_ classifications by dividing the number of correct predictions by the total number of images in the `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 89.64%\n"
     ]
    }
   ],
   "source": [
    "S_testing = 0;\n",
    "for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    y == ŷ ? S_testing+=1 : nothing\n",
    "end\n",
    "correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "println(\"Correct prediction on the test data: $(correct_prediction_test)%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535531c-5361-4a4a-8f52-d8170be52f5b",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "Let's do a few manual examples, and see what we get. \n",
    "\n",
    "* In the code block below, we take a few _random_ images from the `testing_image_dataset` and compare the predicted and actual labels (in one-hot format). If the _prediction is wrong_, we print the actual image and the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "789967b5-7126-40a3-a57c-649c1cdff027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bool[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAA4lJREFUaAW9wU2opXMcAODnfc/vvfeey8zINGVBPhZiYWXBLKSsfEQ2FnbW8lFKVrKUryzGQlFKKGys1ERko2QhUmwsKGUzt7l37j1f7/ueY+qvTqczOpfF73nCMZ3GBcVpXECNBlNX1qC1KiQLyUKycAwDXFDs4gICHaYIDDFCj230aK0LyUKykCwcww6OUKGz1KBGhUuWpooac6tCspAsJAvHMFMsMMNJHKBGa9VJHCjm1oVkIVlIFo6hRYNKcYAac1TYQYUR5pZO4JJVIVlIFpKFDa7GBK1iS/EzbkOFQ+yixhRP4z1csi4kC8lCsrDBIRrsoEONM7jVqgVaRYVdHKG3KiQLyUKysEGNChNFh6dQ4Vs8gQdwI86jw3cYubKQLCQLycIGcyywixFO4xkc4mX8iXMYoMEENbYxtS4kC8lCsrDBEGN0uBb3KOY4jw7X4CJ2FHP/LiQLyUKysMFEscBF3ITrsIdOcYQKMzRoEZhaF5KFZCFZ2KBGjwYtHkWDPdyJEX5BjRZbiiM0aK0KyUKykCxsUCmuwt24F4e4HV/jezyPHxQ9BujRWxeShWQhWdigwxAX8Sp6DBQT3IfX8Rg67GNXMbYuJAvJQrJwDIG7cQdm+BWv4VO8j8dwPz5SjLCDhXUhWUgWkoVjmOATzLCFF/AFdvASHsez+BBDjDFzZSFZSBaSRY05GkVraYAePb7CWbyIbzHEGL9hH1Ocwr5iiCPrQrKQLCSLuaJFYBcNZhgr5ngOW5hihAVO4GYc4BT2EagxQoWFVSFZSBaShcsG6NGhs67BIWYYWJrgOdyAA0Vgogh0VoVkIVlIFi6rUGOuqBEYYILWUo/AGdyCh7CPt1Bhohigsy4kC8lCsnBZj4XiBKaYKXYxxgLbmKLDI3hb8Qbew0IRaDC2LiQLyUKyaLBApzjCHFs4i28sncRdeBIP4C/8hHOoFYEOnSsLyUKykCxaS9uYKR7Gq3gJP+IsXsANaDDGZ3gTv2NL0Vm6FntWhWQhWUgWDVrFHDV6nMIteBfbmGAHC/yMD/CKYogxhhhjFyPsWReShWQhWSws9ZgrvsSDeAen0eJjfIXPcREVFhgrxoqRfxeShWQhWfhHjcBM8QcOcb3iKhxhgB4D1Oj9NyFZSBaSRaeoUKFBq9jDUDFRDBS9/yckC8lCsvCPHj0G2FZ0GFs1Q41AYOK/CclCspDsb8fL5v30ThAKAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAA4lJREFUaAW9wU2opXMcAODnfc/vvfeey8zINGVBPhZiYWXBLKSsfEQ2FnbW8lFKVrKUryzGQlFKKGys1ERko2QhUmwsKGUzt7l37j1f7/ueY+qvTqczOpfF73nCMZ3GBcVpXECNBlNX1qC1KiQLyUKycAwDXFDs4gICHaYIDDFCj230aK0LyUKykCwcww6OUKGz1KBGhUuWpooac6tCspAsJAvHMFMsMMNJHKBGa9VJHCjm1oVkIVlIFo6hRYNKcYAac1TYQYUR5pZO4JJVIVlIFpKFDa7GBK1iS/EzbkOFQ+yixhRP4z1csi4kC8lCsrDBIRrsoEONM7jVqgVaRYVdHKG3KiQLyUKysEGNChNFh6dQ4Vs8gQdwI86jw3cYubKQLCQLycIGcyywixFO4xkc4mX8iXMYoMEENbYxtS4kC8lCsrDBEGN0uBb3KOY4jw7X4CJ2FHP/LiQLyUKysMFEscBF3ITrsIdOcYQKMzRoEZhaF5KFZCFZ2KBGjwYtHkWDPdyJEX5BjRZbiiM0aK0KyUKykCxsUCmuwt24F4e4HV/jezyPHxQ9BujRWxeShWQhWdigwxAX8Sp6DBQT3IfX8Rg67GNXMbYuJAvJQrJwDIG7cQdm+BWv4VO8j8dwPz5SjLCDhXUhWUgWkoVjmOATzLCFF/AFdvASHsez+BBDjDFzZSFZSBaSRY05GkVraYAePb7CWbyIbzHEGL9hH1Ocwr5iiCPrQrKQLCSLuaJFYBcNZhgr5ngOW5hihAVO4GYc4BT2EagxQoWFVSFZSBaShcsG6NGhs67BIWYYWJrgOdyAA0Vgogh0VoVkIVlIFi6rUGOuqBEYYILWUo/AGdyCh7CPt1Bhohigsy4kC8lCsnBZj4XiBKaYKXYxxgLbmKLDI3hb8Qbew0IRaDC2LiQLyUKyaLBApzjCHFs4i28sncRdeBIP4C/8hHOoFYEOnSsLyUKykCxaS9uYKR7Gq3gJP+IsXsANaDDGZ3gTv2NL0Vm6FntWhWQhWUgWDVrFHDV6nMIteBfbmGAHC/yMD/CKYogxhhhjFyPsWReShWQhWSws9ZgrvsSDeAen0eJjfIXPcREVFhgrxoqRfxeShWQhWfhHjcBM8QcOcb3iKhxhgB4D1Oj9NyFZSBaSRaeoUKFBq9jDUDFRDBS9/yckC8lCsvCPHj0G2FZ0GFs1Q41AYOK/CclCspDsb8fL5v30ThAKAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;252m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;233;48;5;255m▀\u001b[38;5;232;48;5;241m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;243m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;251m▀\u001b[38;5;255;48;5;235m▀\u001b[38;5;255;48;5;242m▀\u001b[38;5;255;48;5;245m▀\u001b[38;5;255;48;5;238m▀\u001b[38;5;255;48;5;238m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;233;48;5;239m▀\u001b[38;5;254;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;239;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;242;48;5;250m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;252m▀\u001b[38;5;232;48;5;237m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;239;48;5;252m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;254m▀\u001b[38;5;250;48;5;255m▀\u001b[38;5;236;48;5;250m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;237;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;238m▀\u001b[38;5;240;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;248;48;5;235m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;240;48;5;251m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;247;48;5;233m▀\u001b[38;5;253;48;5;234m▀\u001b[38;5;240;48;5;233m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;255;48;5;254m▀\u001b[38;5;251;48;5;252m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;237m▀\u001b[38;5;247;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;242m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;239m▀\u001b[38;5;232;48;5;249m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;255;48;5;254m▀\u001b[38;5;255;48;5;250m▀\u001b[38;5;245;48;5;235m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;240;48;5;241m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;241;48;5;255m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;243m▀\u001b[38;5;246;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;239;48;5;233m▀\u001b[38;5;254;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;233m▀\u001b[38;5;247;48;5;232m▀\u001b[38;5;241;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let\n",
    "    \n",
    "    N = length(testing_image_dataset); # how many test examples\n",
    "    i = rand(1:N) # select a random test example\n",
    "    x = testing_image_dataset[i][1]; # get the input for this test example\n",
    "    y = testing_image_dataset[i][2]; # get the *actual* output for this test example (onehot encoding)\n",
    "   \n",
    "    # compute onehot encoding of the predicted output -\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    flag = y == ŷ # check that the predicted output is the same as the actual output\n",
    "    if (flag == false)\n",
    "        println(ŷ) # onehot encoding of the *predicted* output\n",
    "        display(reshape(x, number_of_rows, number_of_cols) |> X -> Gray.(transpose(X))); # actual image\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34307dc3",
   "metadata": {},
   "source": [
    "## Next time\n",
    "In lecture `L12c` we will continue our discussion of artificial neural networks, and then move on to a discussion of [Recurrent Neural Networks (RNNs)](https://en.wikipedia.org/wiki/Recurrent_neural_network). \n",
    "\n",
    "* In partciular, we'll discuss basic `RNN` architectures, and how these systems can be used to model data sequences. We will also discuss the [Long Short Term Memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) architecture (in `L12d`), which is a special case of an `RNN` that is particularly well suited for modeling time series data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
