{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750d357c-248b-4551-aa0c-985336b43d61",
   "metadata": {},
   "source": [
    "# L14d: Understanding the Self-Attention Mechanism\n",
    "In this lab, we'll examine two text embedding models: the Continuous Bag of Words (CBOW) and the Skip-Gram model, which are neural network models for learning word embeddings. \n",
    "* __Continuous Bag of Words (CBOW)__: This architecture predicts the target word based on its context words. It uses a shallow neural network to learn the embeddings of words in a given context. No positional information is used, and the model is trained to minimize the loss between the predicted and actual target word.\n",
    "* __Skip-Gram__: A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. Imagine you're reading a sentence and can guess the words that come before and after a particular word.\n",
    "\n",
    "See sections 2 and 3: [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we set up the computational environment and then specify a simple text sequence, e.g., a sentence without punctuation. From this sequence, we'll build a vocabulary, an inverse vocabulary, and the training datasets for the CBOW and skip-gram models. \n",
    "* __Task 2: Build and Train a CBOW model instance (20 min)__: In this task, we build and train a Continuous Bag of Words (CBOW) model instance on a sample input sequence. We start by creating a model instance, and then we train this instance for a few epochs, and finally, we see how the model performs.\n",
    "* __Task 3: Build and train a skip-gram model instance (20 min)__: In this task, we will build and train a skip-gram model instance on the sample input sequence we selected above. We start by creating a model instance, then train it for a few epochs and see how it performs.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201c04",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "In this task, we set up the computational environment and then specify a simple text sequence, e.g., a sentence without punctuation. From this sequence, we'll build a vocabulary, an inverse vocabulary, and the training datasets for the CBOW and skip-gram models. \n",
    "\n",
    "Let's start by setting up the environment, e.g., loading the required library and codes, loading the data, and preparing it for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506a8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140085e",
   "metadata": {},
   "source": [
    "Next, let's specify an example sentence, tokenize it, create a vocabulary, and an inverse vocabulary. \n",
    "* _What sentence to use?_ The example sentence we will work with can be whatever you want, as long as it consists of simple English words, no punctuation, and no control tokens.\n",
    "\n",
    "In the code below, we chop up the `sample_sentence::String` using [the `split(...)` method](https://docs.julialang.org/en/v1/base/strings/#Base.split), which tokenizes around a specified character, in this case the `space` character. We return the `words::Array{String,1}` array, the `vocabulary::Dict{String, Int64}` and the `inverse_vocabulary::Dict{Int64, String}` dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73d1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocabulary, inverse_vocabulary = let \n",
    "    \n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int}();\n",
    "    inverse_vocabulary = Dict{Int, String}();\n",
    "\n",
    "    # TDOD: specify a sample sentence -\n",
    "    sample_sentence = \"<bos> The quick brown fox jumps over the lazy dog my sample sentence and other stuff goes here <eos>\"; # Classical pangram!\n",
    "\n",
    "    # split -\n",
    "    words = split(sample_sentence, ' ') .|> String; # no external ordering\n",
    "\n",
    "    # build the vocabulary -\n",
    "    for (i, word) in enumerate(words)\n",
    "        vocabulary[word] = i;\n",
    "        inverse_vocabulary[i] = word;\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    words, vocabulary, inverse_vocabulary\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16b6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19-element Vector{String}:\n",
       " \"<bos>\"\n",
       " \"The\"\n",
       " \"quick\"\n",
       " \"brown\"\n",
       " \"fox\"\n",
       " \"jumps\"\n",
       " \"over\"\n",
       " \"the\"\n",
       " \"lazy\"\n",
       " \"dog\"\n",
       " \"my\"\n",
       " \"sample\"\n",
       " \"sentence\"\n",
       " \"and\"\n",
       " \"other\"\n",
       " \"stuff\"\n",
       " \"goes\"\n",
       " \"here\"\n",
       " \"<eos>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ded0f",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants for the model. These constants will be used throughout the example codes below. See the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(words); # size of the vocabulary\n",
    "windowsize = 5; # size of the context window # must be odd\n",
    "number_of_epochs = 10000; # number of epochs\n",
    "number_digit_array = range(1, stop=N, step=1) |> collect; # list of numbers from 1 to N\n",
    "β = 0.9; # Inverse temperature of the system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ce09b",
   "metadata": {},
   "source": [
    "### Compute the CBOW embedding\n",
    "In this task, we will build and train a CBOW model instance on the sample input sequence we specified above. We start by creating a model instance, then train it for a few epochs, and finally, we see how the model performs.\n",
    "\n",
    "Let's start with building the `cbow_model::Chain` instance. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to build (and train) the model. The input layer will be a mapping between the vocabulary size $N_{\\mathcal{V}}$ $\\rightarrow$ `windowsize::Int64` (hidden layer dimension). The output layer (which we run through a softmax) will be `windowsize::Int64` $\\rightarrow$ $N_{\\mathcal{V}}$. In both cases, we use the identity activation function, i.e, the transformations do not involve a nonlinear activation function.\n",
    "\n",
    "We save the (initially untrained) CBOW model in the `cbow_model::Chain` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdef9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(19 => 3),               \u001b[90m# 60 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 19),              \u001b[90m# 76 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m136 parameters, 752 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbow_model = let\n",
    "\n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(N, windowsize, identity),  # layer 1. Notice: identity activation function\n",
    "            hidden = Dense(windowsize, N, identity), # layer 2. Notice: identity activation function\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    cbow_model = MyModel().chain;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d107ac3a-f62c-435a-bfd5-4c1b1c903102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:weight, :bias, :σ)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fieldnames(typeof(cbow_model.layers[:hidden]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9874f64",
   "metadata": {},
   "source": [
    "__CBOW training dataset__: The `cbow_training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` array contains the context (input) and target word (output) for the `sample_sentence::String` where we slide a `windowsize::Int64` window along the sample string. The first element of [the `Tuple`](https://docs.julialang.org/en/v1/base/base/#Core.Tuple) stored in the training data will be the context words, while the second element will be the target word. All will be encoded in [one-hot](https://en.wikipedia.org/wiki/One-hot) format. \n",
    "* _Example_: The context words (input) are the flanking words around the target word. Suppose the `windowsize=3`, and the `sample_sentence` = `The quick brown ...`, the first training sample will have context words `The` and `brown` with `quick` being the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb85edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "    startindex = Int(C/2) + 1; # start index\n",
    "    endindex = N - Int(C/2); # end index\n",
    "    Δ = Int(C/2); # half the context window size\n",
    "\n",
    "    # build the training data -\n",
    "    for i ∈ startindex:endindex\n",
    "\n",
    "        # get the target word -\n",
    "        target_word = words[i]; # target word\n",
    "        target_word_index = vocabulary[target_word]; # target word index\n",
    "        y = onehot(target_word_index, number_digit_array); # target word one-hot vector\n",
    "\n",
    "        # Build the context list -\n",
    "        context_words_list = Vector{Vector{Float32}}();\n",
    "        context_index_array = range(i - Δ, stop=i + Δ, step=1) |> collect; # context index array\n",
    "        for j ∈ context_index_array\n",
    "\n",
    "            # get the context word -\n",
    "            if j == i\n",
    "                continue; # skip the target word\n",
    "            end\n",
    "\n",
    "            # get the context word -\n",
    "            context_word = words[j]; # context word\n",
    "            context_word_index = vocabulary[context_word]; # context word index\n",
    "            context_word_onehot = onehot(context_word_index, number_digit_array); # context word one-hot vector\n",
    "            push!(context_words_list, context_word_onehot); # add to the list of context words\n",
    "\n",
    "        end\n",
    "        x = sum(context_words_list, dims=1) |> vec; # concatenate the context words\n",
    "        \n",
    "        data_tuple = (x[1], y); # data tuple\n",
    "        push!(training_dataset, data_tuple); # add to the training dataset\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d526364",
   "metadata": {},
   "source": [
    "__Training__: In the code block below, we train the CBOW model instance using the `cbow_training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` array. We'll _minimize_ the [logitcrossentropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy) using [the `Momentum` optimizer](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers.Momentum) (all of which are exported by [the `Flux.jl` package](https://github.com/FluxML/Flux.jl))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc319dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 1000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 2000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 3000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 4000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 5000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 6000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 7000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 8000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 9000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 10000 of 10000 completed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(19 => 3),               \u001b[90m# 60 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 19),              \u001b[90m# 76 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m136 parameters, 752 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_cbow_model = let\n",
    "\n",
    "    localmodel = deepcopy(cbow_model); # make a local copy of the model\n",
    "\n",
    "    # setup the loss function -\n",
    "    loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "\n",
    "    # setup the optimizer\n",
    "    λ = 0.64; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(Momentum(λ,β), localmodel);\n",
    "\n",
    "    # training loop -\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "        Flux.train!(localmodel, cbow_training_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y) # loss function\n",
    "        end\n",
    "\n",
    "        # output for the user -\n",
    "        if (rem(i,1000) == 0)\n",
    "            @show \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the trained model -\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdfb54",
   "metadata": {},
   "source": [
    "Let's give the CBOW model a few inputs and see what it predicts. If we give it the original context, it should return the original target word. \n",
    "* _What get's returned?_ The network will return $p(w_{i}|\\mathbf{x})$, the probability of each word in the vocabulary being the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20ee99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,word) = let\n",
    "    \n",
    "    x = cbow_training_dataset[1][1]; # first training data\n",
    "    y = trained_cbow_model(x);\n",
    "    word = y |> argmax |> i-> inverse_vocabulary[i]; # index of the word\n",
    "\n",
    "    (x,y,word) # return the values\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb6161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{String}:\n",
       " \"<bos>\"\n",
       " \"quick\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x |> x-> findall(x-> x!= 0.0, x) .|> i-> inverse_vocabulary[i] # find the words in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa14fed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece56ba-e779-4b37-a9cd-2e8eafd25f92",
   "metadata": {},
   "source": [
    "__What does the embedding look like?__ Let's compute the hidden state $\\mathbf{h}$ (a low-dimensional embedded representation corresponding to our target word) for each target word in our sample sentence. We'll store these in the `CBOW_embedding_dictionary::Dict{String, Array{Float32,1}` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7261b4dc-c2ce-4dbe-97ef-d7a82a82ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_embedding_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    embedding_dictionary = Dict{String, Array{Float32,1}}();\n",
    "    number_of_training_examples = length(cbow_training_dataset);\n",
    "\n",
    "    # get the parameters from the trained model -\n",
    "    W₁ = trained_cbow_model.layers.input.weight\n",
    "    b₁ = trained_cbow_model.layers.input.bias\n",
    "    W₂ = trained_cbow_model.layers.hidden.weight\n",
    "    b₂ = trained_cbow_model.layers.hidden.bias\n",
    "\n",
    "    # let's compute all the embeddings -\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "\n",
    "        # what is h?\n",
    "        x = cbow_training_dataset[i][1]; # first training data\n",
    "        h = W₁*x + b₁;\n",
    "\n",
    "        # what is the key (target word) ?\n",
    "        pᵢ = W₂*h + b₂ |> u -> NNlib.softmax(u);\n",
    "        key = pᵢ |> argmax |> j -> inverse_vocabulary[j];\n",
    "\n",
    "        embedding_dictionary[key] = h;\n",
    "    end\n",
    "        \n",
    "    # return -\n",
    "    embedding_dictionary;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2243eb96-48fc-4f8b-b10b-05f8302d3335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Vector{Float32}} with 17 entries:\n",
       "  \"quick\"    => [2.42246, -3.12691, 3.06027]\n",
       "  \"my\"       => [-0.872877, 3.0572, 8.28969]\n",
       "  \"over\"     => [-10.1697, 3.05824, 0.111518]\n",
       "  \"and\"      => [5.08242, 6.8755, 3.15755]\n",
       "  \"here\"     => [8.19845, -3.7553, -0.896546]\n",
       "  \"sample\"   => [0.30528, 1.30486, 0.801785]\n",
       "  \"stuff\"    => [10.9274, 2.12803, 2.49473]\n",
       "  \"jumps\"    => [-2.36542, -7.34619, -2.66204]\n",
       "  \"the\"      => [-0.548648, -2.67539, -9.34289]\n",
       "  \"fox\"      => [-5.1697, -2.0258, 2.63307]\n",
       "  \"dog\"      => [1.96168, -2.99209, -3.7056]\n",
       "  \"brown\"    => [-2.98576, -8.92488, 3.29007]\n",
       "  \"The\"      => [-4.23359, -1.91239, -2.62269]\n",
       "  \"goes\"     => [4.06902, 2.58961, -3.30548]\n",
       "  \"lazy\"     => [-4.83329, 3.76046, 5.12293]\n",
       "  \"other\"    => [-0.2255, 6.22819, -7.1791]\n",
       "  \"sentence\" => [-2.64456, 6.17913, -0.985498]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CBOW_embedding_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d1d25",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba954bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, embedded_words_list = let\n",
    "    \n",
    "    # initialize -\n",
    "    X = zeros(Float32, N-2, windowsize); # matrix of zeros\n",
    "    embedded_words_list = Vector{String}();\n",
    "\n",
    "    # iterate over the words -\n",
    "    linearindex = 1;\n",
    "    for word ∈ words[2:end-1]\n",
    "        # get the embedding -\n",
    "        embedding = CBOW_embedding_dictionary[word]; # get the embedding\n",
    "        X[linearindex, :] = embedding; # add to the matrix\n",
    "        linearindex += 1; # increment the index\n",
    "\n",
    "        push!(embedded_words_list, word); # add to the list of embedded words\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    X, embedded_words_list;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc459931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17-element Vector{String}:\n",
       " \"The\"\n",
       " \"quick\"\n",
       " \"brown\"\n",
       " \"fox\"\n",
       " \"jumps\"\n",
       " \"over\"\n",
       " \"the\"\n",
       " \"lazy\"\n",
       " \"dog\"\n",
       " \"my\"\n",
       " \"sample\"\n",
       " \"sentence\"\n",
       " \"and\"\n",
       " \"other\"\n",
       " \"stuff\"\n",
       " \"goes\"\n",
       " \"here\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedded_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559567f5",
   "metadata": {},
   "source": [
    "## Task 2: Look at Attention in a Modern Hopfield Network\n",
    "In this task, we revisit the Hopfield network, and particularly the modern Hopfield incantation, which is a recurrent neural network (RNN) that can be used for associative memory. \n",
    "\n",
    "A modern Hopfield network addresses many of the perceived limitations of the original Hopfield network. The original Hopfield network was limited to binary values and could only store a limited number of patterns. The modern Hopfield network uses continuous values and can store a large number of patterns.\n",
    "* For a detailed discussion of the key milestones in the development of modern Hopfield networks, check out [Hopfield Networks is All You Need Blog, GitHub.io](https://ml-jku.github.io/hopfield-layers/)\n",
    "\n",
    "### Algorithm\n",
    "The user provides a set of memory vectors $\\mathbf{X} = \\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{m}\\right\\}$, where $\\mathbf{x}_{i} \\in \\mathbb{R}^{n}$ is a memory vector of size $n$ and $m$ is the number of memory vectors. Further, the user provides an initial _partial memory_ $\\mathbf{s}_{\\circ} \\in \\mathbb{R}^{n}$, which is a vector of size $n$ that is a partial version of one of the memory vectors and specifies the _temperature_ $\\beta$ of the system.\n",
    "\n",
    "__Initialize__ the network with the memory vectors $\\mathbf{X}$, and the inverse temperature $\\beta$. Set current state to the initial state $\\mathbf{s} \\gets \\mathbf{s}_{\\circ}$\n",
    "\n",
    "Until convergence __do__:\n",
    "   1. Compute the _current_ probability vector defined as $\\mathbf{p} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s})$ where $\\mathbf{s}$ is the _current_ state vector, and $\\mathbf{X}^{\\top}$ is the transpose of the memory matrix $\\mathbf{X}$.\n",
    "   2. Compute the _next_ state vector $\\mathbf{s}^{\\prime} = \\mathbf{X}\\mathbf{p}$ and the _next_ probability vector $\\mathbf{p}^{\\prime} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s}^{\\prime})$.\n",
    "   3. If $\\mathbf{p}^{\\prime}$ is _close_ to $\\mathbf{p}$ or we run out of iterations, then __stop__. For example, $\\lVert \\mathbf{p}^{\\prime} - \\mathbf{p}\\rVert_{2}^{2} \\leq \\epsilon$ for some small $\\epsilon > 0$.\n",
    "   4. Otherwise, update the state $\\mathbf{s} \\gets\\mathbf{s}^{\\prime}$, and __go back to__ step 1.\n",
    "\n",
    "   \n",
    "This algorithm is implemented in [the `recover(...)` method](src/Compute.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec00f",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Let's start by creating a model of a modern Hopfield network. \n",
    "* We'll construct [a `MyModernHopfieldNetworkModel` instance](src/Types.jl) using a custom [`build(...)` function](src/Factory.jl). The [`build(...)` method](src/Factory.jl) takes the type of thing we want to build, the (linearized) image library we want to encode, and the (inverse) system temperature $\\beta$ as inputs — images along the columns.\n",
    "* The [`build(...)` function](src/Factory.jl) returns a `MyModernHopfieldNetworkModel` instance, where the image library is stored in the `X::Array{Float32,2}` field, and the system temperature is stored in the `β::Float64` field.\n",
    "\n",
    "We'll store the problem instance in the `model::MyModernHopfieldNetworkModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6dc31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_words_to_learn = size(X,1);\n",
    "    linearwordcollection = Array{Float32,2}(undef, windowsize, number_of_words_to_learn); # words on columns\n",
    "    index_vector = range(1,stop=number_of_words_to_learn, step=1) |> collect; # # turn our set into a sorted vector - we'll process this in the sorted order \n",
    "\n",
    "    # populate the data array that we give to the model\n",
    "    for k ∈ eachindex(index_vector)\n",
    "        j = index_vector[k]; # what image index will we load?\n",
    "        sₖ = X[j,:]; # original data, vectorized\n",
    "        \n",
    "        for i ∈ 1:windowsize\n",
    "            linearwordcollection[i,k] = sₖ[i];  # fill the columns of the array with the image data\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # build model -\n",
    "    model = build(MyModernHopfieldNetworkModel, (\n",
    "            memories = linearwordcollection, # this is the data we want to memorize. Images on columns\n",
    "            β = β, # Inverse temperature of the system. A big beta means we are more likely to get the right answer\n",
    "    ));\n",
    "\n",
    "    model; # return the model to the calling scope\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "760a07c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.β"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9240bce",
   "metadata": {},
   "source": [
    "We implemented the modern Hopfield recovery algorithm above in [the `recover(...)` method](src/Compute.jl). This method takes our `model::MyModernHopfieldNetworkModel` instance, the initial configuration vector `sₒ::Array{Int32,1}`, and the maximum number `maxiterations::Int64`, and iteration tolerance parameter `ϵ::Float64`. \n",
    "* [The `recover(...)` method](src/Compute.jl) returns the recovered word in the `s₁::Array{Float32,1}` variable, the word at each iteration in the `f::Dict{Int, Array{Float32,2}}` dictionary, and the probability of the word at each iteration in the `p::Dict{Int, Array{Float32,2}}` variable. The frames and probability dictionaries are indexed from `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b714799",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_word_index, s₁, f, p = let \n",
    "    \n",
    "    starting_word_index = 5; # index of the starting word\n",
    "    sₒ = X[starting_word_index,:]; # initial state\n",
    "    (s₁,f,p) = recover(model, sₒ, maxiterations = 10000, ϵ = 1e-16); # iterate until we hit stop condition\n",
    "\n",
    "    # return -\n",
    "    starting_word_index, s₁,f,p\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4abd066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 6\n"
     ]
    }
   ],
   "source": [
    "println(\"How many iterations: $(length(f))\") # how many iterations did we need to converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded8a47",
   "metadata": {},
   "source": [
    "__Check__: Let's check to see if the recovered word is identical to the original word (not guaranteed). We can do this by checking the `s₁::Array{Float32,1}` variable against the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed76135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting word: jumps and recovered word: brown\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    n = length(f) - 1; # number of iterations (last index)\n",
    "    recovered_memory = p[n] |> argmax |> i-> embedded_words_list[i] # index of the most probable word\n",
    "    starting_word = embedded_words_list[starting_word_index]; # starting word\n",
    "    println(\"Starting word: $starting_word and recovered word: $recovered_memory\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1d650",
   "metadata": {},
   "source": [
    "## Task 3: Scaled Dot-Product Attention\n",
    "In this task, we explore the scaled dot-product attention mechanism. The scaled dot-product attention is a key component of the transformer architecture, which has revolutionized natural language processing and other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b09cdca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b82468d9",
   "metadata": {},
   "source": [
    "### Single Query Attention\n",
    "Now, let's implement the single query attention mechanism. Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f767f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, α = let\n",
    "\n",
    "    # initalize -\n",
    "    d_in = windowsize; # input dimension\n",
    "    d_attn = 3; # attention dimension \n",
    "    d_out = windowsize; # output dimension\n",
    "    number_of_key_vectors = size(X,1); # number of key vectors \n",
    "    example_word_index = 4; # index of the example word\n",
    "    w = X[example_word_index,:]; # raw embededd vector\n",
    "    W = zeros(Float32, number_of_key_vectors, d_in); # initialize the embedding matrix\n",
    "    k = Vector{Vector{Float32}}(undef, number_of_key_vectors); # initialize the key vectors\n",
    "    v = Vector{Vector{Float32}}(undef, number_of_key_vectors); # initialize the value vector\n",
    "\n",
    "    # populate the W - matrix\n",
    "    for i ∈ 1:number_of_key_vectors\n",
    "        W[i, :] = X[i,:]; # get the embedding\n",
    "    end\n",
    "\n",
    "    # generate some random weights and biases -\n",
    "    W₁ = Matrix{Float32}(I,d_attn,d_in); # weights W_q\n",
    "    b₁ = randn(Float32,d_attn); # bias b_q\n",
    "    W₂ = Matrix{Float32}(I,d_attn,d_in); # weights W_k\n",
    "    b₂ = zeros(Float32,d_attn); # bias b_k\n",
    "    W₃ = Matrix{Float32}(I,d_out,d_in); # weights W_v\n",
    "    b₃ = zeros(Float32,d_out); # bias b_v\n",
    "\n",
    "    # compute the query, key and value -\n",
    "    q = W₁ * w + b₁; # query vector\n",
    "    for i ∈ 1:number_of_key_vectors\n",
    "        k[i] = W₂ * W[i, :] + b₂; # key\n",
    "        v[i] = W₃ * W[i, :] + b₃; # value\n",
    "    end\n",
    "\n",
    "    # compute the attention scores -\n",
    "    s = zeros(Float32, number_of_key_vectors); # initialize the attention scores\n",
    "    for i ∈ 1:number_of_key_vectors\n",
    "        s[i] = (1/sqrt(d_attn))*dot(q, k[i]); # attention score\n",
    "    end\n",
    "    α = NNlib.softmax(s); # attention weights\n",
    "   \n",
    "    (q, k, v, α) # return the value and attention weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd3e7604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float32}:\n",
       " -2.752718\n",
       " -1.4726125\n",
       "  2.618626"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d4e757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 transpose(::Vector{Float32}) with eltype Float32:\n",
       " -3.24818  -8.29206  3.2086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transpose(v)*α # weighted sum of the value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddbc73-142e-4aac-b6fd-2a8cd71466e5",
   "metadata": {},
   "source": [
    "## What's coming up?\n",
    "In lecture `L15a`, we'll look at alternatives to [the attention mechanism](https://en.wikipedia.org/wiki/Attention_(machine_learning)) that underpins most [large language models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model). What comes after attention and transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b92b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
