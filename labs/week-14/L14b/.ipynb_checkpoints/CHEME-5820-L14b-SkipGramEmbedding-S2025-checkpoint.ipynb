{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750d357c-248b-4551-aa0c-985336b43d61",
   "metadata": {},
   "source": [
    "# L14b: Fun with Text Embedding Models\n",
    "In this lab, we'll examine two text embedding models: the Continuous Bag of Words (CBOW) and the Skip-Gram model, which are neural network models for learning word embeddings. \n",
    "* __Continuous Bag of Words (CBOW)__: This architecture predicts the target word based on its context words. It uses a shallow neural network to learn the embeddings of words in a given context. No positional information is used, and the model is trained to minimize the loss between the predicted and actual target word.\n",
    "* __Skip-Gram__: A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. Imagine you're reading a sentence and can guess the words that come before and after a particular word.\n",
    "\n",
    "See sections 2 and 3: [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we set up the computational environment and then specify a simple text sequence, e.g., a sentence without punctuation. From this sequence, we'll build a vocabulary, an inverse vocabulary, and the training datasets for the CBOW and skip-gram models. \n",
    "* __Task 2: Build and Train a CBOW model instance (20 min)__: In this task, we build and train a Continuous Bag of Words (CBOW) model instance on a sample input sequence. We start by creating a model instance, and then we train this instance for a few epochs, and finally, we see how the model performs.\n",
    "* __Task 3: Build and train a skip-gram model instance (20 min)__: In this task, we will build and train a skip-gram model instance on the sample input sequence we selected above. We start by creating a model instance, then train it for a few epochs and see how it performs.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201c04",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "In this task, we set up the computational environment and then specify a simple text sequence, e.g., a sentence without punctuation. From this sequence, we'll build a vocabulary, an inverse vocabulary, and the training datasets for the CBOW and skip-gram models. \n",
    "\n",
    "Let's start by setting up the environment, e.g., loading the required library and codes, loading the data, and preparing it for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506a8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140085e",
   "metadata": {},
   "source": [
    "Next, let's specify an example sentence, tokenize it, create a vocabulary, and an inverse vocabulary. \n",
    "* _What sentence to use?_ The example sentence we will work with can be whatever you want, as long as it consists of simple English words, no punctuation, and no control tokens.\n",
    "\n",
    "In the code below, we chop up the `sample_sentence::String` using [the `split(...)` method](https://docs.julialang.org/en/v1/base/strings/#Base.split), which tokenizes around a specified character, in this case the `space` character. We return the `words::Array{String,1}` array, the `vocabulary::Dict{String, Int64}` and the `inverse_vocabulary::Dict{Int64, String}` dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73d1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocabulary, inverse_vocabulary = let \n",
    "    \n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int}();\n",
    "    inverse_vocabulary = Dict{Int, String}();\n",
    "\n",
    "    # TDOD: specify a sample sentence -\n",
    "    sample_sentence = \"The quick brown fox jumps over the lazy dog\"; # Classical pangram!\n",
    "\n",
    "    # split -\n",
    "    words = split(sample_sentence, ' ') .|> String; # no external ordering\n",
    "\n",
    "    # build the vocabulary -\n",
    "    for (i, word) in enumerate(words)\n",
    "        vocabulary[word] = i;\n",
    "        inverse_vocabulary[i] = word;\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    words, vocabulary, inverse_vocabulary\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b16b6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, String} with 9 entries:\n",
       "  5 => \"jumps\"\n",
       "  4 => \"fox\"\n",
       "  6 => \"over\"\n",
       "  7 => \"the\"\n",
       "  2 => \"quick\"\n",
       "  9 => \"dog\"\n",
       "  8 => \"lazy\"\n",
       "  3 => \"brown\"\n",
       "  1 => \"The\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ded0f",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants for the model. These constants will be used throughout the example codes below. See the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ea0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(words); # size of the vocabulary\n",
    "windowsize = 3; # size of the context window # must be odd\n",
    "number_of_epochs = 10000; # number of epochs\n",
    "number_digit_array = range(1, stop=N, step=1) |> collect; # list of numbers from 1 to N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a64b2c",
   "metadata": {},
   "source": [
    "__CBOW training dataset__: The `cbow_training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` array contains the context (input) and target word (output) for the `sample_sentence::String` where we slide a `windowsize::Int64` window along the sample string. The first element of [the `Tuple`](https://docs.julialang.org/en/v1/base/base/#Core.Tuple) stored in the training data will be the context words, while the second element will be the target word. All will be encoded in [one-hot](https://en.wikipedia.org/wiki/One-hot) format. \n",
    "* _Example_: The context words (input) are the flanking words around the target word. Suppose the `windowsize=3`, and the `sample_sentence` = `The quick brown ...`, the first training sample will have context words `The` and `brown` with `quick` being the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95570b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "\n",
    "    # build the training data -\n",
    "    for i ∈ 2:(N-1)\n",
    "        \n",
    "        targetword = words[i]; # target word\n",
    "        contextwords = words[(i-1):(i+1)] |> v-> [v[1], v[end]] # context words\n",
    "        \n",
    "        # proces the target word -\n",
    "        targetword_index = vocabulary[targetword]; # index of the target word\n",
    "        y = onehot(targetword_index, number_digit_array); # one-hot encoding of the target word\n",
    "\n",
    "        # process the context words -\n",
    "        tmp = Array{Float32,2}(undef, N, C); # temporary array\n",
    "        for (j,word) in enumerate(contextwords)\n",
    "            contextword_index = vocabulary[word]; # index of the context word\n",
    "            z = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the context word\n",
    "            tmp[:, j] .= z; # store the context word\n",
    "        end\n",
    "        x = sum(tmp, dims=2) |> vec .|> Float32; # average of the context words\n",
    "        \n",
    "        # store the training data -\n",
    "        push!(training_dataset, (x, y)); # store the training data\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d378e1",
   "metadata": {},
   "source": [
    "__Skip Gram training datatset__: The `skip_gram_training_dataset::Vector{Tuple{Vector{Float32}, Vector{Float32}}}` array will be in the _inverse_ of the CBOW training data. We give the network a target word (the first element [of the `Tuple`](https://docs.julialang.org/en/v1/base/base/#Core.Tuple)), and we predict the context words flanking the target word (the second element of training data [`Tuple`](https://docs.julialang.org/en/v1/base/base/#Core.Tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53163669",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram_training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, Vector{Float32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "\n",
    "    # build the training data -\n",
    "    for i ∈ 2:(N-1)\n",
    "        \n",
    "        contextword = words[i]; # target word\n",
    "        targetwords = words[(i-1):(i+1)] |> v-> [v[1], v[3]] # context words\n",
    "\n",
    "        # proces the context word -\n",
    "        contextword_index = vocabulary[contextword]; # index of the target word\n",
    "        x = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the target word\n",
    "\n",
    "        # @show contextword, contextword_index, targetwords, x; # show the context word and the target words\n",
    "\n",
    "        # process the targets words -\n",
    "        tmp = Array{Float32,2}(undef, N, C); # temporary array\n",
    "        for (j,word) in enumerate(targetwords);\n",
    "            contextword_index = vocabulary[word]; # index of the context word\n",
    "            z = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the context word\n",
    "            tmp[:, j] .= z; # store the context word\n",
    "        end\n",
    "        y = sum(tmp, dims=2) |> vec .|> Float32; # average of the context words\n",
    "        \n",
    "        # store the training data -\n",
    "        push!(training_dataset, (x, y)); # store the training data\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bcd8b44-20fe-495d-815c-473097e9c0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_training_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ce09b",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train a CBOW model instance\n",
    "In this task, we will build and train a CBOW model instance on the sample input sequence we specified above. We start by creating a model instance, then train it for a few epochs, and finally, we see how the model performs.\n",
    "\n",
    "Let's start with building the `cbow_model::Chain` instance. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to build (and train) the model. The input layer will be a mapping between the vocabulary size $N_{\\mathcal{V}}$ $\\rightarrow$ `windowsize::Int64` (hidden layer dimension). The output layer (which we run through a softmax) will be `windowsize::Int64` $\\rightarrow$ $N_{\\mathcal{V}}$. In both cases, we use the identity activation function, i.e, the transformations do not involve a nonlinear activation function.\n",
    "\n",
    "We save the (initially untrained) CBOW model in the `cbow_model::Chain` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bdef9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(9 => 3),                \u001b[90m# 30 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 9),               \u001b[90m# 36 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m66 parameters, 472 bytes."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model = let\n",
    "\n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(N, windowsize, identity),  # layer 1. Notice: identity activation function\n",
    "            hidden = Dense(windowsize, N, identity), # layer 2. Notice: identity activation function\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    cbow_model = MyModel().chain;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d107ac3a-f62c-435a-bfd5-4c1b1c903102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:weight, :bias, :σ)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldnames(typeof(cbow_model.layers[:input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d526364",
   "metadata": {},
   "source": [
    "__Training__: In the code block below, we train the CBOW model instance using the `cbow_training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` array. We'll _minimize_ the [logitcrossentropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy) using [the `Momentum` optimizer](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers.Momentum) (all of which are exported by [the `Flux.jl` package](https://github.com/FluxML/Flux.jl))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc319dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 1000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 2000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 3000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 4000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 5000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 6000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 7000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 8000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 9000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 10000 of 10000 completed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(9 => 3),                \u001b[90m# 30 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 9),               \u001b[90m# 36 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m66 parameters, 472 bytes."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_cbow_model = let\n",
    "\n",
    "    localmodel = deepcopy(cbow_model); # make a local copy of the model\n",
    "\n",
    "    # setup the loss function -\n",
    "    loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "\n",
    "    # setup the optimizer\n",
    "    λ = 0.64; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(Momentum(λ,β), localmodel);\n",
    "\n",
    "    # training loop -\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "        Flux.train!(localmodel, cbow_training_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y) # loss function\n",
    "        end\n",
    "\n",
    "        # output for the user -\n",
    "        if (rem(i,1000) == 0)\n",
    "            @show \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the trained model -\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdfb54",
   "metadata": {},
   "source": [
    "Let's give the CBOW model a few inputs and see what it predicts. If we give it the original context, it should return the original target word. \n",
    "* _What get's returned?_ The network will return $p(w_{i}|\\mathbf{x})$, the probability of each word in the vocabulary being the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a20ee99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,word) = let\n",
    "    \n",
    "    x = cbow_training_dataset[1][1]; # first training data\n",
    "    y = trained_cbow_model(x);\n",
    "    word = y |> argmax |> i-> inverse_vocabulary[i]; # index of the word\n",
    "\n",
    "    (x,y,word) # return the values\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cb6161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{String}:\n",
       " \"The\"\n",
       " \"brown\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x |> x-> findall(x-> x!= 0.0, x) .|> i-> inverse_vocabulary[i] # find the words in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa14fed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"quick\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece56ba-e779-4b37-a9cd-2e8eafd25f92",
   "metadata": {},
   "source": [
    "__What does the embedding look like?__ Let's compute the hidden state $\\mathbf{h}$ (a low-dimensional embedded representation corresponding to our target word) for each target word in our sample sentence. We'll store these in the `CBOW_embedding_dictionary::Dict{String, Array{Float32,1}` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7261b4dc-c2ce-4dbe-97ef-d7a82a82ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_embedding_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    embedding_dictionary = Dict{String, Array{Float32,1}}();\n",
    "    number_of_training_examples = length(cbow_training_dataset);\n",
    "\n",
    "    # get the parameters from the trained model -\n",
    "    W₁ = trained_cbow_model.layers.input.weight\n",
    "    b₁ = trained_cbow_model.layers.input.bias\n",
    "    W₂ = trained_cbow_model.layers.hidden.weight\n",
    "    b₂ = trained_cbow_model.layers.hidden.bias\n",
    "\n",
    "    # let's compute all the embeddings -\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "\n",
    "        # what is h?\n",
    "        x = cbow_training_dataset[i][1]; # first training data\n",
    "        h = W₁*x + b₁;\n",
    "\n",
    "        # what is the key (target word) ?\n",
    "        pᵢ = W₂*h + b₂ |> u -> NNlib.softmax(u);\n",
    "        key = pᵢ |> argmax |> j -> inverse_vocabulary[j];\n",
    "\n",
    "        embedding_dictionary[key] = h;\n",
    "    end\n",
    "        \n",
    "    # return -\n",
    "    embedding_dictionary;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2243eb96-48fc-4f8b-b10b-05f8302d3335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Vector{Float32}} with 7 entries:\n",
       "  \"brown\" => [-4.12924, -0.591874, -2.55188]\n",
       "  \"jumps\" => [-2.11527, 4.3435, -3.82844]\n",
       "  \"lazy\"  => [-3.41541, -1.97225, 4.19899]\n",
       "  \"the\"   => [-1.42853, 3.7855, 1.36336]\n",
       "  \"quick\" => [4.82387, 2.12251, 2.98309]\n",
       "  \"fox\"   => [4.56134, -2.00698, -1.09246]\n",
       "  \"over\"  => [-0.0999005, -4.99337, 0.041354]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW_embedding_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1d650",
   "metadata": {},
   "source": [
    "## Task 3: Build and train a skip-gram model instance\n",
    "In this task, we will build and train a skip-gram model instance on the sample input sequence we selected above. We start by creating a model instance, and then we train this instance for a few epochs, and see how the model performs. \n",
    "\n",
    "Let's start with building the `skip_gram_model::Chain` instance. We'll use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to build (and train) the model. The input layer will be a mapping between the vocabulary size $N_{\\mathcal{V}}$ $\\rightarrow$ `windowsize::Int64` (hidden layer dimension). The output layer (which we run through a softmax) will be `windowsize::Int64` $\\rightarrow$ $N_{\\mathcal{V}}$. In both cases, we use the identity activation function, i.e, the transformations do not involve a nonlinear activation function.\n",
    "\n",
    "We save the (initially untrained) skip gram model in the `skip_gram_model::Chain` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95041dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(9 => 3),                \u001b[90m# 30 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 9),               \u001b[90m# 36 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m66 parameters, 472 bytes."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_model = let\n",
    "\n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(N, windowsize, identity),  # layer 1\n",
    "            hidden = Dense(windowsize, N, identity), # layer 2\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    skip_gram_model = MyModel().chain;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b228cd",
   "metadata": {},
   "source": [
    "__Training__: In the code block below, we train the skip-gram model instance using the `skip_gram_training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` array. We'll _minimize_ the [logitcrossentropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy) using [the `Momentum` optimizer](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers.Momentum) (all of which are exported by [the `Flux.jl` package](https://github.com/FluxML/Flux.jl))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6968c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 1000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 2000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 3000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 4000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 5000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 6000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 7000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 8000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 9000 of 10000 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 10000 of 10000 completed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(9 => 3),                \u001b[90m# 30 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 9),               \u001b[90m# 36 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m66 parameters, 472 bytes."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_skip_gram_model = let\n",
    "\n",
    "    localmodel = deepcopy(skip_gram_model); # make a local copy of the model\n",
    "\n",
    "    # setup the loss function -\n",
    "    loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "\n",
    "    # setup the optimizer\n",
    "    λ = 0.61; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(AdaBelief(), localmodel); # changed Momentum to AdaBelief: https://arxiv.org/abs/2010.07468\n",
    "\n",
    "    # training loop -\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "        Flux.train!(localmodel, skip_gram_training_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y) # loss function\n",
    "        end\n",
    "\n",
    "        # some output for the user .... short attention span ...\n",
    "        if (rem(i,1000) == 0)\n",
    "            @show \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the trained model -\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab0382",
   "metadata": {},
   "source": [
    "Let's give the skip-gram model a few inputs and see what it predicts. If we give it the original context, it should return the original target words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f3affde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x₂,y₂,ŷ₂) = let\n",
    "    \n",
    "    example_index = 4;\n",
    "    skip_gram_context = skip_gram_training_dataset[example_index][1]; # first training data\n",
    "    skip_gram_target_actual = skip_gram_training_dataset[example_index][2]; # this is what *should* see\n",
    "    skip_gram_target_model = trained_skip_gram_model(skip_gram_context); # this what we actually see?\n",
    "\n",
    "    (skip_gram_context, skip_gram_target_actual, skip_gram_target_model)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f9aa87db-fecb-4b30-83ed-ac1176358e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Matrix{Float32}:\n",
       " 0.0  0.0  9.39936f-14\n",
       " 0.0  0.0  6.20029f-7\n",
       " 0.0  0.0  2.60902f-13\n",
       " 0.0  1.0  0.49972\n",
       " 1.0  0.0  8.35616f-8\n",
       " 0.0  1.0  0.500279\n",
       " 0.0  0.0  2.58987f-9\n",
       " 0.0  0.0  8.15599f-7\n",
       " 0.0  0.0  8.11161f-9"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x₂ y₂ ŷ₂]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddbc73-142e-4aac-b6fd-2a8cd71466e5",
   "metadata": {},
   "source": [
    "## What's coming up next time?\n",
    "In lecture `L14c`, we'll look at [the attention mechanism](https://en.wikipedia.org/wiki/Attention_(machine_learning)) that underpins most [large language models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model). Want to get ahead?\n",
    "* __Check out__: [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All You Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
