{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750d357c-248b-4551-aa0c-985336b43d61",
   "metadata": {},
   "source": [
    "# L14b: The Skip-Gram Embedding Model\n",
    "In this lab, we'll look at the Skip-Gram model, which is a neural network model for learning word embeddings. This is the second text embedding model we'll cover in this course. \n",
    "* __Continuous Bag of Words (CBOW)__: This architecture predicts the target word based on its context words. It uses a shallow neural network to learn the embeddings of words in a given context. No positional information is used, and the model is trained to minimize the loss between the predicted and actual target word.\n",
    "* __Skip-Gram__: A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. Imagine you're reading a sentence and can guess the words that come before and after a particular word.\n",
    "\n",
    "See section 2: [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we'll load a public dataset of headlines curated as either sarcastic or not sarcastic. Our dataset is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection). After loading the data, we'll tokenize the data (convert text strings to numerical arrays).\n",
    "* __Task 2: Build and Train a HiPPO-LegS model instance (15 min)__: In this task, we will build and train a HiPPO-S4-LegS model instance on the sample input sequence we selected above. We start by creating a model instance, and the we train this instance for different hidden state sizes.\n",
    "* __Task 3: Does the S4 model generalize? (25 min)__: In this task, we'll explore how the S4-LegS model performs when we give input sequences that are _similar_ but not the same as the training data. We'll take the training data, perturb some words, and feed the perturbed sequence into the model.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201c04",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "In this task, we'll set up the environment, load the data, and prepare it for training. We'll also install the required libraries and load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "506a8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140085e",
   "metadata": {},
   "source": [
    "Next, let's specify an example sentence, tokenize it and create a vocabulary. We'll also create a mapping from words to indices and vice versa. This will help us convert the text data into numerical arrays that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocabulary, inverse_vocabulary = let \n",
    "    \n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int}();\n",
    "    inverse_vocabulary = Dict{Int, String}();\n",
    "\n",
    "    # TDOD: specify a sample sentence -\n",
    "    sample_sentence = \"The quick brown fox jumps over the lazy dog\"; # Classical pangram!\n",
    "\n",
    "    # split -\n",
    "    words = split(sample_sentence, \" \") .|> lowercase |> unique; # no external ordering\n",
    "\n",
    "    # build the vocabulary -\n",
    "    for (i, word) in enumerate(words)\n",
    "        vocabulary[word] = i;\n",
    "        inverse_vocabulary[i] = word;\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    words, vocabulary, inverse_vocabulary\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16b6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{String}:\n",
       " \"the\"\n",
       " \"quick\"\n",
       " \"brown\"\n",
       " \"fox\"\n",
       " \"jumps\"\n",
       " \"over\"\n",
       " \"lazy\"\n",
       " \"dog\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ded0f",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants for the model. These constants will be used throughout the example codes below. See the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(words); # size of the vocabulary\n",
    "windowsize = 3; # size of the context window\n",
    "number_of_epochs = 1000; # number of epochs\n",
    "number_digit_array = range(1, stop=N, step=1) |> collect; # list of numbers from 1 to N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a64b2c",
   "metadata": {},
   "source": [
    "Fill me on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95570b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "\n",
    "    # build the training data -\n",
    "    for i âˆˆ 2:(N-1)\n",
    "        \n",
    "        targetword = words[i]; # target word\n",
    "        contextwords = words[(i-1):(i+1)] |> v-> [v[1], v[3]] # context words\n",
    "        \n",
    "        # proces the target word -\n",
    "        targetword_index = vocabulary[targetword]; # index of the target word\n",
    "        y = onehot(targetword_index, number_digit_array); # one-hot encoding of the target word\n",
    "\n",
    "        # process the context words -\n",
    "        tmp = Array{Float32,2}(undef, N, C); # temporary array\n",
    "        for (j,word) in enumerate(contextwords)\n",
    "            contextword_index = vocabulary[word]; # index of the context word\n",
    "            x = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the context word\n",
    "            tmp[:, j] .= x; # store the context word\n",
    "        end\n",
    "        x = sum(tmp, dims=2) |> vec .|> Float32; # average of the context words\n",
    "        \n",
    "        # store the training data -\n",
    "        push!(training_dataset, (x, y)); # store the training data\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc8df968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ce09b",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train a CBOW model instance\n",
    "In this task, we will build and train a CBOW model instance on the sample input sequence we selected above. We start by creating a model instance, and the we train this instance for a few epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
