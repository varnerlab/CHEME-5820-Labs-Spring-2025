{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750d357c-248b-4551-aa0c-985336b43d61",
   "metadata": {},
   "source": [
    "# L14b: The Skip-Gram Embedding Model\n",
    "In this lab, we'll look at the Skip-Gram model, which is a neural network model for learning word embeddings. This is the second text embedding model we'll cover in this course. \n",
    "* __Continuous Bag of Words (CBOW)__: This architecture predicts the target word based on its context words. It uses a shallow neural network to learn the embeddings of words in a given context. No positional information is used, and the model is trained to minimize the loss between the predicted and actual target word.\n",
    "* __Skip-Gram__: A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. Imagine you're reading a sentence and can guess the words that come before and after a particular word.\n",
    "\n",
    "See section 2: [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we'll load a public dataset of headlines curated as either sarcastic or not sarcastic. Our dataset is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection). After loading the data, we'll tokenize the data (convert text strings to numerical arrays).\n",
    "* __Task 2: Build and Train a HiPPO-LegS model instance (15 min)__: In this task, we will build and train a HiPPO-S4-LegS model instance on the sample input sequence we selected above. We start by creating a model instance, and the we train this instance for different hidden state sizes.\n",
    "* __Task 3: Does the S4 model generalize? (25 min)__: In this task, we'll explore how the S4-LegS model performs when we give input sequences that are _similar_ but not the same as the training data. We'll take the training data, perturb some words, and feed the perturbed sequence into the model.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201c04",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "In this task, we'll set up the environment, load the data, and prepare it for training. We'll also install the required libraries and load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506a8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140085e",
   "metadata": {},
   "source": [
    "Next, let's specify an example sentence, tokenize it and create a vocabulary. We'll also create a mapping from words to indices and vice versa. This will help us convert the text data into numerical arrays that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73d1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, vocabulary, inverse_vocabulary = let \n",
    "    \n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int}();\n",
    "    inverse_vocabulary = Dict{Int, String}();\n",
    "\n",
    "    # TDOD: specify a sample sentence -\n",
    "    sample_sentence = \"The quick brown fox jumps over the lazy dog\"; # Classical pangram!\n",
    "\n",
    "    # split -\n",
    "    words = split(sample_sentence, \" \") .|> lowercase |> unique; # no external ordering\n",
    "\n",
    "    # build the vocabulary -\n",
    "    for (i, word) in enumerate(words)\n",
    "        vocabulary[word] = i;\n",
    "        inverse_vocabulary[i] = word;\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    words, vocabulary, inverse_vocabulary\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16b6919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{String}:\n",
       " \"the\"\n",
       " \"quick\"\n",
       " \"brown\"\n",
       " \"fox\"\n",
       " \"jumps\"\n",
       " \"over\"\n",
       " \"lazy\"\n",
       " \"dog\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ded0f",
   "metadata": {},
   "source": [
    "__Constants__: Let's set up some constants for the model. These constants will be used throughout the example codes below. See the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ea0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = length(words); # size of the vocabulary\n",
    "windowsize = 3; # size of the context window\n",
    "number_of_epochs = 100; # number of epochs\n",
    "number_digit_array = range(1, stop=N, step=1) |> collect; # list of numbers from 1 to N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a64b2c",
   "metadata": {},
   "source": [
    "__CBOW training datatset__: Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95570b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "\n",
    "    # build the training data -\n",
    "    for i ∈ 2:(N-1)\n",
    "        \n",
    "        targetword = words[i]; # target word\n",
    "        contextwords = words[(i-1):(i+1)] |> v-> [v[1], v[3]] # context words\n",
    "        \n",
    "        # proces the target word -\n",
    "        targetword_index = vocabulary[targetword]; # index of the target word\n",
    "        y = onehot(targetword_index, number_digit_array); # one-hot encoding of the target word\n",
    "\n",
    "        # process the context words -\n",
    "        tmp = Array{Float32,2}(undef, N, C); # temporary array\n",
    "        for (j,word) in enumerate(contextwords)\n",
    "            contextword_index = vocabulary[word]; # index of the context word\n",
    "            x = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the context word\n",
    "            tmp[:, j] .= x; # store the context word\n",
    "        end\n",
    "        x = sum(tmp, dims=2) |> vec .|> Float32; # average of the context words\n",
    "        \n",
    "        # store the training data -\n",
    "        push!(training_dataset, (x, y)); # store the training data\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d378e1",
   "metadata": {},
   "source": [
    "__Skip Gram training datatset__: Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53163669",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_training_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, Vector{Float32}}}();\n",
    "    C = windowsize - 1; # number of context words\n",
    "\n",
    "    # build the training data -\n",
    "    for i ∈ 2:(N-1)\n",
    "        \n",
    "        contextword = words[i]; # target word\n",
    "        targetwords = words[(i-1):(i+1)] |> v-> [v[1], v[3]] # context words\n",
    "        \n",
    "        # proces the context word -\n",
    "        contextword_index = vocabulary[targetword]; # index of the target word\n",
    "        x = onehot(contextwork_index, number_digit_array) |> Float32; # one-hot encoding of the target word\n",
    "\n",
    "        # process the targets words -\n",
    "        tmp = Array{Float32,2}(undef, N, C); # temporary array\n",
    "        for (j,word) in enumerate(contextwords)\n",
    "            contextword_index = vocabulary[word]; # index of the context word\n",
    "            x = onehot(contextword_index, number_digit_array) .|> Float32; # one-hot encoding of the context word\n",
    "            tmp[:, j] .= x; # store the context word\n",
    "        end\n",
    "        x = sum(tmp, dims=2) |> vec .|> Float32; # average of the context words\n",
    "        \n",
    "        # store the training data -\n",
    "        push!(training_dataset, (x, y)); # store the training data\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8df968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbow_training_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ce09b",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train a CBOW model instance\n",
    "In this task, we will build and train a CBOW model instance on the sample input sequence we selected above. We start by creating a model instance, and the we train this instance for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bdef9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(8 => 3),                \u001b[90m# 27 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 8),               \u001b[90m# 32 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m59 parameters, 444 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbow_model = let\n",
    "\n",
    "    # TODO: Uncomment the code below to build the model!\n",
    "    Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "    MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "        Chain(\n",
    "            input = Dense(N, windowsize, identity),  # layer 1\n",
    "            hidden = Dense(windowsize, N, identity), # layer 2\n",
    "            output = NNlib.softmax) # layer 3 (output layer)\n",
    "    );\n",
    "    cbow_model = MyModel().chain;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d526364",
   "metadata": {},
   "source": [
    "__Training__: Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc319dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 10 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 20 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 30 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 40 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 50 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 60 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 70 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 80 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 90 of 100 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 100 of 100 completed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(8 => 3),                \u001b[90m# 27 parameters\u001b[39m\n",
       "  hidden = Dense(3 => 8),               \u001b[90m# 32 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m59 parameters, 444 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_cbow_model = let\n",
    "\n",
    "    localmodel = cbow_model; # make a local copy of the model\n",
    "\n",
    "    # setup the loss function -\n",
    "    loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?\n",
    "\n",
    "    # setup the optimizer\n",
    "    λ = 0.64; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(Momentum(λ,β), localmodel);\n",
    "\n",
    "    # training loop -\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "        Flux.train!(localmodel, cbow_training_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y) # loss function\n",
    "        end\n",
    "\n",
    "        if (rem(i,10) == 0)\n",
    "            @show \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the trained model -\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdfb54",
   "metadata": {},
   "source": [
    "Let's give the model a few inputs and see what it predicts. If we give it the original context, it should return the original target word. \n",
    "* _What get's returned?_ The network will return $p(w_{i}|\\mathbf{x})$, the probability of each word in the vocabulary being the target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a20ee99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,word) = let\n",
    "    \n",
    "    x = cbow_training_dataset[1][1]; # first training data\n",
    "    y = trained_cbow_model(x);\n",
    "    word = y |> argmax |> i-> inverse_vocabulary[i]; # index of the word\n",
    "\n",
    "    (x,y,word) # return the values\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cb6161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{String}:\n",
       " \"the\"\n",
       " \"brown\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x |> x-> findall(x-> x!= 0.0, x) .|> i-> inverse_vocabulary[i] # find the words in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa14fed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"quick\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba29617",
   "metadata": {},
   "source": [
    "What happens if we give it a perturbed context? Does it still return the original target word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94bbcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,word) = let\n",
    "\n",
    "    # Let's add two contexts together, and see what happens -\n",
    "    x₁ = cbow_training_dataset[1][1]; # first context\n",
    "    x₂ = cbow_training_dataset[4][1]; # second context\n",
    "    x = (x₁ + x₂); # sum of the two contexts\n",
    "\n",
    "    y = trained_cbow_model(x); # run the model with the sum of the two contexts\n",
    "    word = y |> argmax |> i-> inverse_vocabulary[i]; # get the word from the index\n",
    "\n",
    "    \n",
    "    # return -\n",
    "    x, y, word\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5369f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{String}:\n",
       " \"the\"\n",
       " \"brown\"\n",
       " \"fox\"\n",
       " \"over\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x |> x-> findall(x-> x!= 0.0, x) .|> i-> inverse_vocabulary[i] # find the words in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8c626dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jumps\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1d650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
