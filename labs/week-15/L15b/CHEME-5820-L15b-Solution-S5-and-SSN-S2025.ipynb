{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0b40ea-0b0d-4237-b28e-653a016bbe43",
   "metadata": {},
   "source": [
    "# L15b: Combining S5 and Spike Nueral Networks\n",
    "Fill me in.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "Before we start, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Setup, Data, Prerequisites (10 min)__: In this task, we set up the computational environment and then specify a simple text sequence, e.g., a sentence without punctuation. From this sequence, we'll build a vocabulary, an inverse vocabulary, and the training datasets for the CBOW and skip-gram models. \n",
    "* __Task 2: Build and Train a CBOW model instance (20 min)__: In this task, we build and train a Continuous Bag of Words (CBOW) model instance on a sample input sequence. We start by creating a model instance, and then we train this instance for a few epochs, and finally, we see how the model performs.\n",
    "* __Task 3: Build and train a skip-gram model instance (20 min)__: In this task, we will build and train a skip-gram model instance on the sample input sequence we selected above. We start by creating a model instance, then train it for a few epochs and see how it performs.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e06d22",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3aa51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17752d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end]; # helper function to get the file extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bb1c0",
   "metadata": {},
   "source": [
    "### Load the MNIST digits data set\n",
    "In this example, we'll use a Hopfield network to learn a single image pattern from [the MNIST handwritten digits dataset](https://en.wikipedia.org/wiki/MNIST_database). The goal is to train the network to recognize a digit, e.g., \"3\" or \"5\", etc, and then retrieve it from a noisy input.\n",
    "\n",
    "Load a training image dataset that we'll encode into the Hopfield network. We'll save training data in the `training_image_dataset` variable.\n",
    "* _What's in the training dataset_? The `training_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x.` The second element is the `label,` i.e., whether the image corresponds to `0,....,9`. However, the label is encoded as a `OneHotVector` (see below).\n",
    "* _Hmmm. That's strange_. The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of _weird features_. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values).\n",
    "\n",
    "However, before we load the training data, let's set some constants, which we use below. The comment next to each constant describes it, its permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0deacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 30; # how many training examples of *each* number to include from the library\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?\n",
    "number_of_images_to_memorize = 3; # number of images that we want to encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1c2e2",
   "metadata": {},
   "source": [
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable. First, we load all the images into the `training_image_dictionary::Dict{Int64, Array{Gray{N0f8},3}}`, and then we'll convert these to a vector format below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec7e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_dictionary = let\n",
    "    training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "    for i ∈ number_digit_array\n",
    "        \n",
    "        # create a set for this digit -\n",
    "        image_digit_array = Array{Gray{N0f8},3}(undef, number_of_rows, number_of_cols, number_of_training_examples);\n",
    "        files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "        imagecount = 1;\n",
    "        for fileindex ∈ 1:number_of_training_examples\n",
    "            filename = files[fileindex];\n",
    "            ext = file_extension(filename)\n",
    "            if (ext == \"jpg\")\n",
    "                image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "                imagecount += 1\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # capture -\n",
    "        training_image_dictionary[i] = image_digit_array\n",
    "    end\n",
    "    training_image_dictionary\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fc8eb",
   "metadata": {},
   "source": [
    "Next, we take the images in array format and vectorize them. \n",
    "* _What do we mean by vectorize_? Each $N\\times{N}$ image array containing the grayscale values at each pixel is converted to an $N^{2}$ vector of values. What image class, i.e., what number it represents, is then converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). The converted data is stored in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n",
    "* _What's the deal with Float32_? Most neural network libraries (or other machine learning calculations) use `Float32` (or lower) to save memory because of the large number of parameters associated with the network. Additionally, model training is often carried out using specialized hardware [such as Graphical Processing Units (GPUs)](https://www.nvidia.com/en-us/data-center/h100/), which has different memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b79705",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_dataset = let\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array);\n",
    "        X = training_image_dictionary[i];\n",
    "        \n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = Array{Float32,1}(undef, number_of_pixels);\n",
    "            linearindex = 1;\n",
    "            for row ∈ 1:number_of_rows\n",
    "                for col ∈ 1:number_of_cols\n",
    "                    D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                    linearindex+=1;\n",
    "                end\n",
    "            end\n",
    "    \n",
    "            training_tuple = (D,Y);\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "    end\n",
    "    training_image_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745b933",
   "metadata": {},
   "source": [
    "## Task 2: What is a Spiking Neural Network?\n",
    "Fill me in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588116d",
   "metadata": {},
   "source": [
    "## Task 3: Let's build an S5 model \n",
    "Fill me in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3108b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
