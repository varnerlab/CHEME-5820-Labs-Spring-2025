{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe9634d-cfb4-4a0b-9df2-c249620140b8",
   "metadata": {},
   "source": [
    "# L15d: Graph Classification with Graph Neural Networks (GNNs)\n",
    "Fill me in.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019ca79",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e14695",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0ffaf",
   "metadata": {},
   "source": [
    "### Data\n",
    "One common task for graph neural networks is the graph classification problem, e.g., a molecular property prediction task, in which _molecules are represented as graphs_, and the task may be to infer whether a molecule inhibits HIV virus replication or not.\n",
    "\n",
    "The [TU Dortmund University](https://www.tu-dortmund.de/en/) has collected a wide range of different graph classification datasets, known as the [TUDatasets](https://chrsmrrs.github.io/datasets/), which are [accessible via the  `MLDatasets.jl` package](https://juliaml.github.io/MLDatasets.jl/stable/). Let's load and inspect one of the smaller ones, the __MUTAG dataset__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00ff55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"MUTAG\"); # download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33319a67",
   "metadata": {},
   "source": [
    "_What's in the the `dataset` variable?_ This dataset provides _188 different graphs_, and the task is to classify each graph into _one out of two classes_ (binary classification).\n",
    "\n",
    "By inspecting the first graph object of the dataset, we can see that it comes with **17 nodes** and **38 edges**.\n",
    "It also comes with exactly **one graph label**, and provides additional node labels (7 classes) and edge labels (4 classes).\n",
    "However, for the sake of simplicity, we will not make use of edge labels.\n",
    "\n",
    "We have some useful utilities for working with graph datasets, *e.g.*, we can shuffle the dataset and use the first 150 graphs as training graphs, while using the remaining ones for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8835f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = mldataset2gnngraph(dataset)\n",
    "\n",
    "graphs = [GNNGraph(g, \n",
    "\t\t\t\t\tndata=Float32.(onehotbatch(g.ndata.targets, 0:6)),\n",
    "\t\t\t\t\tedata=nothing) \n",
    "\t\t\tfor g in graphs]\n",
    "\n",
    "shuffled_idxs = randperm(length(graphs))\n",
    "train_idxs = shuffled_idxs[1:150]\n",
    "test_idxs = shuffled_idxs[151:end]\n",
    "train_graphs = graphs[train_idxs]\n",
    "test_graphs = graphs[test_idxs]\n",
    "ytrain = onehotbatch(dataset.graph_data.targets[train_idxs], [-1, 1])\n",
    "ytest = onehotbatch(dataset.graph_data.targets[test_idxs], [-1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdad8af",
   "metadata": {},
   "source": [
    "## Task 2: Setup mini-batching of graphs\n",
    "\n",
    "Since graphs in graph classification datasets are usually small, a good idea is to **batch the graphs** before inputting them into a Graph Neural Network to guarantee full GPU utilization.\n",
    "In the image or language domain, this procedure is typically achieved by **rescaling** or **padding** each example into a set of equally-sized shapes, and examples are then grouped in an additional dimension.\n",
    "The length of this dimension is then equal to the number of examples grouped in a mini-batch and is typically referred to as the `batchsize`.\n",
    "\n",
    "However, for GNNs the two approaches described above are either not feasible or may result in a lot of unnecessary memory consumption.\n",
    "Therefore, GNN.jl opts for another approach to achieve parallelization across a number of examples. Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension (the last dimension).\n",
    "\n",
    "This procedure has some crucial advantages over other batching procedures:\n",
    "\n",
    "1. GNN operators that rely on a message passing scheme do not need to be modified since messages are not exchanged between two nodes that belong to different graphs.\n",
    "\n",
    "2. There is no computational or memory overhead since adjacency matrices are saved in a sparse fashion holding only non-zero entries, *i.e.*, the edges.\n",
    "\n",
    "GNN.jl can **batch multiple graphs into a single giant graph** with the help of `collate` option of `DataLoader` that implicitly calls `Flux.batch` on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906f7022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element DataLoader(::Tuple{Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, batchsize=10, collate=Val{true}())\n",
       "  with first element:\n",
       "  (GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}, 2Ã—10 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = DataLoader((train_graphs, ytrain), batchsize=64, shuffle=true, collate=true)\n",
    "test_loader = DataLoader((test_graphs, ytest), batchsize=10, shuffle=false, collate=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20eb2f",
   "metadata": {},
   "source": [
    "## Task 3: Define and Train a GNN model for the Graph Classification task\n",
    "In this task, we will define a GNN model for the graph classification task, train the model on the training set, and evaluate it on the test set.\n",
    "\n",
    "Let's start by setting some constants for the model, such as the number of input features, the number of hidden features, and the number of output classes. See the comments next to the constants for more details on what they mean, permissble values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f540276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nin = 7 # dimension of the node feature vectors  \n",
    "nout = 2 # output dimension for the system\n",
    "nh = 64 # number of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f8df5",
   "metadata": {},
   "source": [
    "Next, lets define and train the GNN model. We will use a [custom `MyCustomConvolutionLayerModel` layer](src/Types.jl). You complete me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33677c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch = 0\n",
      "train = (loss = 0.5919, acc = 50.0)\n",
      "test = (loss = 0.7201, acc = 50.0)\n",
      "# epoch = 10\n",
      "train = (loss = 0.4301, acc = 51.0)\n",
      "test = (loss = 0.5649, acc = 50.0)\n",
      "# epoch = 20\n",
      "train = (loss = 0.3357, acc = 57.67)\n",
      "test = (loss = 0.5356, acc = 56.58)\n",
      "# epoch = 30\n",
      "train = (loss = 0.2771, acc = 80.33)\n",
      "test = (loss = 0.4974, acc = 72.37)\n",
      "# epoch = 40\n",
      "train = (loss = 0.2529, acc = 84.67)\n",
      "test = (loss = 0.3615, acc = 77.63)\n",
      "# epoch = 50\n",
      "train = (loss = 0.2359, acc = 90.33)\n",
      "test = (loss = 0.4466, acc = 76.32)\n",
      "# epoch = 60\n",
      "train = (loss = 0.2006, acc = 87.67)\n",
      "test = (loss = 0.4276, acc = 80.26)\n",
      "# epoch = 70\n",
      "train = (loss = 0.243, acc = 85.67)\n",
      "test = (loss = 0.4435, acc = 82.89)\n",
      "# epoch = 80\n",
      "train = (loss = 0.1659, acc = 89.67)\n",
      "test = (loss = 0.3884, acc = 82.89)\n",
      "# epoch = 90\n",
      "train = (loss = 0.175, acc = 92.0)\n",
      "test = (loss = 0.5314, acc = 75.0)\n",
      "# epoch = 100\n",
      "train = (loss = 0.1706, acc = 92.0)\n",
      "test = (loss = 0.4012, acc = 85.53)\n",
      "# epoch = 110\n",
      "train = (loss = 0.1439, acc = 91.67)\n",
      "test = (loss = 0.4208, acc = 82.89)\n",
      "# epoch = 120\n",
      "train = (loss = 0.1413, acc = 93.67)\n",
      "test = (loss = 0.3829, acc = 84.21)\n",
      "# epoch = 130\n",
      "train = (loss = 0.1435, acc = 92.67)\n",
      "test = (loss = 0.3341, acc = 85.53)\n",
      "# epoch = 140\n",
      "train = (loss = 0.1173, acc = 94.67)\n",
      "test = (loss = 0.3549, acc = 85.53)\n",
      "# epoch = 150\n",
      "train = (loss = 0.1147, acc = 94.0)\n",
      "test = (loss = 0.2803, acc = 88.16)\n",
      "# epoch = 160\n",
      "train = (loss = 0.0906, acc = 94.0)\n",
      "test = (loss = 0.3396, acc = 86.84)\n",
      "# epoch = 170\n",
      "train = (loss = 0.0817, acc = 96.67)\n",
      "test = (loss = 0.3899, acc = 85.53)\n",
      "# epoch = 180\n",
      "train = (loss = 0.0715, acc = 96.67)\n",
      "test = (loss = 0.3793, acc = 85.53)\n",
      "# epoch = 190\n",
      "train = (loss = 0.0819, acc = 97.33)\n",
      "test = (loss = 0.412, acc = 82.89)\n",
      "# epoch = 200\n",
      "train = (loss = 0.0696, acc = 95.33)\n",
      "test = (loss = 0.374, acc = 88.16)\n"
     ]
    }
   ],
   "source": [
    "mymodel = let\n",
    "\t\n",
    "\tFlux.@layer MyCustomConvolutionLayerModel\n",
    "\n",
    "\tinputlayer = MyCustomConvolutionLayerModel(nin => nh, relu);\n",
    "\thiddenlayer = MyCustomConvolutionLayerModel(nh => nh, relu);\n",
    "\toutputlayer = MyCustomConvolutionLayerModel(nh => nh);\n",
    "\n",
    "\tmodel = GNNChain(inputlayer,\n",
    "\t\t\t\thiddenlayer,\n",
    "\t\t\t\toutputlayer,\n",
    "\t\t\t\tGlobalPool(mean), # what is this doing?\n",
    "\t\t\t\tDropout(0.5), # what is this doing?\n",
    "\t\t\t\tDense(nh, nout))\n",
    "\t\t\t\t\n",
    "\ttrain!(model)\n",
    "\tmodel; # return the trained model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90545ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
